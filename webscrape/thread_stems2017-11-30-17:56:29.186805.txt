{'Tomography AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5153359/', 'p': u'The VESSEL12 (VESsel SEgmentation in the Lung) challenge objectively compares the performance of different algorithms to identify vessels in thoracic computed tomography (CT) scans. Vessel segmentation is fundamental in computer aided processing of data generated by 3D imaging modalities. As manual vessel segmentation is prohibitively time consuming, any real world application requires some form of automation. Several approaches exist for automated vessel segmentation, but judging their relative merits is difficult due to a lack of standardized evaluation. We present an annotated reference dataset containing 20 CT scans and propose nine categories to perform a comprehensive evaluation of vessel segmentation algorithms from both academia and industry. Twenty algorithms participated in the VESSEL12 challenge, held at International Symposium on Biomedical Imaging (ISBI) 2012. All results have been published at the VESSEL12 website http://vessel12.grand-challenge.org. The challenge remains ongoing and open to new participants. Our three contributions are: (1) an annotated reference dataset available online for evaluation of new algorithms; (2) a quantitative scoring system for objective comparison of algorithms; and (3) performance analysis of the strengths and weaknesses of the various vessel segmentation methods in the presence of various lung diseases.The aim of VESSEL12 Challenge, organized in conjunction with the International Symposium on Biomedical Imaging 2012 (ISBI\u201912), is to provide a public platform to compare the performance of different segmentation algorithms to identify lung vessels in thoracic computed tomography (CT) data. An additional goal is to characterize what kind of anatomical neighborhood may complicate vessel segmentation, for example the presence of nodules, dense consolidation and parenchymal or bronchial abnormalities.', 'kwd': u'Thoracic computed tomography, Lung vessels, Algorithm comparison, Segmentation, Challenge', 'title': u'Comparing algorithms for automated vessel segmentation in computed tomography scans of the lung: the VESSEL12 study'}], 'Arterial Coronary Syndrome AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}], 'Patient Assessment AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5219634/', 'p': u'Amyotrophic lateral sclerosis (ALS) is a progressive neuromuscular disease, with large variation in survival between patients. Currently, it remains rather difficult to predict survival based on clinical parameters alone. Here, we set out to use clinical characteristics in combination with MRI data to predict survival of ALS patients using deep learning, a machine learning technique highly effective in a broad range of big-data analyses. A group of 135 ALS patients was included from whom high-resolution diffusion-weighted and T1-weighted images were acquired at the first visit to the outpatient clinic. Next, each of the patients was monitored carefully and survival time to death was recorded. Patients were labeled as short, medium or long survivors, based on their recorded time to death as measured from the time of disease onset. In the deep learning procedure, the total group of 135 patients was split into a training set for deep learning (n\xa0=\xa083 patients), a validation set (n\xa0=\xa020) and an independent evaluation set (n\xa0=\xa032) to evaluate the performance of the obtained deep learning networks. Deep learning based on clinical characteristics predicted survival category correctly in 68.8% of the cases. Deep learning based on MRI predicted 62.5% correctly using structural connectivity and 62.5% using brain morphology data. Notably, when we combined the three sources of information, deep learning prediction accuracy increased to 84.4%. Taken together, our findings show the added value of MRI with respect to predicting survival in ALS, demonstrating the advantage of deep learning in disease prognostication.Each of the 135 patients was categorized according to the true survival time (i.e. time between disease onset and death): short survivors with survival up to 25\xa0months after disease onset, medium survivors with survival between 25 and 50\xa0months after disease onset, and long survivors living over 50\xa0months after disease onset (Elamin et al., 2015). The group of long survivors consisted of patients who either died after a disease duration of at least 50\xa0months or were still alive and had a disease duration of at least 50\xa0months at time of analysis.', 'kwd': u'Deep learning, Neural network, Amyotrophic lateral sclerosis, White matter connectivity, Survival, Prediction', 'title': u'Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/', 'p': u'The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements.Under a mild assumption on the activation function, a two-layer neural network with a finite number of hidden units can approximate any continuous function (63), and thus it is regarded as universal approximator. However, it is also possible to approximate complex functions to the same accuracy using a \u2018deep\u2019 architecture, i.e., more than two layers, with much fewer number of units in total (3). Hence, it is possible to reduce the number of trainable parameters, thus allowing to train with a relatively small dataset (64).', 'kwd': u'Medical image analysis, deep learning, unsupervised feature learning', 'title': u'Deep Learning in Medical Image Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869115/', 'p': u'Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name \u201cdeep patient\u201d. We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.Feature learning algorithms are usually evaluated in supervised applications to take advantage of the available manually annotated labels. Here we used the Mount Sinai data warehouse to learn the deep features and we evaluated them in predicting patient future diseases. The Mount Sinai Health System generates a high volume of structured, semi-structured and unstructured data as part of its healthcare and clinical operations, which include inpatient, outpatient and emergency room visits. Patients in the system can have as long as 12\u2009years of follow up unless they moved or changed insurance. Electronic records were completely implemented by our health system starting in 2003. The data related to patients who visited the hospital prior to 2003 was migrated to the electronic format as well but we may lack certain details of hospital visits (i.e., some diagnoses or medications may not have been recorded or transferred). The entire EHR dataset contains approximately 4.2 million de-identified patients as of March 2015, and it was made available for use under IRB approval following HIPAA guidelines. We retained all patients with at least one diagnosed disease expressed as numerical ICD-9 between 1980 and 2014, inclusive. This led to a dataset of about 1.2 million patients, with every patient having an average of 88.9 records. Then, we considered all records up to December 31, 2013 (i.e., \u201csplit-point\u201d) as training data (i.e., 34\u2009years of training information) and all the diagnoses in 2014 as testing data.', 'kwd': '-', 'title': u'Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5661078/', 'p': u'Molecular imaging enables the visualization and quantitative analysis of the alterations of biological procedures at molecular and/or cellular level, which is of great significance for early detection of cancer. In recent years, deep leaning has been widely used in medical imaging analysis, as it overcomes the limitations of visual assessment and traditional machine learning techniques by extracting hierarchical features with powerful representation capability. Research on cancer molecular images using deep learning techniques is also increasing dynamically. Hence, in this paper, we review the applications of deep learning in molecular imaging in terms of tumor lesion segmentation, tumor classification, and survival prediction. We also outline some future directions in which researchers may develop more powerful deep learning models for better performance in the applications in cancer molecular imaging.The papers on diverse applications of deep learning in different molecular imaging of cancer published from 2014 onwards were included. This review contains 25 papers and is organized according to the application of deep learning in cancer molecular imaging, including tumor lesion segmentation, cancer classification, and prediction of patient survival.  Table 1 summarizes the 13 different studies on tumor lesion segmentation, while Table 2 summarizes the 10 different studies on cancer classification. Two interesting papers on prediction of patient survival are also reviewed (Table 3). To our best knowledge, there is no previous work making such a comprehensive review on this issue. In this regard, we believe this survey can present radiologists and physicians with the application status of advanced artificial intelligent techniques in molecular images analysis and hence inspire more applications in clinical practice. Biomedical engineering researchers may also benefit from this survey by acquiring the state of the art in this field or inspiration for better models/methods in future research.', 'kwd': '-', 'title': u'Application of Deep Learning in Automated Analysis of Molecular Images in Cancer: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537095/', 'p': u'Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends.This brain tumor image segmentation challenge in conjunction with the MICCAI conference has been held annually since 2012 in order to evaluate the current state-of-the-art in automated brain tumor segmentation and compare between different methods. For this purpose, a large dataset of brain tumor MR scans and ground truth (five labels: healthy brain tissue, necrosis, edema, non-enhanced, and enhanced regions of tumors) are made publicly available. The training data has increased over the years. Currently (Brats 2015\u20132016), the training set comprises 220 subjects with high grade and 54 subjects with low-grade, and the test set comprises 53 subjects with mixed grades. All datasets have been aligned to the same anatomical template and interpolated to 1\xa0mm3 voxel resolution. Each dataset has pre-contrast T1, post contrast T1, T2, and T2 FLAIR MRI volumes. The co-registered, skull-stripped, and annotated training dataset and evaluation results of algorithms are available via the Virtual Skeleton Database (https://www.virtualskeleton.ch/).', 'kwd': u'Deep learning, Quantitative brain MRI, Convolutional neural network, Brain lesion segmentation', 'title': u'Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5651626/', 'p': u'Myelin imaging is a form of quantitative magnetic resonance imaging (MRI) that measures myelin content and can potentially allow demyelinating diseases such as multiple sclerosis (MS) to be detected earlier. Although focal lesions are the most visible signs of MS pathology on conventional MRI, it has been shown that even tissues that appear normal may exhibit decreased myelin content as revealed by myelin-specific images (i.e., myelin maps). Current methods for analyzing myelin maps typically use global or regional mean myelin measurements to detect abnormalities, but ignore finer spatial patterns that may be characteristic of MS. In this paper, we present a machine learning method to automatically learn, from multimodal MR images, latent spatial features that can potentially improve the detection of MS pathology at early stage. More specifically, 3D image patches are extracted from myelin maps and the corresponding T1-weighted (T1w) MRIs, and are used to learn a latent joint myelin-T1w feature representation via unsupervised deep learning. Using a data set of images from MS patients and healthy controls, a common set of patches are selected via a voxel-wise t-test performed between the two groups. In each MS image, any patches overlapping with focal lesions are excluded, and a feature imputation method is used to fill in the missing values. A feature selection process (LASSO) is then utilized to construct a sparse representation. The resulting normal-appearing features are used to train a random forest classifier. Using the myelin and T1w images of 55 relapse-remitting MS patients and 44 healthy controls in an 11-fold cross-validation experiment, the proposed method achieved an average classification accuracy of 87.9% (SD\xa0=\xa08.4%), which is higher and more consistent across folds than those attained by regional mean myelin (73.7%, SD\xa0=\xa013.7%) and T1w measurements (66.7%, SD\xa0=\xa010.6%), or deep-learned features in either the myelin (83.8%, SD\xa0=\xa011.0%) or T1w (70.1%, SD\xa0=\xa013.6%) images alone, suggesting that the proposed method has strong potential for identifying image features that are more sensitive and specific to MS pathology in normal-appearing brain tissues.A cohort of 55 relapsing-remitting MS (RRMS) patients and a cohort of 44 age- and gender-matched normal control (NC) subjects were included in this study. The median age and range for both groups were 45 and 30\u201360. For the RRMS patients, 63.6% (35/55) were female, and 63.5% (28/44) of the NC subjects were female. The McDonald 2010 criteria (Polman et al., 2011) were used to diagnose the patients for MS. All patients underwent a neurological assessment and were scored on the Expanded Disability Status Scale (EDSS) (Kurtzke, 1983). The median EDSS and range were 4 and 0\u20135. Informed consent from each participant and ethical approval by the local ethics committee were obtained prior to the study.', 'kwd': u'Deep learning, Multiple sclerosis, Myelin water imaging, Machine learning, Magnetic resonance imaging', 'title': u'Deep learning of joint myelin and T1w MRI features in normal-appearing brain tissue to distinguish between multiple\xa0sclerosis\xa0patients and healthy controls'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5380996/', 'p': u'Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.The dataset consisted of 74 whole-slide images of breast tumour resection samples which either retrieved from the AstraZeneca BioBank or acquired from a commercial provider (Dako Denmark A/S). Slides were obtained by cutting formalin-fixed, paraffin embedded human breast cancer samples into 4\u2009\u03bcm-thick sections, stained by IHC for HER2 demonstration (monoclonal Rabbit Anti-Human HER2 antibody, Dako Denmark A/S) and counterstained with haematoxylin using a Dako Autostainer Link48 (Dako Denmark A/S). Slides were digitized with an Aperio ScanScope whole-slide imaging microscope (Aperio, Leica Biosystems Imaging, Inc.) at a resolution of 0.49\u2009\u03bcm/pixel. The slides were reviewed to confirm the presence of invasive carcinoma and a total of 71 invasive carcinoma cases were selected for the study.', 'kwd': '-', 'title': u'Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431941/', 'p': u'Precision medicine approaches rely on obtaining precise knowledge of the true state of health of an individual patient, which results from a combination of their genetic risks and environmental exposures. This approach is currently limited by the lack of effective and efficient non-invasive medical tests to define the full range of phenotypic variation associated with individual health. Such knowledge is critical for improved early intervention, for better treatment decisions, and for ameliorating the steadily worsening epidemic of chronic disease. We present proof-of-concept experiments to demonstrate how routinely acquired cross-sectional CT imaging may be used to predict patient longevity as a proxy for overall individual health and disease status using computer image analysis techniques. Despite the limitations of a modest dataset and the use of off-the-shelf machine learning methods, our results are comparable to previous \u2018manual\u2019 clinical methods for longevity prediction. This work demonstrates that radiomics techniques can be used to extract biomarkers relevant to one of the most widely used outcomes in epidemiological and clinical research \u2013 mortality, and that deep learning with convolutional neural networks can be usefully applied to radiomics research. Computer image analysis applied to routinely collected medical images offers substantial potential to enhance precision medicine initiatives.We created a predictive model using multivariable survival analysis (Cox regression). The model was informed by the top 5 covariates selected by minimum redundancy-maximum relevance feature selection50. These covariates were standardised, and the resulting risk score was dichotomised at the mean to create high-risk and low-risk phenotypes. Table\xa02 shows the 5-year mortality rate for high and low risk phenotypes, and the related Kaplan-Meier curves are presented in Fig.\xa02. The difference between the survival curves for of the high and low risk phenotypes is highly significant (p\u2009<\u20090.00005). The distribution of the raw mortality phenotype scores among cases and controls are presented in a box and whisker plot in Supplemental Figure\xa02.\n\n', 'kwd': '-', 'title': u'Precision Radiology: Predicting longevity using feature engineering and deep learning methods in a radiomics framework'}], 'Coronary Artery Disease AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615055/', 'p': u'957 rest/stress 99mtechnetium gated MPS NC studies from 623 consecutive patients with correlating invasive coronary angiography and 334 with low likelihood of CAD (LLK < 5% ) were assessed. Patients with stenosis \u2265 50% in left main or \u2265 70% in all other vessels were considered abnormal. Total perfusion deficit (TPD) was computed automatically. In addition, ischemic changes (ISCH) and ejection fraction changes (EFC) between stress and rest were derived by quantitative software. The SVM was trained using a group of 125 pts (25 LLK, 25 0-, 25 1-, 25 2- and 25 3-vessel CAD) using above quantitative variables and second order polynomial fitting. The remaining patients (N = 832) were categorized based on probability estimates, with CAD defined as (probability estimate \u2265 0.50). The diagnostic accuracy of SVM was also compared to visual segmental scoring by two experienced readers.Sensitivity of SVM (84%) was significantly better than ISCH (75%, p < 0.05) and EFC (31%, p < 0.05). Specificity of SVM (88%) was significantly better than that of TPD (78%, p < 0.05) and EFC (77%, p < 0.05). Diagnostic accuracy of SVM (86%) was significantly better than TPD (81%), ISCH (81%), or EFC (46%) (p < 0.05 for all). The Receiver-operator-characteristic area-under-the-curve (ROC-AUC) for SVM (0.92) was significantly better than TPD (0.90), ISCH (0.87), and EFC (0.60) (p < 0.001 for all). Diagnostic accuracy of SVM was comparable to the overall accuracy of both visual readers (85% vs. 84%, p < 0.05). ROC-AUC for SVM (0.92) was significantly better than that of both visual readers (0.87 and 0.88, p < 0.03).', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit, Support Vector Machines, Machine Learning', 'title': u'Improved Accuracy of Myocardial Perfusion SPECT for the Detection of Coronary Artery Disease by Utilizing a Support Vector Machines Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563765/', 'p': u'995 rest/stress 99mTc-sestamibi MPS studies, [650 consecutive cases with coronary angiography and 345 with likelihood of CAD < 5% (LLk)] were obtained by MPS with AC. Total perfusion deficit (TPD) for AC and NC data were compared to the visual summed stress and rest scores of 2 experienced readers. Visual reads were performed in 4 consecutive steps with the following information progressively revealed: NC data, AC+NC data, computer results, all clinical information.The diagnostic accuracy of TPD for detection of CAD was similar to both readers (NC: 82% vs. 84%, AC: 86% vs. 85\u201387% p = NS) with the exception of second reader when using clinical information (89%, p < 0.05). The Receiver-Operator-Characteristics Areas-Under-Curve (ROC-AUC) for TPD were significantly better than visual reads for NC (0.91 vs. 0.87 and 0.89, p < 0.01) and AC (0.92 vs. 0.90, p < 0.01), and it was comparable to visual reads incorporating all clinical information. Per-vessel accuracy of TPD was superior to one reader for NC (81% vs. 77%, p < 0.05) and AC (83% vs. 78%, p < 0.05) and equivalent to second reader [NC (79%) and AC (81%)]. Per-vessel ROC-AUC for NC (0.83) and AC (0.84) for TPD were better than (0.78\u20130.80 p < 0.01), and comparable to second reader (0.82\u20130.84, p = NS), for all steps.', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit', 'title': u'Comparison of Fully Automated Computer Analysis and Visual Scoring for Detection of Coronary Artery Disease from Myocardial Perfusion SPECT in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547801/', 'p': u'Conceived and designed the experiments: KPL ANA VK ZX SC SNM RMP PS IK SYS EWK TC. Performed the experiments: KPL ANA VK ZX AC VSG SG PC GKS DA JYL SYS TC. Analyzed the data: KPL ANA VK ZX DA SYS TC. Wrote the paper: KPL ANA VK ZX AC VSG SG PC GKS DA SC JYL SNM RMP PS IK SYS EWK TC.Typically, algorithms to classify phenotypes using electronic medical record (EMR) data were developed to perform well in a specific patient population. There is increasing interest in analyses which can allow study of a specific outcome across different diseases. Such a study in the EMR would require an algorithm that can be applied across different patient populations. Our objectives were: (1) to develop an algorithm that would enable the study of coronary artery disease (CAD) across diverse patient populations; (2) to study the impact of adding narrative data extracted using natural language processing (NLP) in the algorithm. Additionally, we demonstrate how to implement CAD algorithm to compare risk across 3 chronic diseases in a preliminary study.', 'kwd': '-', 'title': u'Methods to Develop an Electronic Medical Record Phenotype Algorithm to Compare the Risk of Coronary Artery Disease across 3 Chronic Disease Cohorts'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680364/', 'p': u'Artificial neural networks (ANN) might help to diagnose coronary artery disease. This study aimed to determine whether the diagnostic accuracy of an ANN-based diagnostic system and conventional quantitation are comparable.The ANN was trained to classify potentially abnormal areas as true or false based on the nuclear cardiology expert interpretation of 1001 gated stress/rest 99mTc-MIBI images at 12 hospitals. The diagnostic accuracy of the ANN was compared with 364 expert interpretations that served as the gold standard of abnormality for the validation study. Conventional summed stress/rest/difference scores (SSS/SRS/SDS) were calculated and compared with receiver operating characteristics (ROC) analysis.', 'kwd': u'Artificial intelligence, Diagnostic imaging, Coronary artery disease, Nuclear cardiology, Computer-aided diagnosis', 'title': u'Diagnostic accuracy of an artificial neural network compared with statistical quantitation of myocardial perfusion images: a Japanese multicenter study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4859156/', 'p': u'We aimed to investigate if early revascularization in patients with suspected coronary artery disease (CAD) can be effectively predicted by integrating clinical data and quantitative image features derived from perfusion SPECT (MPS) by machine learning (ML) approach.713 rest 201Thallium/stress 99mTechnetium MPS studies with correlating invasive angiography (372 revascularization events (275 PCI / 97 CABG) within 90 days after MPS (91% within 30 days) were considered. Transient ischemic dilation (TID), stress combined supine/prone total perfusion deficit (TPD), quantitative rest and stress TPD, exercise ejection fraction, and end-systolic volume along with clinical parameters including patient gender, history of hypertension and diabetes mellitus, ST-depression on baseline ECG, ECG and clinical response during stress, and post-ECG probability by boosted ensemble ML algorithm (LogitBoost) to predict revascularization events. These features were selected using an automated feature selection algorithm from all available clinical and quantitative data (33 parameters). 10-fold cross-validation was utilized to train and test the prediction model. The prediction of revascularization by ML algorithm was compared to standalone measures of perfusion and visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data.', 'kwd': u'Machine Learning, Coronary Artery Disease, Myocardial Perfusion SPECT, Revascularization, Total Perfusion Deficit', 'title': u'Prediction of Revascularization after Myocardial Perfusion SPECT by Machine Learning in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3808723/', 'p': u'\nBackground. Coronary artery disease (CAD) is a complex, multifactorial disease in which personality seems to play a role but with no definition in combination with other risk factors. Objective. To explore the nonlinear and simultaneous pathways between traditional and personality traits risk factors and coronary stenosis by Artificial Neural Networks (ANN) data mining analysis. Method. Seventy-five subjects were examined for traditional cardiac risk factors and personality traits. Analyses were based on a new data mining method using a particular artificial adaptive system, the autocontractive map (AutoCM). Results. Several traditional Cardiovascular Risk Factors (CRF) present significant relations with coronary artery plaque (CAP) presence or severity. Moreover, anger turns out to be the main factor of personality for CAP in connection with numbers of traditional risk factors. Hidden connection map showed that anger, hostility, and the Type D personality subscale social inhibition are the core factors related to the traditional cardiovascular risk factors (CRF) specifically by hypertension. Discussion. This study shows a nonlinear and simultaneous pathway between traditional risk factors and personality traits associated with coronary stenosis in CAD patients without history of cardiovascular disease. In particular, anger seems to be the main personality factor for CAP in addition to traditional risk factors.The following traditional cardiac risk factors were examined: hypertension: arterial blood pressure \u2265 140/90\u2009mm\u2009Hg or taking antihypertensive medications [16]; diabetes: nonfasting plasma glucose concentration of at least 200\u2009mg/dL (11.1\u2009mmol/L), or fasting plasma glucose level of at least 126\u2009mg/dL (7.0\u2009mmol/L), or being treated with antidiabetic medication; overweight: body mass index (BMI) (calculated as weight divided by height squared) \u226527\u2009kg/m2 (WHO); dyslipidemia: total serum cholesterol level is higher than 240\u2009mg/dL or a serum triglyceride level is 200\u2009mg/dL or more (or both) or use of a lipid-lowering agent; smoking: at least one cigarette per day or quit smoking during the previous year; family history of CAD: a first degree or second degree relative with premature cardiovascular disease (age \u2264 55 years).', 'kwd': '-', 'title': u'Combining Personality Traits with Traditional Risk Factors for Coronary Stenosis: An Artificial Neural Networks Solution in Patients with Computed Tomography Detected Coronary Artery Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3732038/', 'p': u'We aimed to improve the diagnostic accuracy of myocardial perfusion SPECT (MPS) by integrating clinical data and quantitative image features with machine learning (ML) algorithms.1,181 rest 201Tl/stress 99mTc-sestamibi dual-isotope MPS studies [713 consecutive cases with correlating invasive coronary angiography (ICA) and suspected coronary artery disease (CAD) and 468 with low likelihood (LLk) of CAD <5%] were considered. Cases with stenosis <70% by ICA and LLk of CAD were considered normal. Total stress perfusion deficit (TPD) for supine/prone data, stress/rest perfusion change, and transient ischemic dilatation were derived by automated perfusion quantification software and were combined with age, sex, and post-electrocardiogram CAD probability by a boosted ensemble ML algorithm (Logit-Boost). The diagnostic accuracy of the model for prediction of obstructive CAD \u226570% was compared to standard prone/supine quantification and to visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data. Tenfold stratified cross-validation was performed.', 'kwd': u'Myocardial perfusion imaging, SPECT automated quantification, coronary artery disease, total perfusion deficit, machine learning', 'title': u'Improved accuracy of myocardial perfusion SPECT for detection of coronary artery disease by machine learning in a large population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253773/', 'p': u'Coronary artery disease (CAD) is the result of the accumulation of athermanous plaques within the walls of coronary arteries, which supply the myocardium with oxygen and nutrients. CAD leads to heart attacks or strokes and is, thus, one of the most important causes of death worldwide. Angiography, an imaging modality for blood vessels, is currently the most accurate method of diagnosing artery stenosis. However, the disadvantages of this method such as complications, costs, and possible side effects have prompted researchers to investigate alternative solutions.The current study aimed to use data analysis, a non-invasive and less costly method, and various data mining algorithms to predict the stenosis of arteries. Among many people who refer to hospitals due to chest pain, a great number of them are normal and as such do not need angiography. The objective of this study was to predict patients who are most probably normal using features with the highest correlations with CAD with a view to obviate angiography costs and complications. Not a substitute for angiography, this method would select high-risk cases that definitely need angiography.', 'kwd': u'Data Mining, Sensitivity and Specificity, Coronary Artery Disease', 'title': u'Diagnosing Coronary Artery Disease via Data Mining Algorithms by Considering Laboratory and Echocardiography Features'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643767/', 'p': u'The continuous uninterrupted feedback system is the essential part of any well-organized system. We propose aLYNX concept that is a possibility to use an artificial intelligence algorithm or a neural network model in decision-making system so as to avoid possible mistakes and to remind the doctors to review tactics once more in selected cases.aLYNX system includes: registry with significant factors, decisions and results; machine learning process based on this registry data; the use of the machine learning results as the adviser. We show a possibility to build a computer adviser with a neural network model for making a choice between coronary aortic bypass surgery (CABG) and percutaneous coronary intervention (PCI) in order to achieve a higher 5-year survival rate in patients with angina based on the experience of 5107 patients.', 'kwd': u'Coronary artery bypass grafting, Percutaneous coronary intervention, Artificial intelligence, Decision making', 'title': u'Artificial intelligence: Neural network model as the multidisciplinary team member in clinical decision support to avoid medical mistakes'}], 'Tomography AND image segmentation': [], 'Coronary Artery Disease AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5682365/', 'p': u'Cardiovascular diseases are one of the top causes of deaths worldwide. In developing nations and rural areas, difficulties with diagnosis and treatment are made worse due to the deficiency of healthcare facilities. A viable solution to this issue is telemedicine, which involves delivering health care and sharing medical knowledge at a distance. Additionally, mHealth, the utilization of mobile devices for medical care, has also proven to be a feasible choice. The integration of telemedicine, mHealth and computer-aided diagnosis systems with the fields of machine and deep learning has enabled the creation of effective services that are adaptable to a multitude of scenarios. The objective of this review is to provide an overview of heart disease diagnosis and management, especially within the context of rural healthcare, as well as discuss the benefits, issues and solutions of implementing deep learning algorithms to improve the efficacy of relevant medical applications.According to the World Health Organization (WHO), in 2015, cardiovascular diseases represented 31% of all global deaths (1), with ischemic heart disease often cited as the leading cause of death worldwide. Furthermore, public health statistics have shown an increase of patients with some form of cardiovascular disease in countries with low or middle gross national income (2). Although serious and often life threatening, cardiovascular disease in individuals can be managed clinically as a chronic condition, and treated with medications, diet, and regular monitoring of specific health indicators. Risk factors are fairly well defined and lifestyle changes can mitigate some risks. The motivation to prevent and manage heart disease has spurred development of numerous mHealth applications for consumer use, some of which have been scientifically assessed for efficacy (3). In this paper, we provide an overview of telemedicine and mHealth technologies applied in rural healthcare settings, using one form of cardiovascular disease for context. Additionally, we discuss the need for computer-aided diagnosis (CADx) as well as the implementation of machine and deep learning techniques in these systems. Finally, we explore the issues and solutions associated with using deep learning algorithms for medical applications.', 'kwd': u'Heart disease, rural healthcare, telemedicine, mHealth, computer-aided diagnosis, machine learning, deep learning', 'title': u'Deep learning for cardiac computer-aided diagnosis: benefits, issues & solutions'}], 'Patient Assessment AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689880/', 'p': u"This paper introduces Lynx, an intelligent system for personal safety at home environments, oriented to elderly people living independently, which encompasses a decision support machine for automatic home risk prevention, tested in real-life environments to respond to real time situations. The automatic system described in this paper prevents such risks by an advanced analytic methods supported by an expert knowledge system. It is minimally intrusive, using plug-and-play sensors and machine learning algorithms to learn the elder's daily activity taking into account even his health records. If the system detects that something unusual happens (in a wide sense) or if something is wrong relative to the user's health habits or medical recommendations, it sends at real-time alarm to the family, care center, or medical agents, without human intervention. The system feeds on information from sensors deployed in the home and knowledge of subject physical activities, which can be collected by mobile applications and enriched by personalized health information from clinical reports encoded in the system. The system usability and reliability have been tested in real-life conditions, with an accuracy larger than 81%. The ultimate goal of automatic summaries is to feed the telecare platform with the most relevant clinical data obtained by an unassisted way from medical summaries and move the therapeutics procedures, treatments, or medical recommendations from medical summaries to universAAL ontology. So, the anomaly engine and the predicting intentions engine are capable of learning about personal living habits of the patients, highly correlated with their clinical conditions and prescriptions. To build this integration, thanks to semantic annotation process, we need only to join the most relevant concepts (principal diseases, diagnosis, treatment, and procedures) in the control platform (see Figure 9) with a \u201csame_as\u201d link between ontologies, by the paradigm of Linked Data recommendations [39].", 'kwd': '-', 'title': u'Lynx: Automatic Elderly Behavior Prediction in Home Telecare'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5289061/', 'p': u'In efforts to develop reliable methods to detect the likelihood of impending suicidal behaviors, we have proposed the following.To gain a deeper understanding of the state of suicide risk by determining the combination of variables that distinguishes between groups with and without suicide risk.', 'kwd': u'suicide, affective disorders, artificial intelligence, risk factors, protective factors', 'title': u'Acute Mental Discomfort Associated with Suicide Behavior in a Clinical Sample of Patients with Affective Disorders: Ascertaining Critical Variables Using Artificial Intelligence Tools'}], 'Risk Stratification AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4739399/', 'p': u'Estimating patient risk of future emergency department (ED) revisits can guide the allocation of resources, e.g. local primary care and/or specialty, to better manage ED high utilization patient populations and thereby improve patient life qualities.We set to develop and validate a method to estimate patient ED revisit risk in the subsequent 6\xa0months from an ED discharge date. An ensemble decision-tree-based model with Electronic Medical Record (EMR) encounter data from HealthInfoNet (HIN), Maine\u2019s Health Information Exchange (HIE), was developed and validated, assessing patient risk for a subsequent 6\xa0month return ED visit based on the ED encounter-associated demographic and EMR clinical history data. A retrospective cohort of 293,461 ED encounters that occurred between January 1, 2012 and December 31, 2012, was assembled with the associated patients\u2019 1-year clinical histories before the ED discharge date, for model training and calibration purposes. To validate, a prospective cohort of 193,886 ED encounters that occurred between January 1, 2013 and June 30, 2013 was constructed.', 'kwd': u'ED revisit prediction, Prospective validation, Statistical modeling, EMR', 'title': u'Prospective stratification of patients at risk for emergency department revisit: resource utilization and population management strategy implications'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5382897/', 'p': u'Deep sternal wound infection following coronary artery bypass grafting is a\nserious complication associated with significant morbidity and mortality.\nDespite the substantial impact of deep sternal wound infection, there is a\nlack of specific risk stratification tools to predict this complication\nafter coronary artery bypass grafting. This study was undertaken to develop\na specific prognostic scoring system for the development of deep sternal\nwound infection that could risk-stratify patients undergoing coronary artery\nbypass grafting and be applied right after the surgical procedure.Between March 2007 and August 2016, continuous, prospective surveillance data\non deep sternal wound infection and a set of 27 variables of 1500 patients\nwere collected. Using binary logistic regression analysis, we identified\nindependent predictors of deep sternal wound infection. Initially we\ndeveloped a predictive model in a subset of 500 patients. Dataset was\nexpanded to other 1000 consecutive cases and a final model and risk score\nwere derived. Calibration of the scores was performed using the\nHosmer-Lemeshow test.', 'kwd': u'Coronary Artery Bypass, Wound Infection, Risk Assessment/Methods', 'title': u'Development and Validation of a Stratification Tool for Predicting\nRisk of Deep Sternal Wound Infection after Coronary Artery Bypass Grafting at a\nBrazilian Hospital'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5627253/', 'p': u'The accurate assessment of a patient\u2019s risk of adverse events remains a mainstay of clinical care. Commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history, presenting signs and symptoms, and lab values. More sophisticated methods, such as Artificial Neural Networks (ANN), form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data, yielding classifiers with improved performance. Using two cohorts consisting of patients admitted with a non-ST-segment elevation acute coronary syndrome, we constructed an ANN that identifies patients at high risk of cardiovascular death (CVD). The ANN was trained and tested using patient subsets derived from a cohort containing 4395 patients (Area Under the Curve (AUC) 0.743) and validated on an independent holdout set containing 861 patients (AUC 0.767). The ANN 1-year Hazard Ratio for CVD was 3.72 (95% confidence interval 1.04\u201314.3) after adjusting for the TIMI Risk Score, left ventricular ejection fraction, and B-type natriuretic peptide. A unique feature of our approach is that it captures small changes in the ST segment over time that cannot be detected by visual inspection. These findings highlight the important role that ANNs can play in risk stratification.The study populations consisted of two patient cohorts (Table\xa01) that were derived from two different clinical studies used in previous work to evaluate the performance of several computational biomarkers14,17,18. The first study19,20 included 6,560 patients with interpretable continuous ECG data, and the second study21 included 990 patients with interpretable ECG data. All patients in both cohorts were enrolled, and ECG collection began, within 48\u2009hours after presenting with symptoms consistent with a NSTE-ACS. From these datasets, we restricted our analysis to patients who had at least one day of continuous ECG signal and values for seven baseline characteristics: age, gender, current smoker, history of hypertension, history of diabetes, previous myocardial infarction (MI), and a history of previous angiography. These features correspond to the subset of features that were in common to both cohorts. Using these criteria, 4,395 patients from Cohort-1 and 861 patients from Cohort-2 were used in the analysis.\n', 'kwd': '-', 'title': u'Machine Learning Improves Risk Stratification After Acute Coronary Syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5596298/', 'p': u'To improve health outcomes and cut health care costs, we often need to conduct prediction/classification using large clinical datasets (aka, clinical big data), for example, to identify high-risk patients for preventive interventions. Machine learning has been proposed as a key technology for doing this. Machine learning has won most data science competitions and could support many clinical activities, yet only 15% of hospitals use it for even limited purposes. Despite familiarity with data, health care researchers often lack machine learning expertise to directly use clinical big data, creating a hurdle in realizing value from their data. Health care researchers can work with data scientists with deep machine learning knowledge, but it takes time and effort for both parties to communicate effectively. Facing a shortage in the United States of data scientists and hiring competition from companies with deep pockets, health care systems have difficulty recruiting data scientists. Building and generalizing a machine learning model often requires hundreds to thousands of manual iterations by data scientists to select the following: (1) hyper-parameter values and complex algorithms that greatly affect model accuracy and (2) operators and periods for temporally aggregating clinical attributes (eg, whether a patient\u2019s weight kept rising in the past year). This process becomes infeasible with limited budgets.This study\u2019s goal is to enable health care researchers to directly use clinical big data, make machine learning feasible with limited budgets and data scientist resources, and realize value from data.', 'kwd': u'machine learning, automated temporal aggregation, automatic model selection, care management, clinical big data', 'title': u'Automating Construction of Machine Learning Models With Clinical Big Data: Proposal Rationale and Methods'}], 'Patient Assessment AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5426358/', 'p': '-', 'kwd': u'observational measurement, computational behavioral science, affective computing, contemporary validity theory, inter-observer reliability', 'title': u'A Primer on Observational Measurement'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410094/', 'p': '-', 'kwd': u'Raven\u2019s Standard Progressive Matrices, intelligence testing, mental abstraction, analogical reasoning, scale reduction, predictive model', 'title': u'Development of Abbreviated Nine-item Forms of the Raven\u2019s Standard Progressive Matrices Test'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4099207/', 'p': u"Dedicated, automatic algorithms for image analysis and processing are becoming more and more common in medical diagnosis. When creating dedicated algorithms, many factors must be taken into consideration. They are associated with selecting the appropriate algorithm parameters and taking into account the impact of data acquisition on the results obtained. An important feature of algorithms is the possibility of their use in other medical units by other operators. This problem, namely operator\u2019s (acquisition) impact on the results obtained from image analysis and processing, has been shown on a few examples.The analysed images were obtained from a variety of medical devices such as thermal imaging, tomography devices and those working in visible light. The objects of imaging were cellular elements, the anterior segment and fundus of the eye, postural defects and others. In total, almost 200'000 images coming from 8 different medical units were analysed. All image analysis algorithms were implemented in C and Matlab.", 'kwd': u'Image processing, Expert, Operator, Measurement automation, Error, Segmentation, Ultrasound, Cosmetology, Microscope, Cornea', 'title': u'Quantitative assessment of the impact of biomedical image acquisition on the results obtained from image analysis and processing'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3782694/', 'p': u'Medical image processing provides core innovation for medical imaging. This paper is focused on recent developments from science to applications analyzing the past fifteen years of history of the proceedings of the German annual meeting on medical image processing (BVM). Furthermore, some members of the program committee present their personal points of views: (i) multi-modality for imaging and diagnosis, (ii) analysis of diffusion-weighted imaging, (iii) model-based image analysis, (iv) registration of section images, (v) from images to information in digital endoscopy, and (vi) virtual reality and robotics. Medical imaging and medical image computing is seen as field of rapid development with clear trends to integrated applications in diagnostics, treatment planning and treatment. The author(s) confirm that this article content has no conflict of interest.', 'kwd': u'Medical imaging, Image processing, Image analysis, Visualization, Multi-modal imaging, Diffusion-weighted imaging, Model-based imaging, Registration, Digital endoscopy, Virtual reality, Robotics.', 'title': u'Viewpoints on Medical Image Processing: From Science to Application'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5463621/', 'p': u'Accurate tumor segmentation from PET images is crucial in many radiation\noncology applications. Among others, partial volume effect (PVE) is recognized\nas one of the most important factors degrading imaging quality and segmentation\naccuracy in PET. Taking into account that image restoration and tumor\nsegmentation are tightly coupled and can promote each other, we proposed a\nvariational method to solve both problems simultaneously in this study. The\nproposed method integrated total variation (TV) semi-blind de-convolution and\nMumford-Shah segmentation with multiple regularizations. Unlike many existing\nenergy minimization methods using either TV or L2\nregularization, the proposed method employed TV regularization over tumor edges\nto preserve edge information, and L2 regularization\ninside tumor regions to preserve the smooth change of the metabolic uptake in a\nPET image. The blur kernel was modeled as anisotropic Gaussian to address the\nresolution difference in transverse and axial directions commonly seen in a\nclinic PET scanner. The energy functional was rephrased using the\n\u0393-convergence approximation and was iteratively\noptimized using the alternating minimization (AM) algorithm. The performance of\nthe proposed method was validated on a physical phantom and two clinic datasets\nwith non-Hodgkin\u2019s lymphoma and esophageal cancer, respectively.\nExperimental results demonstrated that the proposed method had high performance\nfor simultaneous image restoration, tumor segmentation and scanner blur kernel\nestimation. Particularly, the recovery coefficients (RC) of the restored images\nof the proposed method in the phantom study were close to 1, indicating an\nefficient recovery of the original blurred images; for segmentation the proposed\nmethod achieved average dice similarity indexes (DSIs) of 0.79 and 0.80 for two\nclinic datasets, respectively; and the relative errors of the estimated blur\nkernel widths were less than 19% in the transversal direction and\n7% in the axial direction.Each iteration of the alternating minimization of the proposed method\nconsists of two gradient descent algorithms and two bisection methods. In the\nwhole iterative process, the most time-consuming part was the gradient descent\nalgorithm due to its disadvantage of the slow convergence. For an image with\nsize of\nN\xd7N\xd7N, the\ncomputational complexity of our algorithm is\nO(M\xd7N3),\nwhere M denotes the iteration number of the AM algorithm.\nFortunately, tumor segmentation in radiation oncology was generally conducted in\na ROI with small size (small N) rather than the whole PET\nimage. Our current algorithm was implemented in Matlab, and the typical\nexecution time was about several minutes for one patient in our experiments\n(3.40 GHz CPU, 8GB RAM). The current code still has room to speed up.', 'kwd': u'image restoration, tumor segmentation, blur kernel estimation, variational method, TV regularization, L2 regularization', 'title': u'Simultaneous Tumor Segmentation, Image Restoration, and Blur Kernel\nEstimation in PET Using Multiple Regularizations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4241832/', 'p': u'Compressed sensing (CS)-based iterative reconstruction (IR) techniques are able to reconstruct cone-beam CT (CBCT) images from undersampled noisy data, allowing for imaging dose reduction. However, there are a few practical concerns preventing the clinical implementation of these techniques. On the image quality side, data truncation along the superior\u2013inferior direction under the cone-beam geometry produces severe cone artifacts in the reconstructed images. Ring artifacts are also seen in the half-fan scan mode. On the reconstruction efficiency side, the long computation time hinders clinical use in image-guided radiation therapy (IGRT).Image quality improvement methods are proposed to mitigate the cone and ring image artifacts in IR. The basic idea is to use weighting factors in the IR data fidelity term to improve projection data consistency with the reconstructed volume. In order to improve the computational efficiency, a multiple graphics processing units (GPUs)-based CS-IR system was developed. The parallelization scheme, detailed analyses of computation time at each step, their relationship with image resolution, and the acceleration factors were studied. The whole system was evaluated in various phantom and patient cases.', 'kwd': u'iterative reconstruction, low-dose, CBCT, artifact, GPU', 'title': u'Towards the clinical implementation of iterative low-dose cone-beam CT reconstruction in image-guided radiation therapy: Cone/ring artifact correction and multiple GPU implementation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5123388/', 'p': '-', 'kwd': '-', 'title': u'5th International Symposium on Focused Ultrasound'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5374646/', 'p': u'When exposed to sufficiently high ultrasound pressures, microbubbles can be generated spontaneously in tissue and undergo inertial cavitation where collapses result in physical effects. These effects range from petechial haemorrhage to complete cellular disruption, termed Histotripsy, depending on ultrasound parameters. This presentation will explore the mechanisms associated with histotripsy along with the tissue effects and the wide range of potential applications for this mechanical disruption method.In this lecture, we will compare the design of clinical trials that led to approval of MR guided focused ultrasound for the treatment of uterine fibroids and osseous metastases. The impact of these trials on the evidence base, and thus on adoption by users and coverage by insurers will be compared. We will also review the process of expanding approved FUS applications, either via investigator- or industry-initiated studies or through off-label clinical use.', 'kwd': '-', 'title': u'International Society for Therapeutic Ultrasound Conference 2016'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2816803/', 'p': u'Cardiac magnetic resonance (CMR) imaging enables accurate and reproducible quantification of measurements of global and regional ventricular function, blood flow, perfusion at rest and stress as well as myocardial injury. Recent advances in MR hardware and software have resulted in significant improvements in image quality and a reduction in imaging time. Methods for automated and robust assessment of the parameters of cardiac function, blood flow and morphology are being developed. This article reviews the recent advances in image acquisition and quantitative image analysis in CMR.The diagnosis and management of cardiac disease requires a precise assessment of the parameters of cardiac morphology and function. Cardiac magnetic resonance (CMR) imaging has shown to be a versatile non-invasive imaging modality providing accurate and reproducible assessment of global and ventricular regional function, blood flow, myocardial perfusion and myocardial scar. In addition to enhancing clinical decision making, the accuracy and reproducibility of the CMR quantitative measures of cardiac function and morphology allow research studies to be carried out with fewer subjects enhancing cost effectiveness. Significant recent advances have been made in the generation of new CMR acquisition protocols as well as MR hardware enabling more rapid image acquisition. Despite these advances, the quantitative analysis of the images often still relies on manual tracing of the contours in many images, a time-consuming process. Reliable automated or semi-automated image segmentation and analysis software allowing for reproducible and rapid quantification are under development. In this paper an overview is provided on some of the recent work that has been carried out on image acquisition, computerized quantitative image analysis methods and semi-automated contour detection software for CMR imaging. The emerging clinical applications of quantitative CMR parameters are highlighted.', 'kwd': u'Cardiac MRI, Quantification', 'title': u'Quantification in cardiac MRI: advances in image acquisition and processing'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4943498/', 'p': u'Schizophrenia is a serious and chronic mental illness which has a profound effect on the health and well-being related with the well-known nature of psychotic symptoms. The exercise has the potential to improve the life of people with schizophrenia improving physical health and alleviating psychiatric symptoms. However, most people with schizophrenia remains sedentary and lack of access to exercise programs are barriers to achieve health benefits. The aim of this study is to evaluate the effect of exercise on I) the type of intervention in mental health, II) in salivary levels of alpha-amylase and cortisol and serum levels of S100B and BDNF, and on III) the quality of life and self-perception of the physical domain of people with schizophrenia. The sample consisted of 31 females in long-term institutions in the Casa de Sa\xfade Rainha Santa Isabel, with age between 25 and 63, and with diagnosis of schizophrenia according to the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR). Physical fitness was assessed by the six-minute walk distance test (6MWD). Biological variables were determined by ELISA (Enzyme-Linked Immunosorbent Assay). Psychological variables were assessed using SF-36, PSPP-SCV, RSES and SWLS tests. Walking exercise has a positive impact on physical fitness (6MWD \u2013 p\u2009=\u20090.001) and physical components of the psychological tests ([SF-36] physical functioning p\u2009<\u20090.05; [PSPP-SCV] functionality p\u2009<\u20090.05 and SWLS p\u2009<\u20090.05 of people with schizophrenia. The walking program enhances the quality of life and self-perception of the physical domain and physical fitness of people with schizophrenia.\u115f', 'kwd': '-', 'title': u'Proceedings of the 3rd IPLeiria\u2019s International Health Congress'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4793253/', 'p': u'Cardiac computed tomography (CCT) is a reliable and accurate tool for diagnosis of coronary artery diseases and is also frequently used in surgery guidance. Low-dose scans should be considered in order to alleviate the harm to patients caused by X-ray radiation. However, low dose CT (LDCT) images tend to be degraded by quantum noise and streak artifacts. In order to improve the cardiac LDCT image quality, a 3D sparse representation-based processing (3D SR) is proposed by exploiting the sparsity and regularity of 3D anatomical features in CCT. The proposed method was evaluated by a clinical study of 14 patients. The performance of the proposed method was compared to the 2D spares representation-based processing (2D SR) and the state-of-the-art noise reduction algorithm BM4D. The visual assessment, quantitative assessment and qualitative assessment results show that the proposed approach can lead to effective noise/artifact suppression and detail preservation. Compared to the other two tested methods, 3D SR method can obtain results with image quality most close to the reference standard dose CT (SDCT) images.Both the 2D SR method and the proposed 3D SR method include a dictionary training step and an OMP step. In our experiment, it takes 6.27\u2009seconds to train a dictionary for the general 2D SR method. Since the dictionary size (512\u2009\xd7\u20091000) and the training set are much larger than 2D SR method, the training step in 3D SR processing is rather computational intensive and takes about 217.67\u2009seconds. Fortunately, the dictionaries, once trained, can be used to process all the LDCT cases as demonstrated in the above experiments (refer also to41). Table 4 lists the average computation cost required in the operations following the dictionary training only. We can see that, with the trained dictionary available, the 2D SR method requires about 0.90\u2009seconds (in average) to process one 512\u2009\xd7\u2009512 slice; the BM4D method takes 3703.74\u2009seconds (in average) to process one 512\u2009\xd7\u2009512\u2009\xd7\u2009270 CT data set (about 13.72\u2009seconds per 2-D slice) and the 3D SR method requires 843.67\u2009seconds (in average) to process one 512\u2009\xd7\u2009512\u2009\xd7\u2009270 CT data set (about 3.12\u2009seconds per 2-D slice).', 'kwd': '-', 'title': u'Improving Low-dose Cardiac CT Images based on 3D Sparse Representation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4364461/', 'p': '-', 'kwd': '-', 'title': u'Abstracts for the 15th International Congress on Schizophrenia Research (ICOSR)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4059792/', 'p': u'To evaluate different susceptibility weighted imaging (SWI) phase processing methods and parameter selection, thereby improving understanding of potential artifacts, as well as facilitating choice of methodology in clinical settings.Two major phase processing methods, Homodyne-filtering and phase unwrapping-high pass (HP) filtering, were investigated with various phase unwrapping approaches, filter sizes, and filter types. Magnitude and phase images were acquired from a healthy subject and brain injury patients on a 3T clinical Siemens MRI system. Results were evaluated based on image contrast to noise ratio and presence of processing artifacts.', 'kwd': u'susceptibility weighted imaging, homodyne filter, phase unwrapping, image contrast', 'title': u'Quantitative assessment of susceptibility weighted imaging processing methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3611729/', 'p': u'To compare two methods for assessment of image-processing algorithms in digital mammography: free-response receiver operating characteristic (FROC) for the specific task of microcalcification detection and visual grading analysis (VGA).The FROC study was conducted prior to the VGA study reported here. 200 raw data files of low breast density (Breast Imaging\u2013Reporting and Data System I\u2013II) mammograms (Novation DR, Siemens, Germany)\u2014100 of which abnormal\u2014were processed by four image-processing algorithms: Raffaello (IMS, Bologna, Italy), Sigmoid (Sectra, Link\xf6ping, Sweden), and OpView v. 2 and v. 1 (Siemens, Erlangen, Germany). Four radiologists assessed the mammograms for the detection of microcalcifications. 8 months after the FROC study, a subset (200) of the 800 images was reinterpreted by the same radiologists, using the VGA methodology in a side-by-side approach. The VGA grading was based on noise, saturation, contrast, sharpness and confidence with the image in terms of normal structures. Ordinal logistic regression was applied; OpView v. 1 was the reference processing algorithm.', 'kwd': '-', 'title': u'Comparison of visual grading and free-response ROC analyses for assessment of image-processing algorithms in digital mammography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3893567/', 'p': u"Nonrigid image registration is an important, but time-consuming task in medical image analysis. In typical neuroimaging studies, multiple image registrations are performed, i.e., for atlas-based segmentation or template construction. Faster image registration routines would therefore be beneficial. In this paper we explore acceleration of the image registration package elastix by a combination of several techniques: (i) parallelization on the CPU, to speed up the cost function derivative calculation; (ii) parallelization on the GPU building on and extending the OpenCL framework from ITKv4, to speed up the Gaussian pyramid computation and the image resampling step; (iii) exploitation of certain properties of the B-spline transformation model; (iv) further software optimizations. The accelerated registration tool is employed in a study on diagnostic classification of Alzheimer's disease and cognitively normal controls based on T1-weighted MRI. We selected 299 participants from the publicly available Alzheimer's Disease Neuroimaging Initiative database. Classification is performed with a support vector machine based on gray matter volumes as a marker for atrophy. We evaluated two types of strategies (voxel-wise and region-wise) that heavily rely on nonrigid image registration. Parallelization and optimization resulted in an acceleration factor of 4\u20135x on an 8-core machine. Using OpenCL a speedup factor of 2 was realized for computation of the Gaussian pyramids, and 15\u201360 for the resampling step, for larger images. The voxel-wise and the region-wise classification methods had an area under the receiver operator characteristic curve of 88 and 90%, respectively, both for standard and accelerated registration. We conclude that the image registration package elastix was substantially accelerated, with nearly identical results to the non-optimized version. The new functionality will become available in the next release of elastix as open source under the BSD license.Data from the ADNI1 database was used. The ADNI cohort used for our experiments is adopted from the study of Cuingnet et al. (2011), from which we selected the AD patient group and the normal elderly control group. The inclusion criteria for participants were defined in the ADNI GO protocol (www.adni-info.org/Scientists/AboutADNI.aspx\\#). The patient group consisted of 137 patients (67 males, age = 76.0 \xb1 7.3 years, Mini Mental State Examination (MMSE) score = 23.2 \xb1 2.0), and the control group of 162 participants (76 males, age = 76.3 \xb1 5.4 years, MMSE = 29.2 \xb1 1.0). The participants were randomly split into two groups of the same size, a training set and a test set, while preserving the age and sex distribution (Cuingnet et al., 2011). Structural MRI (T1w) data were acquired at 1.5T according to the ADNI acquisition protocol (Jack et al., 2008).", 'kwd': u"image registration, parallelization, acceleration, OpenCL, elastix, Alzheimer's disease", 'title': u"Fast parallel image registration on CPU and GPU for diagnostic classification of Alzheimer's disease"}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3785070/', 'p': u'Optical coherence tomography (OCT) is a recently established imaging technique to describe different information about the internal structures of an object and to image various aspects of biological tissues. OCT image segmentation is mostly introduced on retinal OCT to localize the intra-retinal boundaries. Here, we review some of the important image segmentation methods for processing retinal OCT images. We may classify the OCT segmentation approaches into five distinct groups according to the image domain subjected to the segmentation algorithm. Current researches in OCT segmentation are mostly based on improving the accuracy and precision, and on reducing the required processing time. There is no doubt that current 3-D imaging modalities are now moving the research projects toward volume segmentation along with 3-D rendering and visualization. It is also important to develop robust methods capable of dealing with pathologic cases in OCT imaging.It should be noted that OCT is not the only possible device for assessing retinal pathologies; the field of ophthalmology was revolutionized in 1851 with the invention of the ophthalmoscope by Hermann von Helmholtz[3] as for the first time detailed examinations of the interior of the eye could be made in living patients. Fundus photography (a low powered microscope attached with a camera),[4] fluorescein Angiography[5] (photographing the retina by injecting fluorescent dyes) and Retinal thickness analyzer (RTA)[6] are other modalities proposed for diagnosis of retinal malfunctions. The latter is capable of rapidly obtaining retinal thickness map covering an area of 3 \xd7 3 mm. The oblique projection of a narrow laser slit beam on retina and recording the backscattered light are the principles of this method.[6] Furthermore, confocal scanning laser ophthalmoscopy (CSLO)[7] provides a three-dimensional topographic representationof the optic disk and peripapillary retina, which is constructed from a series of two-dimensional slices. This three-dimensional representation consists of 256 \xd7 256 (65,536) pixel elements, each of which is a measurement of retinal height at its corresponding location. Three topography images are usually acquired in a single session and thereafter are automatically aligned and averaged to obtain a single mean topography image. Although the CSLO is similar, in many respects, to a CT scan, the light rays used for CSLO cannot penetrate tissue; this limits this modality to depicting the surface topography of the optic disk and para-papillary retina.[8]', 'kwd': u'Optical coherence tomography, retina, segmentation', 'title': u'A Review of Algorithms for Segmentation of Optical Coherence Tomography from Retina'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4318357/', 'p': u'The theory of task-based assessment of image quality is reviewed in the context of imaging with ionizing radiation, and objective figures of merit (FOMs) for image quality are summarized. The variation of the FOMs with the task, the observer and especially with the mean number of photons recorded in the image is discussed. Then various standard methods for specifying radiation dose are reviewed and related to the mean number of photons in the image and hence to image quality. Current knowledge of the relation between local radiation dose and the risk of various adverse effects is summarized, and some graphical depictions of the tradeoffs between image quality and risk are introduced. Then various dose-reduction strategies are discussed in terms of their effect on task-based measures of image quality.Popescu and Myers [32] recently published a paper describing a signal-search paradigm, along with a phantom design, that enables the evaluation and comparison of iterative reconstruction algorithms in terms of signal detectability vs dose performance using a few tens of images. Standard commercial phantoms for CT quality assessment have limited utility for quantitative evaluation of image quality, because of the limited area available for deriving the statistics of a model observer in the signal-absent condition. Moreover, such phantoms yield only one signal size/contrast combination per image. Phantoms that allow for multiple realizations of background-only and signal-present regions in each image, along with the addition of search as a component of the detection task, enable the investigator to customize the signal detectability level through adjustment of the search region area so that meaningful image quality comparisons can be made across algorithms or imaging systems with a fairly small number of images.', 'kwd': '-', 'title': u'Task-based measures of image quality and their relation to radiation dose and patient risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5116630/', 'p': u'Neuropsychological assessment of human visual processing capabilities strongly depends on visual testing conditions including room lighting, stimuli, and viewing-distance. This limits standardization, threatens reliability, and prevents the assessment of core visual functions such as visual processing speed. Increasingly available virtual reality devices allow to address these problems. One such device is the portable, light-weight, and easy-to-use Oculus Rift. It is head-mounted and covers the entire visual field, thereby shielding and standardizing the visual stimulation. A fundamental prerequisite to use Oculus Rift for neuropsychological assessment is sufficient test-retest reliability. Here, we compare the test-retest reliabilities of Bundesen\u2019s visual processing components (visual processing speed, threshold of conscious perception, capacity of visual working memory) as measured with Oculus Rift and a standard CRT computer screen. Our results show that Oculus Rift allows to measure the processing components as reliably as the standard CRT. This means that Oculus Rift is applicable for standardized and reliable assessment and diagnosis of elementary cognitive functions in laboratory and clinical settings. Oculus Rift thus provides the opportunity to compare visual processing components between individuals and institutions and to establish statistical norm distributions.Author Contributions R.M.F. and C.H.P. share the first authorship. R.M.F. designed the experiment, interpreted and discussed the results, and wrote the manuscript, C.H.P. designed the experiment, modified the combiTVA code for the CRT assessment, performed the data analysis, interpreted and discussed the results, prepared the supplementary information, and wrote the manuscript. C.B. programmed the Oculus Rift combiTVA test, conducted the experiment, and interpreted and discussed the results, M.B. supervised the Oculus Rift programming, interpreted and discussed the results, and wrote the manuscript, W.X.S. suggested the research question, supervised the empirical work, interpreted and discussed the results, and revised the manuscript.', 'kwd': '-', 'title': u'Using the virtual reality device Oculus Rift for neuropsychological assessment of visual processing capabilities'}], 'Patient Assessment AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5233461/', 'p': u'Digital pathology and microscopy image analysis is widely used for comprehensive studies of cell morphology or tissue structure. Manual assessment is labor intensive and prone to inter-observer variations. Computer-aided methods, which can significantly improve the objectivity and reproducibility, have attracted a great deal of interest in recent literatures. Among the pipeline of building a computer-aided diagnosis system, nucleus or cell detection and segmentation play a very important role to describe the molecular morphological information. In the past few decades, many efforts have been devoted to automated nucleus/cell detection and segmentation. In this review, we provide a comprehensive summary of the recent state-of-the-art nucleus/cell segmentation approaches on different types of microscopy images including bright-field, phase-contrast, differential interference contrast (DIC), fluorescence, and electron microscopies. In addition, we discuss the challenges for the current methods and the potential future work of nucleus/cell detection and segmentation.Random walk is a graph-based K-way image segmentation approach given a small number of user-defined labels [289], in which one graph edge weight represents the likelihood that a random walker will cross that edge. Specifically, the algorithm can be summarized in four steps: 1) construct the graph and calculate the edge weight with an intensity similarity-based weighting function, 2) obtain seeded nodes with K labels, 3) compute the probability of each label in every node by solving a combinatorial Dirichlet problem, and 4) assign each node the label associated with the largest probability to obtain image segmentation. The random walk algorithm is applied to joint segmentation of nuclei and cytoplasm of Pap smear cells in [290], which consists of three major stages: 1) extract object edges with Sobel operator, 2) enhance the edges with a maximum gray-level gradient difference method, and 3) refine edges with random walk, which can separate overlapping objects. A fast random walker algorithm is presented in [291] for interactive blood smear cell segmentation, which improves the running time using offline precomputing. However, these random walk algorithms rely on interactive seeding for good segmentation. An automatically seed generation method for random walker in presented in [292], which could be extended to nucleus or cell segmentation in digital pathology or microscopy images.', 'kwd': u'Microscopy images, digital pathology, histopathology, nucleus, cell, detection, segmentation', 'title': u'Robust Nucleus/Cell Detection and Segmentation in Digital Pathology and Microscopy Images: A Comprehensive Review'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4521615/', 'p': u'Our aim is to review and explain the capabilities and performance of currently available approaches for segmentation of lungs with pathologic conditions on chest CT images, with illustrations to give radiologists a better understanding of potential choices for decision support in everyday practice.In consideration of the anatomic variabilities in the clinical data, no single segmentation method can provide a generic solution to be used in clinical practice. Therefore, recently developed practical applications have concentrated on intelligently concatenating multiple segmentation strategies to provide a global generic solution. For instance, Mansoor et al (25) proposed a novel pathologic lung segmentation method that couples region-based approaches with neighboring anatomy constraints and a machine learning\u2013based pathology recognition system for the delineation of lung fields. The proposed framework works in multiple stages; during stage 1, a modified fuzzy connectedness segmentation algorithm (a region-based segmentation approach) is used to perform the initial lung parenchyma extraction. During the second stage, texture-based local features are used to segment abnormal imaging patterns (consolidations, ground glass, interstitial thickening, tree-in-bud pattern, honeycombing, nodules, and micronodules) that are missed during the first stage of the algorithm. This refinement stage is further complemented by a neighboring anatomy\u2013guided segmentation approach to include abnormalities that are texturally similar to neighboring organs or pleura regions. Therein, texture means spatial arrangement of an image or repeated patterns in a region of image. Although hybrid methods are feasible when varying numbers or types of abnormalities exist in the lungs, creating the class labels in the machine learning part of the hybrid methods, as well as their parameter training, is not trivial. In another example, Hua et al (54) used a graph-search algorithm to find anatomic constraints such as ribs and then constrained the graph-cut algorithm for finding the lung boundary. There are also initial preprocessing steps in which possible pathologic regions are detected and included in the segmentation. However, this approach may require accurately defined seed sets for identifying background and foreground objects.', 'kwd': '-', 'title': u'Segmentation and Image Analysis of Abnormal Lungs at CT: Current Approaches, Challenges, and Future Trends'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4402572/', 'p': u"Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain's anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation.It can be noted from the 1D histogram of the bias-corrected T1-W MRI of an adult brain in Figure 8(a) that there is an overlap between different tissue classes. Also, it can be seen that an overlap between WM and GM tissue is higher than between GM and CSF. This overlap between the class distributions can cause ambiguities in the decision boundaries when intensity-based segmentation methods are used [21]. However, many researchers showed that adding additional MRI sequences with different contrast properties (e.g., T2-W MRI, Proton Density MRI) can improve intensity-based segmentation and help separate the class distributions [22\u201324]; see Figure 13.", 'kwd': '-', 'title': u'MRI Segmentation of the Human Brain: Challenges, Methods, and Applications'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049832/', 'p': u'The dataset contains 18 T1-weighted MR brain images and their manual segmentations. The images provided by IBSR have been normalized into the Talairach space (rotation only) and preprocessed by intensity inhomogeneity correction routines. The images have slice thickness of 1.5 mm with in-plane resolution varying between 1.0 mm \xd7 1.0 mm and 0.84 mm \xd7 0.84 mm. The manual segmentations contain labels for gray matter, white matter and the ventricles. Notably, cerebrospinal fluid (CSF) outside of the ventricles is assigned the gray matter label in the IBSR segmentations.BET (Smith, 2002) uses a deformable model to separate the brain from other tissues in MR images. In our experiments, BET was applied with the default parameters to segment each of the 18 brain images from IBSR. The EC method was used to improve the accuracy of brain extraction relative to the brain masks in IBSR. 10 cross-validation experiments were performed. For each cross-validation evaluation, 9 subjects were randomly selected for training the EC method, and the remaining 9 for testing. The brain volumes have millions of voxels, posing a challenge for the AdaBoost learning, which requires loading all data in memory for efficient learning. For efficiency, we randomly selected 1% voxels uniformly from the working ROIs for training.', 'kwd': u'medical image segmentation, error correction, AdaBoost, hippocampal segmentation, brain extraction, brain tissue segmentation', 'title': u'A Learning-Based Wrapper Method to Correct Systematic Errors in Automatic Image Segmentation: Consistently Improved Performance in Hippocampus, Cortex and Brain Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4060809/', 'p': u'Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results.The basic concept of PET is to label a radio-pharmaceutical compound with a biologically active ligand to form a radiotracer and inject it intravenously into a patient. The PET scanner then measures the distribution and concentration of the radiotracer accumulation throughout the patient\u2019s body as a function of time [12]. To do this, PET utilizes positron emitting radioisotopes as molecular probes so the biochemical process can be measured through imaging in vivo [13]. There have been many radiotracers developed and among them F DG (18F combined with deoxyglucose) is considered the radiotracer of choice in most studies [14]. Metabolically active lesions have up regulation of glucose metabolism. For example, the rapid cell division in cancer cases and the immune response in infectious diseases require high levels of glucose. Therefore, labeling glucose with 18F renders these lesions detectable using PET imaging because the FDG accumulates in these areas [14]. Meanwhile, a large number of new compounds are also becoming prospects for PET imaging which have some advantages over FDG such as tracers that do not accumulate in the heart/kidney. However, FDG still remains the most commonly used radiotracer in the clinical routine for body imaging [15]. At the time of this writing, there is no reported study in the literature mimicking the differences of segmentation accuracy caused by using different radiotracers in PET imaging. Therefore, in this manuscript, an evaluation of image segmentation methods are assumed to be independent of the choice of radiotracer.', 'kwd': u'Image Segmentation, PET, SUV, Thresholding, PET-CT, MRI-PET, Review', 'title': u'A Review on Segmentation of Positron Emission Tomography Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5683197/', 'p': u'The MRBrainS131 (Mendrik et al., 2015) framework consists of MR images and manual segmentations from 20 patients (age, mean \xb1 standard deviation: 71 \xb1 4 years; 10 male, 10 female). The MR images were manually segmented in eight classes: white matter (WM), cortical grey matter (cGM), basal ganglia and thalami (BGT), cerebellum (CB), brain stem (BS), lateral ventricular cerebrospinal fluid (lvCSF), peripheral cerebrospinal fluid (pCSF), and WMH. Note that the MRBrainS13 challenge only includes evaluation of three combined tissue classes: white matter (including WMH), grey matter (including BGT) and CSF (pCSF and lvCSF) instead of all eight classes.Patients with type 2 diabetes mellitus and healthy controls were included from the Utrecht Diabetic Encephalopathy Study part 2 (UDES2) (Reijmer et al., 2013). The images used in MRBrainS13 were selected from the UDES2 cohort. From the UDES2 cohort we analysed images from 96 additional patients (age, mean \xb1 standard deviation: 71 \xb1 5 years; 58 male, 38 female; 51 with type 2 diabetes mellitus and 45 healthy controls). Reference segmentations of WMH were performed by manual outlining on the FLAIR images using relatively strict criteria (Brundel et al., 2014).', 'kwd': u'Brain MRI, Segmentation, White matter hyperintensities, Deep learning, Convolutional neural networks, Motion artefacts, Brain atrophy', 'title': u'Evaluation of a deep learning approach for the segmentation of brain tissues and white matter hyperintensities of presumed vascular origin in\xa0MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5463621/', 'p': u'Accurate tumor segmentation from PET images is crucial in many radiation\noncology applications. Among others, partial volume effect (PVE) is recognized\nas one of the most important factors degrading imaging quality and segmentation\naccuracy in PET. Taking into account that image restoration and tumor\nsegmentation are tightly coupled and can promote each other, we proposed a\nvariational method to solve both problems simultaneously in this study. The\nproposed method integrated total variation (TV) semi-blind de-convolution and\nMumford-Shah segmentation with multiple regularizations. Unlike many existing\nenergy minimization methods using either TV or L2\nregularization, the proposed method employed TV regularization over tumor edges\nto preserve edge information, and L2 regularization\ninside tumor regions to preserve the smooth change of the metabolic uptake in a\nPET image. The blur kernel was modeled as anisotropic Gaussian to address the\nresolution difference in transverse and axial directions commonly seen in a\nclinic PET scanner. The energy functional was rephrased using the\n\u0393-convergence approximation and was iteratively\noptimized using the alternating minimization (AM) algorithm. The performance of\nthe proposed method was validated on a physical phantom and two clinic datasets\nwith non-Hodgkin\u2019s lymphoma and esophageal cancer, respectively.\nExperimental results demonstrated that the proposed method had high performance\nfor simultaneous image restoration, tumor segmentation and scanner blur kernel\nestimation. Particularly, the recovery coefficients (RC) of the restored images\nof the proposed method in the phantom study were close to 1, indicating an\nefficient recovery of the original blurred images; for segmentation the proposed\nmethod achieved average dice similarity indexes (DSIs) of 0.79 and 0.80 for two\nclinic datasets, respectively; and the relative errors of the estimated blur\nkernel widths were less than 19% in the transversal direction and\n7% in the axial direction.Each iteration of the alternating minimization of the proposed method\nconsists of two gradient descent algorithms and two bisection methods. In the\nwhole iterative process, the most time-consuming part was the gradient descent\nalgorithm due to its disadvantage of the slow convergence. For an image with\nsize of\nN\xd7N\xd7N, the\ncomputational complexity of our algorithm is\nO(M\xd7N3),\nwhere M denotes the iteration number of the AM algorithm.\nFortunately, tumor segmentation in radiation oncology was generally conducted in\na ROI with small size (small N) rather than the whole PET\nimage. Our current algorithm was implemented in Matlab, and the typical\nexecution time was about several minutes for one patient in our experiments\n(3.40 GHz CPU, 8GB RAM). The current code still has room to speed up.', 'kwd': u'image restoration, tumor segmentation, blur kernel estimation, variational method, TV regularization, L2 regularization', 'title': u'Simultaneous Tumor Segmentation, Image Restoration, and Blur Kernel\nEstimation in PET Using Multiple Regularizations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481086/', 'p': u'Breast cancer is the most frequently diagnosed cancer in women. However, the exact cause(s) of breast cancer still remains unknown. Early detection, precise identification of women at risk, and application of appropriate disease prevention measures are by far the most effective way to tackle breast cancer. There are more than 70 common genetic susceptibility factors included in the current non-image-based risk prediction models (e.g., the Gail and the Tyrer-Cuzick models). Image-based risk factors, such as mammographic densities and parenchymal patterns, have been established as biomarkers but have not been fully incorporated in the risk prediction models used for risk stratification in screening and/or measuring responsiveness to preventive approaches. Within computer aided mammography, automatic mammographic tissue segmentation methods have been developed for estimation of breast tissue composition to facilitate mammographic risk assessment. This paper presents a comprehensive review of automatic mammographic tissue segmentation methodologies developed over the past two decades and the evidence for risk assessment/density classification using segmentation. The aim of this review is to analyse how engineering advances have progressed and the impact automatic mammographic tissue segmentation has in a clinical environment, as well as to understand the current research gaps with respect to the incorporation of image-based risk factors in non-image-based risk prediction models.Clinical evaluation has indicated that SFM and FFDM are similar in their ability to detect cancer [107]; however, FFDM is more effective at finding cancer in certain groups of the population, such as women who are premenopausal or perimenopausal, under the age of 50, and have dense breasts. This indicates that in this subgroup some anatomical regions are better visualised by FFDM than SFM. In particular, FFDM demonstrated improved image quality with significantly better depiction of the nipple, skin, pectoral muscle, and especially contrast in parenchymal and fatty tissue [108]. Note that digital mammography imaging generates two types of images for analysis, raw (\u201cfor processing\u201d) and vendor postprocessed (\u201cfor presentation\u201d), of which postprocessed images are commonly used in clinical practice.', 'kwd': '-', 'title': u'A Review on Automatic Mammographic Density and Parenchymal Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5429849/', 'p': u'Ultrasound (US) is the most commonly used liver imaging modality worldwide. Due to its low cost, it is increasingly used in the follow-up of cancer patients with metastases localized in the liver. In this contribution, we present the results of an interactive segmentation approach for liver metastases in US acquisitions. A (semi-) automatic segmentation is still very challenging because of the low image quality and the low contrast between the metastasis and the surrounding liver tissue. Thus, the state of the art in clinical practice is still manual measurement and outlining of the metastases in the US images. We tackle the problem by providing an interactive segmentation approach providing real-time feedback of the segmentation results. The approach has been evaluated with typical US acquisitions from the clinical routine, and the datasets consisted of pancreatic cancer metastases. Even for difficult cases, satisfying segmentations results could be achieved because of the interactive real-time behavior of the approach. In total, 40 clinical images have been evaluated with our method by comparing the results against manual ground truth segmentations. This evaluation yielded to an average Dice Score of 85% and an average Hausdorff Distance of 13 pixels.Ultrasound acquisitions have been performed by using a multifrequency curved probe. The US probes LOGIQ E9 and Aplio 80 from GE Healthcare (Milwaukee, Il, USA) and Toshiba (Otawara, Japan), respectively, allowed image scans with a bandwidth of one to six MHz. Fulfilling the following criteria, images have been selected retrospectively from the digital picture archive of the Katharinenhospital Stuttgart (Germany) ultrasound unit. Regarding the first part of the study, where the algorithm was adjusted using images with different echo pattern, ultrasound examinations of patients treated for different malignancies were chosen. The main selection criteria consisted of the echogenicity class of the lesion, which could be hypo-, hyper or isoechoic. The selection process was stopped when one lesion per echogenicity class was identified. Additional three images with metastases of different echogenicity were used in the first part of this study to visualize the performance of the algorithm. The selection of 40 images in the second part of the study for the evaluation of the algorithm was done by choosing ultrasound examinations of consecutive patients treated for metastatic pancreatic cancer. However, we excluded images were text or markers overlaid the target lesion. Afterwards, we removed the patient information from the image, and the anonymized image was segmented with our algorithm. The local ethical committee from the Katharinenhospital, Stuttgart, Germany, provided a waiver of the requirement for informed consent for this retrospective study and allowed the publication of anonymized data. Note, we added figure serial numbers to the images in the lower left corners. Cases we used to develop, test and present our algorithm in the first part of the manuscript are indicated with #T followed by a number, e.g. #T1 in in Fig.\xa02. The 40 cases we used to evaluate our algorithm are indicated with #E followed by a number from 1 to 40, e.g. #E39. The anonymized raw data can be used for own research purposes as long as our work is cited27: https://www.researchgate.net/publication/307907688_Ultrasound_Liver_Tumor_Datasets.\n', 'kwd': '-', 'title': u'Interactive Outlining of Pancreatic Cancer Liver Metastases in Ultrasound Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4000389/', 'p': u'Due to rapid advances in radiation therapy (RT), especially image guidance and treatment adaptation, a fast and accurate segmentation of medical images is a very important part of the treatment. Manual delineation of target volumes and organs at risk is still the standard routine for most clinics, even though it is time consuming and prone to intra- and interobserver variations. Automated segmentation methods seek to reduce delineation workload and unify the organ boundary definition. In this paper, the authors review the current autosegmentation methods particularly relevant for applications in RT. The authors outline the methods\u2019 strengths and limitations and propose strategies that could lead to wider acceptance of autosegmentation in routine clinical practice. The authors conclude that currently, autosegmentation technology in RT planning is an efficient tool for the clinicians to provide them with a good starting point for review and adjustment. Modern hardware platforms including GPUs allow most of the autosegmentation tasks to be done in a range of a few minutes. In the nearest future, improvements in CT-based autosegmentation tools will be achieved through standardization of imaging and contouring protocols. In the longer term, the authors expect a wider use of multimodality approaches and better understanding of correlation of imaging with biology and pathology.Among the methods which do not use prior-knowledge for segmentation, region based methods such as adaptive thresholding1 and graph cuts2 as well as edge detection based methods, e.g., watershed segmentation3 have been used for radiotherapy planning.4, 5 Moreover, different types of deformable models6 such as geodesic active contours7 have been applied.8 In order to take advantage of the flexibility of these methods and at the same time compensate their deficiency of using prior-knowledge, especially approaches based on graph cuts and deformable models are often used in combination with atlas- or model-based segmentation methods in hybrid approaches (see Sec. 2C).', 'kwd': u'segmentation, radiation therapy, image processing', 'title': u'Vision 20/20: Perspectives on automated image segmentation for radiotherapy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5609357/', 'p': u'Image segmentation pipelines often are sensitive to algorithm input parameters. Algorithm parameters optimized for a set of images do not necessarily produce good-quality-segmentation results for other images. Even within an image, some regions may not be well segmented due to a number of factors, including multiple pieces of tissue with distinct characteristics, differences in staining of the tissue, normal versus tumor regions, and tumor heterogeneity. Evaluation of quality of segmentation results is an important step in image analysis. It is very labor intensive to do quality assessment manually with large image datasets because a whole-slide tissue image may have hundreds of thousands of nuclei. Semi-automatic mechanisms are needed to assist researchers and application developers to detect image regions with bad segmentations efficiently.Our goal is to develop and evaluate a machine-learning-based semi-automated workflow to assess quality of nucleus segmentation results in a large set of whole-slide tissue images.', 'kwd': u'Classification, nuclei segmentation quality assessment, texture feature', 'title': u'A Methodology for Texture Feature-based Quality Assessment in Nucleus Segmentation of Histopathology Image'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4874930/', 'p': u'The purpose of this work was to develop, validate, and compare a highly computer-aided method for the segmentation of hot lesions in head and neck 18F-FDG PET scans.A semiautomated segmentation method was developed, which transforms the segmentation problem into a graph-based optimization problem. For this purpose, a graph structure around a user-provided approximate lesion centerpoint is constructed and a suitable cost function is derived based on local image statistics. To handle frequently occurring situations that are ambiguous (e.g., lesions adjacent to each other versus lesion with inhomogeneous uptake), several segmentation modes are introduced that adapt the behavior of the base algorithm accordingly. In addition, the authors present approaches for the efficient interactive local and global refinement of initial segmentations that are based on the \u201cjust-enough-interaction\u201d principle. For method validation, 60 PET/CT scans from 59 different subjects with 230 head and neck lesions were utilized. All patients had squamous cell carcinoma of the head and neck. A detailed comparison with the current clinically relevant standard manual segmentation approach was performed based on 2760 segmentations produced by three experts.', 'kwd': u'cancer segmentation, FDG PET imaging, graph-based segmentation, optimal surface finding, just-enough-interaction principle', 'title': u'Semiautomated segmentation of head and neck cancers in 18F-FDG PET scans: A just-enough-interaction approach'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706546/', 'p': u'Segmentation of the hippocampus from magnetic resonance (MR) images is a key task in the evaluation of mesial temporal lobe epilepsy (mTLE) patients. Several automated algorithms have been proposed although manual segmentation remains the benchmark. Choosing a reliable algorithm is problematic since structural definition pertaining to multiple edges, missing and fuzzy boundaries, and shape changes varies among mTLE subjects. Lack of statistical references and guidance for quantifying the reliability and reproducibility of automated techniques has further detracted from automated approaches. The purpose of this study was to develop a systematic and statistical approach using a large dataset for the evaluation of automated methods and establish a method that would achieve results better approximating those attained by manual tracing in the epileptogenic hippocampus.A template database of 195 (81 males, 114 females; age range 32\u201367 yr, mean 49.16 yr) MR images of mTLE patients was used in this study. Hippocampal segmentation was accomplished manually and by two well-known tools (FreeSurfer and hammer) and two previously published methods developed at their institution [Automatic brain structure segmentation (ABSS) and LocalInfo]. To establish which method was better performing for mTLE cases, several voxel-based, distance-based, and volume-based performance metrics were considered. Statistical validations of the results using automated techniques were compared with the results of benchmark manual segmentation. Extracted metrics were analyzed to find the method that provided a more similar result relative to the benchmark.', 'kwd': u'medical imaging, image processing, segmentation, hippocampus, temporal lobe epilepsy, magnetic resonance imaging (MRI)', 'title': u'Comparative performance evaluation of automated segmentation methods of hippocampus from magnetic resonance images of temporal lobe epilepsy patients'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3378724/', 'p': u'Accurate segmentation of prostate is the key to the success of external beam radiotherapy of prostate cancer. However, accurate segmentation of prostate in computer tomography (CT) images remains challenging mainly due to three factors: (1) low image contrast between the prostate and its surrounding tissues, (2) unpredictable prostate motion across different treatment days, and (3) large variations of intensities and shapes of bladder and rectum around the prostate. In this paper, an online-learning and patient-specific classification method based on the location-adaptive image context is presented to deal with all these challenging issues and achieve the precise segmentation of prostate in CT images. Specifically, two sets of location-adaptive classifiers are placed, respectively, along the two coordinate directions of the planning image space of a patient, and further trained with the planning image and also the previous-segmented treatment images of the same patient to jointly perform prostate segmentation for a new treatment image (of the same patient). In particular, each location-adaptive classifier, which itself consists of a set of sequential sub-classifiers, is recursively trained with both the static image appearance features and the iteratively-updated image context features (extracted at different scales and orientations) for better identification of each prostate region. The proposed learning-based prostate segmentation method has been extensively evaluated on 161 images of 11 patients, each with more than 9 daily treatment 3D CT images. Our method achieves the mean Dice value 0.908 and the mean \xb1 SD of average surface distance (ASD) value 1.40 \xb1 0.57 mm. Its performance is also compared with three prostate segmentation methods, indicating the best segmentation accuracy by the proposed method among all methods under comparison.Our data consists of 11 patients, each with more than 9 daily prostate CT scans, with total image count of 161. Each image has XY size 512\xd7512 and 34 to 97 slices with voxel resolution of 1mm\xd71mm\xd73mm. For each patient, the first 3 images and their manual segmentations were used to initialize the patient-specific learning process. To validate the proposed algorithm quantitatively, the expert manual segmentation results for each image were used as ground-truth. Three quantitative measures were used to evaluate the performance of our method, as listed below:\n', 'kwd': u'Radiotherapy, prostate segmentation, learning-based classification, image context', 'title': u'Learning Image Context for Segmentation of Prostate in CT-Guided Radiotherapy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5165245/', 'p': u'Image segmentation is performed independently for each time point using the cross-sectional pipeline referred to as MSmetrix-cross (Jain et al., 2015). The cross-sectional method iteratively segments the T1-weighted image into GM, WM, and CSF, segments the WM lesions on the FLAIR image as an outlier to normal brain using Mahalanobis distance, and performs lesion filling in the T1-weighted image to improve tissue segmentation at next iteration. After convergence, segmentations of WM, GM, CSF and lesions are created. In addition, bias corrected T1-weighted and FLAIR images are also produced. The segmentation tasks of the MSmetrix-cross are optimized using an EM algorithm (Van Leemput et al., 1999) as implemented in NiftySeg (Cardoso, 2012).A FLAIR based difference image is created by image co-registration and intensity normalization. Image co-registration is performed using affine registration, which comprises a rigid registration based on the whole T1-weighted image, followed by a skull based affine registration to avoid small scaling differences, and a final whole brain rigid registration (Smeets et al., 2016). The rigid registration and skull based affine registration use an inverse consistent registration algorithm (Modat et al., 2010). Subsequently, the GM, WM, CSF, lesion segmentation and the bias corrected FLAIR images obtained from the cross-sectional analysis are propagated using the final affine transformation. The matched bias corrected FLAIR images are then corrected for differential bias field as described in Lewis and Fox (2004). Subsequently, the differential bias field corrected images are intensity normalized using a cumulative histogram matching technique Castleman (1995) with the image of time point 1 as reference. A FLAIR based difference image is now created in time point 1 space. To avoid bias toward a specific time point, a second difference image is created, using time point 2 space as reference.', 'kwd': u'MSmetrix, multiple sclerosis, longitudinal lesion segmentation, expectation-maximization, MRI', 'title': u'Two Time Point MS Lesion Segmentation in Brain MRI: An Expectation-Maximization Framework'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4673359/', 'p': u'Glaucoma is the second leading cause of loss of vision in the world. Examining the head of optic nerve (cup-to-disc ratio) is very important for diagnosing glaucoma and for patient monitoring after diagnosis. Images of optic disc and optic cup are acquired by fundus camera as well as Optical Coherence Tomography. The optic disc and optic cup segmentation techniques are used to isolate the relevant parts of the retinal image and to calculate the cup-to-disc ratio. The main objective of this paper is to review segmentation methodologies and techniques for the disc and cup boundaries which are utilized to calculate the disc and cup geometrical parameters automatically and accurately to help the professionals in the glaucoma to have a wide view and more details about the optic nerve head structure using retinal fundus images. We provide a brief description of each technique, highlighting its classification and performance metrics. The current and future research directions are summarized and discussed.The Structured Analysis of Retina (STARE) dataset [22] is funded by the US National Institutes of Health. The project has 400 fundus images. Each image is diagnosis. The blood vessels are annotated in 40 images. The ONH is localized in 80 images. A TopCon TRV-50 fundus camera with 35\xb0 field of view was used to capture the images.', 'kwd': '-', 'title': u'Optic Disc and Optic Cup Segmentation Methodologies for Glaucoma Image Detection: A Survey'}], 'Angiography AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4409669/', 'p': u'The high incidence of breast cancer and growing number of breast cancer patients undergoing mastectomy has led to breast reconstruction becoming an important part of holistic treatment for these patients. In planning autologous reconstructions, preoperative assessment of donor site microvascular anatomy with advanced imaging modalities has assisted in the appropriate selection of flap donor site, individual perforators, and lead to an overall improvement in flap outcomes. In this review, we compare the accuracy of fluorescent angiography, computed tomographic angiography (CTA), and magnetic resonance angiography (MRA) and their impact on clinical outcomes.A review of the published English literature dating from 1950 to 2015 using databases, such as PubMed, Medline, Web of Science, and EMBASE was undertaken.', 'kwd': u'Breast reconstruction, indocyanine green fluorescence angiography (ICGFA), computed tomographic angiography (CTA), magnetic resonance angiography (MRA)', 'title': u'Comparative analysis of fluorescent angiography, computed tomographic angiography and magnetic resonance angiography for planning autologous breast reconstruction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5667846/', 'p': '-', 'kwd': '-', 'title': u'Multi-categorical deep learning neural network to classify retinal images: A pilot study employing small database'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5131839/', 'p': u'This article provides a comprehensive overview of imaging of frostbite injuries, including radiography, digital subtraction angiography, multiphase bone scintigraphy, and SPECT/CT, with a focus on the full range of imaging findings, their clinical relevance, and how use of these modalities can help guide care of the frostbite patient.After completing this journal-based SA-CME activity, participants will be able to:', 'kwd': '-', 'title': u'Frostbite: Spectrum of Imaging Findings and Guidelines for Management'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017267/', 'p': u'To assess the ability of trabecular micro-bypass stents to improve aqueous humor outflow (AHO) in regions initially devoid of AHO as assessed by aqueous angiography.Enucleated human eyes (14 total from 7 males and 3 females [ages 52\u201384]) were obtained from an eye bank within 48 hours of death. Eyes were oriented by inferior oblique insertion, and aqueous angiography was performed with indocyanine green (ICG; 0.4%) or fluorescein (2.5%) at 10 mm Hg. With an angiographer, infrared and fluorescent images were acquired. Concurrent anterior segment optical coherence tomography (OCT) was performed, and fixable fluorescent dextrans were introduced into the eye for histologic analysis of angiographically positive and negative areas. Experimentally, some eyes (n = 11) first received ICG aqueous angiography to determine angiographic patterns. These eyes then underwent trabecular micro-bypass sham or stent placement in regions initially devoid of angiographic signal. This was followed by fluorescein aqueous angiography to query the effects.', 'kwd': u'minimally invasive glaucoma surgery, aqueous angiography, imaging', 'title': u'Aqueous Angiography\u2013Mediated Guidance of Trabecular Bypass Improves Angiographic Outflow in Human Enucleated Eyes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5375621/', 'p': u'Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works.The assigning of a class or label to a group of pixels, such as those labeled as tumor with use of a segmentation algorithm. For instance, if segmentation has been used to mark some part of an image as \u201cabnormal brain,\u201d the classifier might then try to determine whether the marked part represents benign or malignant tissue.', 'kwd': '-', 'title': u'Machine Learning for Medical Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3267081/', 'p': u'\nAuthor contributions: Guarantors of integrity of entire study, A.S.R., R.K.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; manuscript final version approval, all authors; literature research, A.S.R., L.M.P., R.K.; clinical studies, A.S.R., R.K.; statistical analysis, A.S.R., I.K.I., L.M.P., C.F., R.H.; and manuscript editing, all authorsTo determine the effect of evidence-based clinical decision support (CDS) on the use and yield of computed tomographic (CT) pulmonary angiography for acute pulmonary embolism (PE) in the emergency department (ED).', 'kwd': '-', 'title': u'Effect of Computerized Clinical Decision Support on the Use and Yield of CT Pulmonary Angiography in the Emergency Department'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5518500/', 'p': u'Medical images play an important role in medical diagnosis and research. In this paper, a transfer learning- and deep learning-based super resolution reconstruction method is introduced. The proposed method contains one bicubic interpolation template layer and two convolutional layers. The bicubic interpolation template layer is prefixed by mathematics deduction, and two convolutional layers learn from training samples. For saving training medical images, a SIFT feature-based transfer learning method is proposed. Not only can medical images be used to train the proposed method, but also other types of images can be added into training dataset selectively. In empirical experiments, results of eight distinctive medical images show improvement of image quality and time reduction. Further, the proposed method also produces slightly sharper edges than other deep learning approaches in less time and it is projected that the hybrid architecture of prefixed template layer and unfixed hidden layers has potentials in other applications.In the learning procedure, the mapping function F requires the estimation of a set of parameters \u03b8 = {Wi, Bi}, i = 2, 3, 4. Wi and Bi are weights and biases of neurons that should be obtained as above steps mentioned, and W1 and B1 of hidden layer 1 are precalculated in Section 3.1.2. For minimizing the loss between ground-truth HR image and the reconstructed image, it can use mean squared error (MSE) to indicate the loss function:\n', 'kwd': '-', 'title': u'Deep Learning- and Transfer Learning-Based Super Resolution Reconstruction from Single Medical Image'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4261018/', 'p': u'In this paper we describe an efficient tool based on natural language processing for classifying the detail state of pulmonary embolism (PE) recorded in CT pulmonary angiography reports. The classification tasks include: PE present vs. absent, acute PE vs. others, central PE vs. others, and sub-segmental PE vs. others. Statistical learning algorithms were trained with features extracted using the NLP tool and gold standard labels obtained via chart review from two radiologists. The areas under the receiver operating characteristic curves (AUG) for the four tasks were 0.998, 0.945, 0.987, and 0.986, respectively. We compared our classifiers with bag-of-words Naive Bayes classifiers, a standard text mining technology, which gave AUG 0.942, 0.765, 0.766, and 0.712, respectively.The program counted mentions of the concepts Pulmonary Embolism, Filling defect, Clot, and Thrombosis Each mention had a semantic attribute for presence, which could be YES, NO, or MAYBE. The program counted them respectively, thus each concept gave 3 variables. When counting the NO\u2019s, we only counted those with no modifiers other than \u201cacute\u201d. For example, \u201cno other filling defects\u201d and \u201cno filling defects in the lower lobes\u201d did not count as NO because they do not imply \u201cno filling defects\u201d overall.', 'kwd': u'Natural language processing, NILE, Nested modification structure, Pulmonary embolism, CT pulmonary angiography', 'title': u'Classification of CT Pulmonary Angiography Reports by Presence, Chronicity, and Location of Pulmonary Embolism with Natural Language Processing'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4791355/', 'p': u'The role and choice of preoperative imaging for planning in breast reconstruction is still a disputed topic in the reconstructive community, with varying opinion on the necessity, the ideal imaging modality, costs and impact on patient outcomes. Since the advent of perforator flaps their use in microsurgical breast reconstruction has grown. Perforator based flaps afford lower donor morbidity by sparing the underlying muscle provide durable results, superior cosmesis to create a natural looking new breast, and are preferred in the context of radiation therapy. However these surgeries are complex; more technically challenging that implant based reconstruction, and leaves little room for error. The role of imaging in breast reconstruction can assist the surgeon in exploring or confirming flap choices based on donor site characteristics and presence of suitable perforators. Vascular anatomical studies in the lab have provided the surgeon a foundation of knowledge on location and vascular territories of individual perforators to improve our understanding for flap design and safe flap harvest. The creation of a presurgical map in patients can highlight any abnormal or individual anatomical variance to optimize flap design, intraoperative decision-making and execution of flap harvest with greater predictability and efficiency. This article highlights the role and techniques for preoperative planning using the newer technologies that have been adopted in reconstructive clinical practice: computed tomographic angiography (CTA), magnetic resonance angiography (MRA), laser-assisted indocyanine green fluorescence angiography (LA-ICGFA) and dynamic infrared thermography (DIRT). The primary focus of this paper is on the application of CTA and MRA imaging modalities.The use of thermal imaging to assess the cutaneous circulation was introduced in the 1980s but its application in preoperative planning of perforator flaps is more novel (85-88). There is limited evidence on the efficacy and comparison of this imaging modality as a tool for preoperative perforator selection in breast reconstruction. The technique is based on surface cooling followed by a period of rewarming, and during the rewarming phase, the cutaneous perfusion is analyzed with an infrared camera and hot spots correlate with perforator location. This technology is non-invasive and requires no contrast agents, however the accuracy of the this technology has been compared with Doppler (85) and findings were dependent on the pattern of rewarming. This technology, although non-invasive and with low patient risk profile, can only provide moderate and variable data on perforator location through a 2D map. Compared to the 3D architectural mapping from CTA and MRA, it is considered an inferior preoperative imaging choice. It is not widely available and although it may be associated with lower costs, there is a dearth of evidence to support its utility for preoperative planning and limited evidence intra-operatively that would support decision-making in breast reconstruction (89).', 'kwd': u'Imaging, computed tomographic angiography (CTA), magnetic resonance angiography (MRA), anatomy, technology, breast reconstruction', 'title': u'Advances in imaging technologies for planning breast reconstruction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4791345/', 'p': u'Fluorescent angiography (FA) has been useful for assessing blood flow and assessing tissue perfusion in ophthalmology and other surgical disciplines for decades. In plastic surgery, indocyanine green (ICG) dye-based FA is a relatively novel imaging technology with high potential in various applications. We review the various FA detector systems currently available and critically appraise its utility in breast reconstruction.A review of the published English literature dating from 1950 to 2015 using databases, such as PubMed, Medline, Web of Science, and EMBASE was undertaken.', 'kwd': u'Fluorescent angiography (FA), indocyanine green (ICG), fluorescein, sentinel lymph node biopsy, mastectomy skin flap, perfusion, anastomotic patency, superficial inferior epigastric artery flap (SIEA flap)', 'title': u'Indocyanine green-based fluorescent angiography in breast reconstruction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4719095/', 'p': u'The brain is a complex ecosystem, consisting of multiple layers and tissue compartments. To facilitate the understanding of its function and its response to neurological insults, a fast in vivo imaging tool with a micron-level resolution, which can provide a field of view at a few millimeters, is desirable. Optical coherence tomography (OCT) is a noninvasive method for imaging three-dimensional biological tissues with high resolution (\xa0\u223c\xa010\u2009\u2009\u03bcm) and without a need for contrast agents. Recent development of OCT-based angiography has started to shed some new light on cerebral hemodynamics in neuroscience. We give an overview of the recent developments of OCT-based imaging techniques for neuroscience applications in rodents. We summarize today\u2019s technological alternatives for OCT-based angiography for neuroscience and provide a discussion of challenges and opportunities. Moreover, a summary of OCT angiography studies for stroke, traumatic brain injury, and subarachnoid hemorrhage cases on rodents is provided.Utku Baran received his BS and MS degrees in electrical engineering from Koc University, Istanbul, Turkey, in 2010 and 2012, respectively. He is currently a PhD candidate in the Department of Electrical Engineering at the University of Washington, Seattle. He is the author/coauthor of more than 16 journal articles, and the coinventor of three pending U.S. patents. His interests include MEMS, displays, biomedical optics, and neuroscience. He is a recipient of SPIE Optics and Photonics Education Scholarship and Newport Research Excellence Travel Award.', 'kwd': u'optical coherence tomography, angiography, stroke, traumatic brain injury', 'title': u'Review of optical coherence tomography based angiography in neuroscience'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890616/', 'p': u'CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32 \xd7 32 images of 10 object classes. The objects are normally centered in the images. Some example images and class categories from the Cifar10 dataset are shown in Figure 7. CifarNet has three convolution layers, three pooling layers, and one fully-connected layer. This CNN architecture, also used in [22] has about 0.15 million free parameters. We adopt it as a baseline model for the LN detection.The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. This success has revived the interest in CNNs [3] in computer vision. ImageNet consists of 1.2 million 256 \xd7 256 images belonging to 1000 categories. At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model. More details about the ImageNet dataset will be discussed in Sec. III-B. AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters. AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.', 'kwd': '-', 'title': u'Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4754386/', 'p': '-', 'kwd': u'optical coherence tomography, angiography, image processing', 'title': u'Imaging and graphing of cortical vasculature using dynamically focused optical coherence microscopy angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3603772/', 'p': u'Pulmonary venous developmental anomalies have been evaluated conventionally with echocardiography and catheter angiography, multidetector CT angiography (MDCTA) and MR angiography are playing increasing roles in their characterisation. Here, we report a rare case of a 15-year-old boy, who presented with cyanosis and dyspnoea which he had had since childhood. Cardiac type of total anomalous pulmonary venous connection (TAPVC) was diagnosed and demonstrated using MDCTA in this case. Only a few case reports describing the MDCTA findings in cardiac TAPVC are available in the published literature.Total anomalous pulmonary venous connection (TAPVC), an uncommon congenital cardiac anomaly, is characterised by abnormal diversion of oxygenated blood into systemic venous circulation, where mixed blood flows to systemic circulation through an atrial septal defect (ASD), a patent foramen ovale (PFO) or a patent ductus arteriosus (PDA). The anomalous venous communication can be cardiac, supracardiac, infracardiac or mixed, with the supracardiac entity being the commonest. In cardiac type of TAPVC, pulmonary veins connect directly to the coronary sinus (CS) or right atrium (RA) and have no connection with the left atrium (LA). With the advent of multidetector CT (MDCT), it has become possible to conduct a non-invasive evaluation of this condition with high precision.', 'kwd': '-', 'title': u'Novel diagnostic procedure: Cardiac type of total anomalous pulmonary venous connection: diagnosis and demonstration by multidetector CT angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4146432/', 'p': u'Recent studies have suggested that endoscopic vein harvest (EVH) compromises graft patency. To test whether the learning curve for EVH alters conduit integrity owing to increased trauma compared with an open harvest, we analyzed the quality and early patency of conduits procured by technicians with varying EVH experience.During coronary artery bypass grafting, veins were harvested open (n = 10) or by EVH (n = 85) performed by experienced (>900 cases, >30/month) versus novice <100 cases, <3/month) technicians. Harvested conduits were imaged intraoperatively using optical coherence tomography and on day 5 to assess graft patency using computed tomographic angiography.', 'kwd': '-', 'title': u'Impact of the Learning Curve for Endoscopic Vein Harvest on Conduit Quality and Early Graft Patency'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3604536/', 'p': u'Coronary angiography remains as the gold standard for the diagnosis of coronary artery disease. Serious complication due to coronary angiography is very rare and generally it is considered safe in experienced hands. Here, we would like to present two cases without severe stenosis in the coronary arteries suffering from cardiac arrest soon after the procedures, and draw attention to this lethal complication and review the possible causes of sudden death occurring after coronary angiography.Coronary artery disease remains as the foremost cause of death in the world. Coronary angiography is regarded as the gold standard for the diagnosis of coronary artery disease. Serious complication due to coronary angiography is very rare and generally it is considered safe in experienced hands. Here, we would like to present two cases without severe stenosis in the coronary arteries suffering from cardiac arrest soon after the procedures. We want to draw attention to this lethal complication and review the possible causes of sudden death occurring after coronary angiography.', 'kwd': '-', 'title': u'Case Report: Sudden death after normal coronary angiography and possible causes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4429500/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 37th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405523/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 38th Annual Meeting of the Society of General Internal Medicine'}], 'Arterial Coronary Syndrome AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028318/', 'p': u"Coronary atherosclerosis is the major cause of mortality and disability in developed nations. A deeper understanding of mechanical properties of coronary arteries and hence their mechanical response to stress is significant for clinical prevention and treatment. Microstructure-based models of blood vessels can provide predictions of arterial mechanical response at the macro- and micro-mechanical level for each constituent structure. Such models must be based on quantitative data of structural parameters (constituent content, orientation angle and dimension) and mechanical properties of individual adventitia and media layers of normal arteries as well as change of structural and mechanical properties of atherosclerotic arteries. The microstructural constitutive models of healthy coronary arteries consist of three major mechanical components: collagen, elastin, and smooth muscle cells, while the models of atherosclerotic arteries should account for additional constituents including intima, fibrous plaque, lipid, calcification, etc. This review surveys the literature on morphology, mechanical properties, and microstructural constitutive models of normal and atherosclerotic coronary arteries. It also provides an overview of current gaps in knowledge that must be filed in order to advance this important area of research for understanding initiation, progression and clinical treatment of vascular disease. Patient-specific structural models are highlighted to provide diagnosis, virtual planning of therapy and prognosis when realistic patient-specific geometries and material properties of diseased vessels can be acquired by advanced imaging techniques.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Constitutive model, atherosclerosis, elastin, collagen, smooth muscle cells', 'title': u'Microstructure-Based Biomechanics of Coronary Arteries in Health and Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408160/', 'p': u'See Glossary (Chapter 29) for explanation of terms.The NHANES 2011 to 2014 data are used in this Update to present\nestimates of the percentage of people with high lipid values, DM, overweight,\nand obesity. The NHIS is used for the prevalence of cigarette smoking and\nphysical inactivity. Data for students in grades 9 through 12 are obtained from\nthe YRBSS.', 'kwd': u'AHA Scientific Statements, cardiovascular diseases, epidemiology, risk factors, statistics, stroke', 'title': u'Heart Disease and Stroke Statistics\u20142017\nUpdate'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4263214/', 'p': u'In adults with prior arterial switch operation (ASO) for d-transposition of the great arteries, the need for routine coronary artery assessment and evaluation for silent myocardial ischemia is not well defined. In this observational study we aimed to determine the value of a comprehensive cardiovascular magnetic resonance (CMR) protocol for the detection of coronary problems in adults with prior ASO for d-transposition of the great arteries.Adult ASO patients (\u226518\xa0years of age) were recruited consecutively. Patients underwent a comprehensive stress perfusion CMR protocol that included measurement of biventricular systolic function, myocardial scar burden, coronary ostial assessment and myocardial perfusion during vasodilator stress by perfusion CMR. Single photon emission computed tomography (SPECT) was performed on the same day as a confirmatory second imaging modality. Stress studies were visually assessed for perfusion defects (qualitative analysis). Additionally, myocardial blood flow was quantitatively analysed from mid-ventricular perfusion CMR images. In unclear cases, CT coronary angiography or conventional angiography was done.', 'kwd': u'Transposition of the great vessels, Cardiovascular magnetic resonance, Nuclear cardiology, Ischemia', 'title': u'Evaluation of a comprehensive cardiovascular magnetic resonance protocol in young adults late after the arterial switch operation for d-transposition of the great arteries'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4418670/', 'p': u'In light of the current national focus on healthcare utilization, costs, and quality, it is critical to monitor and understand the magnitude of healthcare delivery and costs, as well as the quality of healthcare delivery, related to CVDs. The Update provides these critical data in several sections.Chapter 20 reviews many metrics related to the quality of care delivered to patients with CVDs, as well as healthcare disparities. In particular, quality data are available from the AHA\u2019s \u201cGet With The Guidelines\u201d programs for coronary artery disease and heart failure and the American Stroke Association/ AHA\u2019s \u201cGet With the Guidelines\u201d program for acute stroke. Similar data from the Veterans Healthcare Administration, national Medicare and Medicaid data and National Cardiovascular Data Registry Acute Coronary Treatment and Intervention Outcomes Network - \u201cGet With The Guidelines\u201d Registry data are also reviewed. These data show impressive adherence with guideline recommendations for many, but not all, metrics of quality of care for these hospitalized patients. Data are also reviewed on screening for cardiovascular risk factor levels and control.', 'kwd': u'AHA Statistical Update, cardiovascular diseases, epidemiology, risk factors, statistics, stroke', 'title': u'Heart Disease and Stroke Statistics\u20142011 Update'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3334245/', 'p': u'Exercise training (EX) induces increases in coronary transport capacity through adaptations in the coronary microcirculation including increased arteriolar diameters and/or densities and changes in the vasomotor reactivity of coronary resistance arteries. In large animals, EX increases capillary exchange capacity through angiogenesis of new capillaries at a rate matched to EX-induced cardiac hypertrophy so that capillary density remains normal. However, after EX coronary capillary exchange area is greater (i.e., capillary permeability surface area product is greater) at any given blood flow because of altered coronary vascular resistance and matching of exchange surface area and blood flow distribution. The improved coronary capillary blood flow distribution appears to be the result of structural changes in the coronary tree and alterations in vasoreactivity of coronary resistance arteries. EX also alters vasomotor reactivity of conduit coronary arteries in that after EX, \u03b1-adrenergic receptor responsiveness is blunted. Of interest, \u03b1- and \u03b2-adrenergic tone appears to be maintained in the coronary microcirculation in the presence of lower circulating catecholamine levels because of increased receptor responsiveness to adrenergic stimulation. EX also alters other vasomotor control processes of coronary resistance vessels. For example, coronary arterioles exhibit increased myogenic tone after EX, likely because of a calcium-dependent PKC signaling-mediated alteration in voltage-gated calcium channel activity in response to stretch. Conversely, EX augments endothelium-dependent vasodilation throughout the coronary arteriolar network and in the conduit arteries in coronary artery disease (CAD). The enhanced endothelium-dependent dilation appears to result from increased nitric oxide bioavailability because of changes in nitric oxide synthase expression/activity and decreased oxidant stress. EX also decreases extravascular compressive forces in the myocardium at rest and at comparable levels of exercise, mainly because of decreases in heart rate and duration of systole. EX does not stimulate growth of coronary collateral vessels in the normal heart. However, if exercise produces ischemia, which would be absent or minimal under resting conditions, there is evidence that collateral growth can be enhanced. While there is evidence that EX can decrease the progression of atherosclerotic lesions or even induce the regression of atherosclerotic lesions in humans, the evidence of this is not strong due to the fact that most prospective trials conducted to date have included other lifestyle changes and treatment strategies by necessity. The literature from large animal models of CAD also presents a cloudy picture concerning whether EX can induce the regression of or slow the progression of atherosclerotic lesions. Thus, while evidence from research using humans with CAD and animal models of CAD indicates that EX increases endothelium-dependent dilation throughout the coronary vascular tree, evidence that EX reverses or slows the progression of lesion development in CAD is not conclusive at this time. This suggests that the beneficial effects of EX in CAD may not be the result of direct effects on the coronary artery wall. If this suggestion is true, it is important to determine the mechanisms involved in these beneficial effects.Results from early experiments using corrosion casts of the coronary vasculature of young rodents indicated that EX induces an increase in the volume of the coronary vasculature (154, 157) because of increased conduit artery size (15, 62, 82, 99, 180). For example, 60 min of swimming per day increased coronary artery luminal area in young male rats with cardiac hypertrophy, but rats that exercised only twice each week and did not develop cardiac hypertrophy did not exhibit this adaptation (99). Increases in coronary artery size related to cardiac hypertrophy have also been reported in EX dogs (180) and monkeys (82). EX also appears to increase the size of human epicardial coronary arteries since echocardiographic and MRI measures (80, 81, 128, 185) indicate increases in the cross-sectional area of proximal coronary arteries. These increases in artery size were proportional to increases in LV mass in elite athletes compared with healthy sedentary individuals (128, 185). Windecker et al. (177) reported that 5 mo of EX resulted in a 28% increase in LV mass and a 23% increase in combined cross-sectional area of the left main and right coronary arteries measured with quantitative angiography. In contrast, Haskell et al. (62) reported no difference in angiographically measured cross-sectional areas of right coronary artery, left main, and left anterior descending coronary artery (LAD) of ultra-distance runners and sedentary subjects under basal resting conditions. However, nitroglycerin-induced increases in coronary cross-sectional area were positively correlated with aerobic exercise capacity. Hildick-Smith et al. (71) reported that nitroglycerin produced significantly greater dilation of the LAD in athletes than in sedentary men. Kozakova et al. (80) also reported a twofold greater dipyridamole-induced dilation of the left main coronary artery in athletes than in control subjects. Thus the results indicate that EX causes growth of epicardial arteries in proportion to the degree of exercise-induced cardiac hypertrophy. Simultaneously, conduit coronary artery tone during basal resting conditions is greater in EX individuals, so that conduit artery vasodilator capacity is greater in EX humans (62, 71, 177).', 'kwd': u'atherosclerosis, coronary blood flow, endothelium, physical activity, smooth muscle', 'title': u'The coronary circulation in exercise training'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5181647/', 'p': u'Ischemic heart disease is a complex disease process caused by the development of coronary atherosclerosis, with downstream effects on the left ventricular myocardium. It is characterized by a long preclinical phase, abrupt development of myocardial infarction, and more chronic disease states such as stable angina and ischemic cardiomyopathy. Recent advances in computed tomography (CT) and cardiac magnetic resonance (CMR) now allow detailed imaging of each of these different phases of the disease, potentially allowing ischemic heart disease to be tracked during a patient\u2019s lifetime. In particular, CT has emerged as the noninvasive modality of choice for imaging the coronary arteries, whereas CMR offers detailed assessments of myocardial perfusion, viability, and function. The clinical utility of these techniques is increasingly being supported by robust randomized controlled trial data, although the widespread adoption of cardiac CT and CMR will require further evidence of clinical efficacy and cost effectiveness.The successful application of CT and CMR imaging to the heart was delayed compared with application to static organ systems, principally due to the heart\u2019s complex motion during both the cardiac and the respiratory cycles. However, advances in scanner technology now offer robust methods for motion correction and much improved spatial and temporal resolution, heralding a new era of noninvasive cardiac imaging. Each technology has different strengths and weaknesses (Table\xa01) that can potentially provide complementary information regarding ischemic heart disease. We focus here on\xa0the ability of these technologies to image plaque\xa0burden, high-risk plaque characteristics, and luminal stenoses in the coronary arteries, whereas in the left ventricle, we concentrate on the imaging of myocardial perfusion, infarction, viability, and function (Central Illustration).', 'kwd': u'acute coronary syndrome, atherosclerosis, calcium, coronary artery disease, myocardial infarction, noninvasive imaging', 'title': u'Computed Tomography and Cardiac\xa0Magnetic Resonance in Ischemic\xa0Heart\xa0Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684738/', 'p': u'Coronary computed tomography angiography (CTA) can be used to detect and quantitatively assess high-risk plaque features.To validate the ROMICAT score, which was derived using semi-automated quantitative measurements of high-risk plaque features, for the prediction of ACS.', 'kwd': u'coronary computed tomography angiography, acute coronary syndrome, coronary atherosclerotic plaque, acute chest pain, risk score', 'title': u'Computed Tomography-Based High-Risk Coronary Plaque Score to Predict Acute Coronary Syndrome Among Patients With Acute Chest Pain \u2013 Results from the ROMICAT II Trial'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243850/', 'p': u'Endothelins act via two receptor subtypes, ETA and ETB. Under physiological conditions in coronary arteries, ETA receptors expressed in smooth muscle cells mediate vasoconstriction whereas ETB receptors mainly found in endothelial cells mediate vasorelaxation. However, under pathophysiological conditions, ETB receptors may also be expressed in vascular smooth muscle cells mediating vasoconstriction. Here, we have investigated whether vasoconstrictor ETB receptors are up-regulated in coronary arteries after experimental myocardial ischaemia in rats.Male Sprague-Dawley rats were subjected to either heart ischaemia\u2013reperfusion (15\u2009min ischaemia and 22\u2009h reperfusion), permanent ischaemia (22\u2009h) by ligation of the left anterior descending coronary artery, or sham operation. Using wire myography, the endothelin receptor subtypes mediating vasoconstriction were examined in isolated segments of the left anterior descending and the non-ligated septal coronary arteries. Endothelin receptor-mediated vasoconstriction was examined with cumulative administration of sarafotoxin 6c (ETB receptor agonist) and endothelin-1 (with or without ETA or ETB receptor blockade). The distribution of ETB receptors was localized with immunohistochemistry and quantified by Western blot.', 'kwd': u'endothelin receptor, endothelin, coronary artery, sarafotoxin 6c, vascular smooth muscle', 'title': u'Heart ischaemia\u2013reperfusion induces local up-regulation of vasoconstrictor endothelin ETB receptors in rat coronary arteries\ndownstream of occlusion'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756468/', 'p': u'Advances in atherosclerosis imaging technology and research have provided a range of diagnostic tools to characterize high-risk plaque in vivo; however, these important vascular imaging methods additionally promise great scientific and translational applications beyond this quest. When combined with conventional anatomic- and hemodynamic-based assessments of disease severity, cross-sectional multimodal imaging incorporating molecular probes and other novel noninvasive techniques can add detailed interrogation of plaque composition, activity, and overall disease burden. In the catheterization laboratory, intravascular imaging provides unparalleled access to the world beneath the plaque surface, allowing tissue characterization and measurement of cap thickness with micrometer spatial resolution. Atherosclerosis imaging captures key data that reveal snapshots into underlying biology, which can test our understanding of fundamental research questions and shape our approach toward patient management. Imaging can also be used to quantify response to therapeutic interventions and ultimately help predict cardiovascular risk. Although there are undeniable barriers to clinical translation, many of these hold-ups might soon be surpassed by rapidly evolving innovations to improve image acquisition, coregistration, motion correction, and reduce radiation exposure. This article provides a comprehensive review of current and experimental atherosclerosis imaging methods and their uses in research and potential for translation to the clinic.Within the arterial wall, innate and adaptive immune responses triggered largely by clinical cardiovascular risk factors are major determinants of atherosclerotic progression and plaque rupture. Macrophages direct proinflammatory cell signaling cascades underlying high-risk plaque morphology, thus, presenting an attractive molecular imaging target to track vascular inflammation.', 'kwd': u'atherosclerosis, coronary artery disease, molecular imaging, multimodal imaging, risk factors', 'title': u'Imaging Atherosclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4674198/', 'p': u'Invasive coronary angiography (ICA) was performed by femoral or radial approach. The MLD was measured in projections showing the most severe narrowing of three main coronary arteries (LAD, LCx, and RCA) by a radiologist with more than 15 years of experience in cardiac imaging. Similarly, 3 consecutive measurements of the MLD within the same lesion were obtained, and the mean value was averaged.Statistical analyses were performed using SPSS 21.0 (SPSS, Inc, Chicago, IL). All continuous variables were expressed as the mean\u200a\xb1\u200astandard deviation, while categorical variables were presented as frequencies or percentages. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) for the detection of significant stenosis (>50%) on CCTA were calculated for individual three main coronary arteries with ICA as the gold standard. Receiver-operating characteristic (ROC) curve analysis was used to evaluate the diagnostic value of CCTA using different image postprocessing approaches in the measurement of MLD and detection of coronary stenosis compared to ICA. The areas under the ROC curves (AUCs) were compared among these three methods. A difference with P-value of <0.05 was considered statistically significant.', 'kwd': '-', 'title': u'Coronary CT Angiography in Heavily Calcified Coronary Arteries: Improvement of Coronary Lumen Visualization and Coronary Stenosis Assessment With Image Postprocessing Methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4778304/', 'p': u'Based on intravascular ultrasound of the coronary arteries expansive arterial remodeling is supposed to be a feature of the vulnerable atheroslerotic plaque. However, till now little is known regarding the clinical impact of expansive remodeling of carotid lesions. Therefore, we sought to evaluate the correlation of expansive arterial remodeling of the carotid arteries with atherosclerotic plaque composition and vulnerability using in-vivo Cardiovascular Magnetic Resonance (CMR).One hundred eleven symptomatic patients (74 male/71.8\u2009\xb1\u200910.3y) with acute unilateral ischemic stroke and carotid plaques of at least 2\xa0mm thickness were included. All patients received a dedicated multi-sequence black-blood carotid CMR (3Tesla) of the proximal internal carotid arteries (ICA). Measurements of lumen, wall, outer wall, hemorrhage, calcification and necrotic core were determined. Each vessel-segment was classified according to American Heart Association (AHA) criteria for vulnerable plaque. A modified remodeling index (mRI) was established by dividing the average outer vessel area of the ICA segments by the lumen area measured on TOF images in a not affected reference segment at the distal ipsilateral ICA. Correlations of mRI and clinical symptoms as well as plaque morphology/vessel dimensions were evaluated.', 'kwd': u'Plaque, Remodeling, Stroke, CMR, MRI', 'title': u'Expansive arterial remodeling of the carotid arteries and its effect on atherosclerotic plaque composition and vulnerability: an in-vivo black-blood 3T CMR study in symptomatic stroke patients'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4347263/', 'p': u'The development of cardiac CT has provided a non-invasive alternative to echocardiography, exercise electrocardiogram, and invasive angiography and cardiac CT continues to develop at an exponential speed even now. The appropriate use of cardiac CT may lead to improvements in the medical performances of physicians and can reduce medical costs which eventually contribute to better public health. However, until now, there has been no guideline regarding the appropriate use of cardiac CT in Korea. We intend to provide guidelines for the appropriate use of cardiac CT in heart diseases based on scientific data. The purpose of this guideline is to assist clinicians and other health professionals in the use of cardiac CT for diagnosis and treatment of heart diseases, especially in patients at high risk or suspected of heart disease.This work was developed through collaboration between the Korean Society of Radiology and the Korean Society of Cardiology and has been published jointly by invitation and consent in Journal of the Korean Society of Radiology and Korean Journal of Radiology.', 'kwd': u'Guideline, Appropriateness criteria, Cardiac computed tomography', 'title': u'Korean Guidelines for the Appropriate Use of Cardiac CT'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4717568/', 'p': u'The growing need for coronary evaluation has raised interest in non-radioactive, non-invasive monitoring systems. In particular, radiation exposure during coronary investigations has been shown to be a possible cause of an enhanced risk of secondary tumors. Literature search has indicated that transthoracic echocardiography (TTE) has been widely applied to coronary arteries up to 2003, following which the lack of adequate equipment and the increased availability of invasive diagnostics, has reduced interest in this low cost, low-risk technology. The more recent availability of newer, more sensitive machines, allows evaluation of a larger number of arterial trees, including the aorta in newborns, the prenatal aortic intima-media thickness, as well as the detection of coronary artery anomalies in the adult. Improved technology for this highly operator sensitive technique may thus predict a possible evolution toward the clinical diagnostics of coronary disease and, eventually, also of the progression/regression of disease. We sought to evaluate the present status of this seldom quoted non-invasive technology.TTE US of the coronaries is a potentially important diagnostic tool for coronary evaluation, both as a preliminary to an invasive procedure, or as a possible substitute. In addition, it may offer a readily accessible method for evaluating coronary changes after therapeutic procedures affecting the coronary arteries. The procedure is, at present, still operator sensitive, but the recent progress in IMT measurement in the aorta of newborns, or even in utero, with the development of appropriate software, indicates the feasibility of this approach.', 'kwd': u'Coronary artery, Intima-media thickness, Ultrasound, Harmonic imaging: coronary reserve, Statins', 'title': u'Status and potential clinical value of a transthoracic evaluation of the coronary arteries'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534347/', 'p': '-', 'kwd': '-', 'title': u'B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3533624/', 'p': '-', 'kwd': '-', 'title': u'ECR 2011 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4591589/', 'p': u'Temporal patterns of coronary blood flow velocity can provide important information on disease state and are currently assessed invasively using a Doppler guidewire. A non-invasive alternative would be beneficial as it would allow study of a wider patient population and serial scanning.A retrospectively-gated breath-hold spiral phase velocity mapping sequence (TR 19\xa0ms) was developed at 3 Tesla. Velocity maps were acquired in 8 proximal right and 15 proximal left coronary arteries of 18 subjects who had previously had a Doppler guidewire study at the time of coronary angiography. Cardiovascular magnetic resonance (CMR) velocity-time curves were processed semi-automatically and compared with corresponding invasive Doppler data.', 'kwd': u'Spiral, Phase velocity mapping, Coronary blood flow, Temporal pattern, Doppler, Validation', 'title': u'Validation of high temporal resolution spiral phase velocity mapping of temporal patterns of left and right coronary artery blood flow against Doppler guidewire'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4815360/', 'p': u'The endothelins comprise three structurally similar 21-amino acid peptides. Endothelin-1 and -2 activate two G-protein coupled receptors, ETA and ETB, with equal affinity, whereas endothelin-3 has a lower affinity for the ETA subtype. Genes encoding the peptides are present only among vertebrates. The ligand-receptor signaling pathway is a vertebrate innovation and may reflect the evolution of endothelin-1 as the most potent vasoconstrictor in the human cardiovascular system with remarkably long lasting action. Highly selective peptide ETA and ETB antagonists and ETB agonists together with radiolabeled analogs have accurately delineated endothelin pharmacology in humans and animal models, although surprisingly no ETA agonist has been discovered. ET antagonists (bosentan, ambrisentan) have revolutionized the treatment of pulmonary arterial hypertension, with the next generation of antagonists exhibiting improved efficacy (macitentan). Clinical trials continue to explore new applications, particularly in renal failure and for reducing proteinuria in diabetic nephropathy. Translational studies suggest a potential benefit of ETB agonists in chemotherapy and neuroprotection. However, demonstrating clinical efficacy of combined inhibitors of the endothelin converting enzyme and neutral endopeptidase has proved elusive. Over 28 genetic modifications have been made to the ET system in mice through global or cell-specific knockouts, knock ins, or alterations in gene expression of endothelin ligands or their target receptors. These studies have identified key roles for the endothelin isoforms and new therapeutic targets in development, fluid-electrolyte homeostasis, and cardiovascular and neuronal function. For the future, novel pharmacological strategies are emerging via small molecule epigenetic modulators, biologicals such as ETB monoclonal antibodies and the potential of signaling pathway biased agonists and antagonists.Despite the identification of ECE-1 as a rate limiting enzyme in the synthesis of ET-1, there has been much less development of selective small molecule inhibitors that could potentially reduce levels of ET-1 in pathophysiological conditions compared with the considerable effort to discover receptor antagonists. PD159790 was developed to be selective for ECE-1 compared with NEP (Ahn et al., 1998). The compound has been validated experimentally by altering the pH of endothelial cells in culture: at the optimum for ECE-1 activity, pH 6.9, PD159790 inhibited Big-ET-1 conversion but not at the optimum for ECE-2, pH 5.4 (Russell and Davenport, 1999a). In addition, the compound had no effect on the alternative pathway for ET-1 metabolism via chymase generation of ET-1(1-31) (Maguire et al., 2001).', 'kwd': '-', 'title': u'Endothelin'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5566546/', 'p': u'Existing clinical intravascular imaging modalities are not capable of accurate detection of critical plaque pathophysiology in the coronary arteries. This study reports the first intravascular catheter combining intravascular ultrasound (IVUS) with multispectral fluorescence lifetime imaging (FLIm) that enables label-free simultaneous assessment of morphological and biochemical features of coronary vessels in vivo. A 3.7 Fr catheter with a fiber-optic channel was constructed based on a 40\u2009MHz clinical IVUS catheter. The ability to safely acquire co-registered FLIm-IVUS data in vivo using Dextran40 solution flushing was demonstrated in swine coronary arteries. FLIm parameters from the arterial wall were consistent with the emission of fluorophores present in healthy arterial wall (collagen, elastin). Additionally, structural and biochemical features from atherosclerotic lesions were acquired in ex vivo human coronary samples and corroborated with histological findings. Current results show that FLIm parameters linked to the amount of structural proteins (e.g. collagen, elastin) and lipids (e.g. foam cells, extracellular lipids) in the first 200\u2009\u03bcm of the intima provide important biochemical information that can supplement IVUS data for a comprehensive assessment of plaques pathophysiology. The unique FLIm-IVUS system evaluated here has the potential to provide a comprehensive insight into atherosclerotic lesion formation, diagnostics and response to therapy.We present results from two representative cases chosen from human coronary arteries imaged ex vivo to illustrate the typical contrast provided by FLIm in diseased vessels. We will also use this data to illustrate how FLIm-derived compositional features can complement the IVUS-derived morphological features (e.g. lumen geometry, plaque thickness and presence of calcifications) enabling a more comprehensive evaluation of plaque pathophysiology, The cases consist of datasets of co-registered IVUS and multispectral FLIm from 20\u2009mm-long arterial segments, with matching histology sections. The use of a dedicated sample holder during imaging enables the accurate co-registration of the imaging data with histology sections. One example is presented in Fig.\xa04. Results from a second artery are presented in the supplemental materials (Figs\xa0s6 and s7).\n', 'kwd': '-', 'title': u'In vivo label-free structural and biochemical imaging of coronary arteries using an integrated ultrasound and multispectral fluorescence lifetime catheter system'}], 'Angiography AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4679245/', 'p': u'This article provides an overview of advanced image processing for three dimensional (3D) optical coherence tomographic (OCT) angiography of macular diseases, including age-related macular degeneration (AMD) and diabetic retinopathy (DR). A fast automated retinal layers segmentation algorithm using directional graph search was introduced to separates 3D flow data into different layers in the presence of pathologies. Intelligent manual correction methods are also systematically addressed which can be done rapidly on a single frame and then automatically propagated to full 3D volume with accuracy better than 1 pixel. Methods to visualize and analyze the abnormalities including retinal and choroidal neovascularization, retinal ischemia, and macular edema were presented to facilitate the clinical use of OCT angiography.The OCT angiography data was acquired using a commercial spectral domain OCT instrument (RTVue-XR; Optovue). It has a center wavelength of 840 nm with a full-width half-maximum bandwidth of 45 nm and an axial scan rate of 70 kHz. Volumetric macular scans consisted of a 3 \xd7 3 mm or 6 \xd7 6 mm area with a 1.6 mm depth (304 \xd7 304 \xd7 512 pixels). In the fast transverse scanning direction, 304 A-scans were sampled. Two repeated B-scans were captured at a fixed position before proceeding to the next location. A total of 304 locations along a 3 mm or 6 mm distance in the slow transverse direction were sampled to form a 3D data cube. The SSADA algorithm split the spectrum into 11 sub-spectra and detected blood flow by calculating the signal amplitude-decorrelation between two consecutive B-scans of the same location. All 608 B-scans in each data cube were acquired in 2.9 seconds. Two volumetric raster scans, including one x-fast scan and one y-fast scan, were obtained and registered [20].', 'kwd': u'(110.4500) Optical coherence tomography, (100.0100) Image processing, (100.2960) Image analysis, (170.4470) Ophthalmology', 'title': u'Advanced image processing for optical coherence tomographic angiography of macular diseases'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756595/', 'p': u'We propose new methods for automatic segmentation of images based on an atlas of manually labeled scans and contours in the image. First, we introduce a Bayesian framework for creating initial label maps from manually annotated training images. Within this framework, we model various registration- and patch-based segmentation techniques by changing the deformation field prior. Second, we perform contour-driven regression on the created label maps to refine the segmentation. Image contours and image parcellations give rise to non-stationary kernel functions that model the relationship between image locations. Setting the kernel to the covariance function in a Gaussian process establishes a distribution over label maps supported by image structures. Maximum a posteriori estimation of the distribution over label maps conditioned on the outcome of the atlas-based segmentation yields the refined segmentation. We evaluate the segmentation in two clinical applications: the segmentation of parotid glands in head and neck CT scans and the segmentation of the left atrium in cardiac MR angiography images.Given a novel image I, we aim to infer its segmentation S based on an atlas that contains training images \u2110 =\xa0{I1,\xa0\u2026,\xa0In} with segmentations \U0001d4ae =\xa0{S1,\xa0\u2026,\xa0Sn}. A probabilistic label map \u2112 =\xa0{L1,\xa0\u2026,\xa0L\u03b7} specifies the likelihood for each label l \u2208 {1, ..., \u03b7} ', 'kwd': u'Image segmentation, atlas, patch, spectral clustering, Gaussian process, left atrium, parotid glands', 'title': u'Contour-Driven Atlas-Based Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4382050/', 'p': u'Conceived and designed the experiments: Y. Zhao Y. Zheng. Performed the experiments: Y. Zhao. Analyzed the data: Y. Zhao Y. Zheng. Contributed reagents/materials/analysis tools: YL XW SH. Wrote the paper: Y. Zhao Y. Zheng YL SH XW. Shared helpful suggestions during the implementation of method: Y. Zhao Y. Zheng SH YL XW.Our application concerns the automated detection of vessels in retinal images to improve understanding of the disease mechanism, diagnosis and treatment of retinal and a number of systemic diseases. We propose a new framework for segmenting retinal vasculatures with much improved accuracy and efficiency. The proposed framework consists of three technical components: Retinex-based image inhomogeneity correction, local phase-based vessel enhancement and graph cut-based active contour segmentation. These procedures are applied in the following order. Underpinned by the Retinex theory, the inhomogeneity correction step aims to address challenges presented by the image intensity inhomogeneities, and the relatively low contrast of thin vessels compared to the background. The local phase enhancement technique is employed to enhance vessels for its superiority in preserving the vessel edges. The graph cut-based active contour method is used for its efficiency and effectiveness in segmenting the vessels from the enhanced images using the local phase filter. We have demonstrated its performance by applying it to four public retinal image datasets (3 datasets of color fundus photography and 1 of fluorescein angiography). Statistical analysis demonstrates that each component of the framework can provide the level of performance expected. The proposed framework is compared with widely used unsupervised and supervised methods, showing that the overall framework outperforms its competitors. For example, the achieved sensitivity (0:744), specificity (0:978) and accuracy (0:953) for the DRIVE dataset are very close to those of the manual annotations obtained by the second observer.', 'kwd': '-', 'title': u'Retinal Vessel Segmentation: An Efficient Graph Cut Approach with Retinex and Local Phase'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3138936/', 'p': '-', 'kwd': u'Magnetic resonance angiography (MRA), time-of-flight (TOF), cerebrovascular segmentation, statistical model analysis, fast curve evolution', 'title': u'A Fast and Fully Automatic Method for Cerebrovascular Segmentation on Time-of-Flight (TOF) MRA Image'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4528352/', 'p': u'This paper presents a new procedure for automatic extraction of the blood vessels and optic disk (OD) in fundus fluorescein angiogram (FFA). In order to extract blood vessel centerlines, the algorithm of vessel extraction starts with the analysis of directional images resulting from sub-bands of fast discrete curvelet transform (FDCT) in the similar directions and different scales. For this purpose, each directional image is processed by using information of the first order derivative and eigenvalues obtained from the Hessian matrix. The final vessel segmentation is obtained using a simple region growing algorithm iteratively, which merges centerline images with the contents of images resulting from modified top-hat transform followed by bit plane slicing. After extracting blood vessels from FFA image, candidates regions for OD are enhanced by removing blood vessels from the FFA image, using multi-structure elements morphology, and modification of FDCT coefficients. Then, canny edge detector and Hough transform are applied to the reconstructed image to extract the boundary of candidate regions. At the next step, the information of the main arc of the retinal vessels surrounding the OD region is used to extract the actual location of the OD. Finally, the OD boundary is detected by applying distance regularized level set evolution. The proposed method was tested on the FFA images from angiography unit of Isfahan Feiz Hospital, containing 70 FFA images from different diabetic retinopathy stages. The experimental results show the accuracy more than 93% for vessel segmentation and more than 87% for OD boundary extraction.Source of Support: Nil', 'kwd': u'Diabetic retinopathy, fast discrete curvelet transform, fundus fluorescein angiography, Hessian matrix, level set method', 'title': u'Analysis of Fundus Fluorescein Angiogram Based on the Hessian Matrix of Directional Curvelet Sub-bands and Distance Regularized Level Set Evolution'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4407675/', 'p': u'Despite the existence of automatic segmentation techniques, trained graders still rely on manual segmentation to provide retinal layers and features from clinical optical coherence tomography (OCT) images for accurate measurements. To bridge the gap between this time-consuming need of manual segmentation and currently available automatic segmentation techniques, this paper proposes a user-guided segmentation method to perform the segmentation of retinal layers and features in OCT images. With this method, by interactively navigating three-dimensional (3-D) OCT images, the user first manually defines user-defined (or sketched) lines at regions where the retinal layers appear very irregular for which the automatic segmentation method often fails to provide satisfactory results. The algorithm is then guided by these sketched lines to trace the entire 3-D retinal layer and anatomical features by the use of novel layer and edge detectors that are based on robust likelihood estimation. The layer and edge boundaries are finally obtained to achieve segmentation. Segmentation of retinal layers in mouse and human OCT images demonstrates the reliability and efficiency of the proposed user-guided segmentation method.We have proposed a user-guided segmentation method in which the user can provide a number of defined lines around the layers and features within 3-D OCT images to guide the program to perform robust detection of the boundaries and layers of retinal layers. We have demonstrated, through implementation of the algorithms on retinal OCT images of both mouse and human eyes, that the system is reliable and efficient. The proposed approach will be useful with the current usage of OCT imaging systems because it can serve in filling the gap between current automated segmentation techniques and obtaining accurate measurements for use in clinical practice.', 'kwd': u'biomedical optical imaging, image segmentation, optical coherence tomography, ophthalmology', 'title': u'User-guided segmentation for volumetric retinal optical coherence tomography images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4673359/', 'p': u'Glaucoma is the second leading cause of loss of vision in the world. Examining the head of optic nerve (cup-to-disc ratio) is very important for diagnosing glaucoma and for patient monitoring after diagnosis. Images of optic disc and optic cup are acquired by fundus camera as well as Optical Coherence Tomography. The optic disc and optic cup segmentation techniques are used to isolate the relevant parts of the retinal image and to calculate the cup-to-disc ratio. The main objective of this paper is to review segmentation methodologies and techniques for the disc and cup boundaries which are utilized to calculate the disc and cup geometrical parameters automatically and accurately to help the professionals in the glaucoma to have a wide view and more details about the optic nerve head structure using retinal fundus images. We provide a brief description of each technique, highlighting its classification and performance metrics. The current and future research directions are summarized and discussed.The Structured Analysis of Retina (STARE) dataset [22] is funded by the US National Institutes of Health. The project has 400 fundus images. Each image is diagnosis. The blood vessels are annotated in 40 images. The ONH is localized in 80 images. A TopCon TRV-50 fundus camera with 35\xb0 field of view was used to capture the images.', 'kwd': '-', 'title': u'Optic Disc and Optic Cup Segmentation Methodologies for Glaucoma Image Detection: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989849/', 'p': u'Endobronchial ultrasound (EBUS) is now commonly used for cancer-staging bronchoscopy. Unfortunately, EBUS is challenging to use and interpreting EBUS video sequences is difficult. Other ultrasound imaging domains, hampered by related difficulties, have benefited from computer-based image-segmentation methods. Yet, so far, no such methods have been proposed for EBUS. We propose image-segmentation methods for 2D EBUS frames and 3D EBUS sequences. Our 2D method adapts the fast-marching level-set process, anisotropic diffusion, and region growing to the problem of segmenting 2D EBUS frames. Our 3D method builds upon the 2D method while also incorporating the geodesic level-set process for segmenting EBUS sequences. Tests with lung-cancer patient data showed that the methods ran fully automatically for nearly 80% of test cases. For the remaining cases, the only user-interaction required was the selection of a seed point. When compared to ground-truth segmentations, the 2D method achieved an overall Dice index = 90.0%\xb14.9%, while the 3D method achieved an overall Dice index = 83.9\xb16.0%. In addition, the computation time (2D, 0.070 sec/frame; 3D, 0.088 sec/frame) was two orders of magnitude faster than interactive contour definition. Finally, we demonstrate the potential of the methods for EBUS localization in a multimodal image-guided bronchoscopy system.Because the fast-marching method constructs ROIs based on gradient information, it extracts a significant portion of ROI pixels situated in homogeneous locations. Nevertheless, image noise, which is well-known to greatly affect image-gradient values, limits ROI completeness. ROI Finalization performs complementary region growing based on intensity information to identify pixels missing from ROIs. Starting with filtered image I and initial segmentation R, ROI Finalization proceeds as follows:\n', 'kwd': u'endobronchial ultrasound, image segmentation, bronchoscopy, image-guided intervention system, lung cancer', 'title': u'Methods for 2D and 3D Endobronchial Ultrasound Image Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4882875/', 'p': u'Interventional left ventricular (LV) procedures integrating static 3D anatomy visualization are subject to mismatch with dynamic catheter movements due to prominent LV motion. We aimed to evaluate the accuracy of a recently developed acquisition and post-processing protocol for low radiation dose LV multi-phase rotational angiography (4DRA) in patients.4DRA image acquisition of the LV was performed as investigational acquisition in patients undergoing left-sided ablation (11 men; BMI = 24.7 \xb1 2.5 kg/m\xb2). Iodine contrast was injected in the LA, while pacing from the RA at a cycle length of 700 ms. 4DRA acquisition and reconstruction were possible in all 11 studies. Reconstructed images were post-processed using streak artefact reduction algorithms and an interphase registration-based filtering method, increasing contrast-to-noise ratio by a factor 8.2 \xb1 2.1. This enabled semi-automatic segmentation, yielding LV models of five equidistant phases per cardiac cycle. For evaluation, off-line 4DRA fluoroscopy registration was performed, and the 4DRA LV contours of the different phases were compared with the contours of five corresponding phases of biplane LV angiography, acquired in identical circumstances. Of the distances between these contours, 95% were <4 mm in both incidences. Effective radiation dose for 4DRA, calculated by patient-specific Monte-Carlo simulation, was 5.1 \xb1 1.1 mSv.', 'kwd': u'angiography, catheter ablation, imaging, tomography, ventricle', 'title': u'Multi-phase rotational angiography of the left ventricle to assist ablations: feasibility and accuracy of novel imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3883441/', 'p': u'Manual tracing of the luminal and EEL borders was performed by an expert interventional cardiologist. The independent standard resulted from frame-by-frame editing or retracing borders resulting from [14]. The manual tracing environment allowed to trace surfaces either in individual frames or in one of 6 axially reformatted planes (30 degree increments). The expert observer was allowed to select the individual planes or frames in any sequence and modify the borders until full satisfaction. In the process, all frames of each image sequence were reviewed, and most if not all manually traced and repeatedly edited. In the case of bifurcations, the expert was required to smoothly interpolate the luminal and EEL surfaces. This way of defining an independent standard was very tedious and time consuming, typically requiring 2-3 hours of manual tracing and editing per image sequence. A high-quality independent standard resulted that was used for performance assessment by the methods under comparison.The following quantitative error indices are utilized: mean signed border positioning error (ds), mean unsigned border positioning error (du), root-mean-square (RMS) border positioning error (drms), mean signed area error (As), mean unsigned area error (Au) and RMS area error (Arms). All these quantitative indices were utilized in [14]. In the case of ds and As, a negative value indicates that the segmentation border is inside and a positive value indicates that the border is outside of the expert-defined boundary/surface. To compute quantitative indices, borders on each frame are considered as points in polar coordinates at 360 one-degree angles. Border positioning errors are calculated for each boundary point as a distance between the independent standard point and the point on the segmented boundary. Indices ds, du, drms are calculated per sequence as averaged distances over all boundary points of the 3-D pullback sequence. Similarly, the area errors are calculated for each frame and As, Au and Arms are the averaged results over the entire pullback sequence.', 'kwd': u'intravascular ultrasound, segmentation, graph-based segmentation, segmentation refinement', 'title': u'Graph-Based IVUS Segmentation with Efficient Computer-Aided Refinement'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5568763/', 'p': u'Anatomical-based partial volume correction (PVC) has been shown to improve image quality and quantitative accuracy in cardiac SPECT/CT. However, this method requires manual segmentation of various organs from contrast-enhanced computed tomography angiography (CTA) data. In order to achieve fully automatic CTA segmentation for clinical translation, we investigated the most common multi-atlas segmentation methods. We also modified the multi-atlas segmentation method by introducing a novel label fusion algorithm for multiple organ segmentation to eliminate overlap and gap voxels. To evaluate our proposed automatic segmentation, eight canine 99mTc-labeled red blood cell SPECT/CT datasets that incorporated PVC were analyzed, using the leave-one-out approach. The Dice similarity coefficient of each organ was computed. Compared to the conventional label fusion method, our proposed label fusion method effectively eliminated gaps and overlaps and improved the CTA segmentation accuracy. The anatomical-based PVC of cardiac SPECT images with automatic multi-atlas segmentation provided consistent image quality and quantitative estimation of intramyocardial blood volume, as compared to those derived using manual segmentation. In conclusion, our proposed automatic multi-atlas segmentation method of CTAs is feasible, practical, and facilitates anatomical-based PVC of cardiac SPECT/CT images.After registering each particular atlas image to the target image, the transformation matrices were obtained. Then, the candidate segmentation images were obtained by applying these transformations to the corresponding label images of the five critical organ ROIs, respectively. In our method, we performed this label propagation process for each atlas image respectively to generate all candidate segmentation image sets of five ROIs.', 'kwd': u'multi-atlas based segmentation, partial volume correction, cardiac SPECT/CT', 'title': u'Fully automatic multi-atlas segmentation of CTA for partial volume correction in cardiac SPECT/CT'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4929662/', 'p': u'We propose a novel automated volumetric segmentation method to detect and quantify retinal fluid on optical coherence tomography (OCT). The fuzzy level set method was introduced for identifying the boundaries of fluid filled regions on B-scans (x and y-axes) and C-scans (z-axis). The boundaries identified from three types of scans were combined to generate a comprehensive volumetric segmentation of retinal fluid. Then, artefactual fluid regions were removed using morphological characteristics and by identifying vascular shadowing with OCT angiography obtained from the same scan. The accuracy of retinal fluid detection and quantification was evaluated on 10 eyes with diabetic macular edema. Automated segmentation had good agreement with manual segmentation qualitatively and quantitatively. The fluid map can be integrated with OCT angiogram for intuitive clinical evaluation.Figure 1 summarizes the algorithm. A pre-processing step was first performed to prepare the tissue region for segmentation. Fluid segmentation using fuzzy level-set method followed. Finally, post-processing steps were applied to remove artifacts. The following three sections will describe the process in detail. The algorithm was implemented with custom software written in Matlab 2011a (Mathworks, Natick, MA) installed on a workstation with Intel(R) Xeon(R) CPU E3-1226 v3 @ 3.30GHz and 16.0 GB RAM.', 'kwd': u'(110.4500) Optical coherence tomography, (100.0100) Image processing, (100.2960) Image analysis, (170.4470) Ophthalmology', 'title': u'Automated volumetric segmentation of retinal fluid on optical coherence tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3785070/', 'p': u'Optical coherence tomography (OCT) is a recently established imaging technique to describe different information about the internal structures of an object and to image various aspects of biological tissues. OCT image segmentation is mostly introduced on retinal OCT to localize the intra-retinal boundaries. Here, we review some of the important image segmentation methods for processing retinal OCT images. We may classify the OCT segmentation approaches into five distinct groups according to the image domain subjected to the segmentation algorithm. Current researches in OCT segmentation are mostly based on improving the accuracy and precision, and on reducing the required processing time. There is no doubt that current 3-D imaging modalities are now moving the research projects toward volume segmentation along with 3-D rendering and visualization. It is also important to develop robust methods capable of dealing with pathologic cases in OCT imaging.It should be noted that OCT is not the only possible device for assessing retinal pathologies; the field of ophthalmology was revolutionized in 1851 with the invention of the ophthalmoscope by Hermann von Helmholtz[3] as for the first time detailed examinations of the interior of the eye could be made in living patients. Fundus photography (a low powered microscope attached with a camera),[4] fluorescein Angiography[5] (photographing the retina by injecting fluorescent dyes) and Retinal thickness analyzer (RTA)[6] are other modalities proposed for diagnosis of retinal malfunctions. The latter is capable of rapidly obtaining retinal thickness map covering an area of 3 \xd7 3 mm. The oblique projection of a narrow laser slit beam on retina and recording the backscattered light are the principles of this method.[6] Furthermore, confocal scanning laser ophthalmoscopy (CSLO)[7] provides a three-dimensional topographic representationof the optic disk and peripapillary retina, which is constructed from a series of two-dimensional slices. This three-dimensional representation consists of 256 \xd7 256 (65,536) pixel elements, each of which is a measurement of retinal height at its corresponding location. Three topography images are usually acquired in a single session and thereafter are automatically aligned and averaged to obtain a single mean topography image. Although the CSLO is similar, in many respects, to a CT scan, the light rays used for CSLO cannot penetrate tissue; this limits this modality to depicting the surface topography of the optic disk and para-papillary retina.[8]', 'kwd': u'Optical coherence tomography, retina, segmentation', 'title': u'A Review of Algorithms for Segmentation of Optical Coherence Tomography from Retina'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3991579/', 'p': u'Conceived and designed the experiments: MTK YZ. Performed the experiments: MTK. Analyzed the data: MTK YZ. Contributed reagents/materials/analysis tools: NAVB SPH IJCM. Wrote the paper: MTK IJCM YZ. Shared helpful suggestions during the implementation of method: IJCM NAVB SPH.Capillary non-perfusion (CNP) in the retina is a characteristic feature used in the management of a wide range of retinal diseases. There is no well-established computation tool for assessing the extent of CNP. We propose a novel texture segmentation framework to address this problem. This framework comprises three major steps: pre-processing, unsupervised total variation texture segmentation, and supervised segmentation. It employs a state-of-the-art multiphase total variation texture segmentation model which is enhanced by new kernel based region terms. The model can be applied to texture and intensity-based multiphase problems. A supervised segmentation step allows the framework to take expert knowledge into account, an AdaBoost classifier with weighted cost coefficient is chosen to tackle imbalanced data classification problems. To demonstrate its effectiveness, we applied this framework to 48 images from malarial retinopathy and 10 images from ischemic diabetic maculopathy. The performance of segmentation is satisfactory when compared to a reference standard of manual delineations: accuracy, sensitivity and specificity are 89.0%, 73.0%, and 90.8% respectively for the malarial retinopathy dataset and 80.8%, 70.6%, and 82.1% respectively for the diabetic maculopathy dataset. In terms of region-wise analysis, this method achieved an accuracy of 76.3% (45 out of 59 regions) for the malarial retinopathy dataset and 73.9% (17 out of 26 regions) for the diabetic maculopathy dataset. This comprehensive segmentation framework can quantify capillary non-perfusion in retinopathy from two distinct etiologies, and has the potential to be adopted for wider applications.', 'kwd': '-', 'title': u'A Comprehensive Texture Segmentation Framework for Segmentation of Capillary Non-Perfusion Regions in Fundus Fluorescein Angiograms'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4771479/', 'p': u"Retinal hemodynamics is important for early diagnosis and precise monitoring in retinal vascular diseases. We propose a novel method for measuring absolute retinal blood flow in vivo using the combined techniques of optical coherence tomography (OCT) angiography and Doppler OCT. Doppler values can be corrected by Doppler angles extracted from OCT angiography images. A three-dimensional (3D) segmentation algorithm based on dynamic programming was developed to extract the 3D boundaries of optic disc vessels, and Doppler angles were calculated from 3D vessel geometry. The accuracy of blood flow from the Doppler OCT was validated using a flow phantom. The feasibility of the method was tested on a subject in vivo. The pulsatile retinal blood flow and the parameters for retinal hemodynamics were successfully obtained.The accuracy of the Doppler OCT method was tested on a flow phantom (Fig. 4(a)). The blood flow was simulated by a milk phantom moving in a transparent tube with an inner diameter of 800 \u03bcm. For better penetration, the milk phantom was diluted with 20% water to achieve a refractive index of 1.34. The flow was controlled by a syringe pump, and the tube was mounted on a rotational stage. For each measurement, the raster and repeat scans were performed sequentially. In the raster scan, the direction of the flow was obtained. Figure 4(b) shows the lateral projection of the flow phantom (x\xa0\u2212\xa0z plane) at a Doppler angle of 77.4\xb0. Each repeat scan was composed of 100 frames, and the scan was set perpendicular to the tube (x direction). Figure 4(a) and 4(c) show cross-sectional views of the OCT intensity image and Doppler OCT image. Figure 4(d) shows the phase difference profile that was marked by the blue line passing though the center of the tube in Fig. 4(c). The phase difference at the center of the tube (\u0394\u03d5c) was obtained by using a parabolic fitting. According to Poiseuille's law, the volumetric flow rate (\u03a6) was calculated as follows [36]:", 'kwd': u'(100.2960) Image analysis, (100.6890) Three-dimensional image processing, (170.4500) Optical coherence tomography, (170.4460) Ophthalmic optics and devices', 'title': u'In vivo imaging of retinal hemodynamics with OCT angiography and Doppler OCT'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5108587/', 'p': u'Spectral domain optical coherence tomography (SD-OCT) imaging permits in vivo visualization of the choroid with micron-level resolution over wide areas and is of interest for studies of ocular growth and myopia control. We evaluated the speed, repeatability and accuracy of a new a new image segmentation method to quantify choroid thickness compared to manual segmentation.Two macular volumetric scans (25\xd730\xb0) were taken from 30 eyes of 30 young adult subjects in two sessions, one hour apart. A single rater manually delineated choroid thickness as the distance between Bruch\u2019s membrane and sclera across three B-scans (foveal, inferior and superior-most scan locations). Manual segmentation was compared to an automated method based on graph theory, dynamic programming, and wavelet-based texture analysis. Segmentation performance comparisons included processing speed, choroid thickness measurements across the foveal horizontal midline, and measurement repeatability (95% limits of agreement (LoA)).', 'kwd': u'biomedical optics, choroid, optical coherence tomography, ocular imaging, image processing, biomedical optics, tissue characterization', 'title': u'Validation of Macular Choroidal Thickness Measurements from Automated SD-OCT Image Segmentation'}], 'Arterial Coronary Syndrome AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028318/', 'p': u"Coronary atherosclerosis is the major cause of mortality and disability in developed nations. A deeper understanding of mechanical properties of coronary arteries and hence their mechanical response to stress is significant for clinical prevention and treatment. Microstructure-based models of blood vessels can provide predictions of arterial mechanical response at the macro- and micro-mechanical level for each constituent structure. Such models must be based on quantitative data of structural parameters (constituent content, orientation angle and dimension) and mechanical properties of individual adventitia and media layers of normal arteries as well as change of structural and mechanical properties of atherosclerotic arteries. The microstructural constitutive models of healthy coronary arteries consist of three major mechanical components: collagen, elastin, and smooth muscle cells, while the models of atherosclerotic arteries should account for additional constituents including intima, fibrous plaque, lipid, calcification, etc. This review surveys the literature on morphology, mechanical properties, and microstructural constitutive models of normal and atherosclerotic coronary arteries. It also provides an overview of current gaps in knowledge that must be filed in order to advance this important area of research for understanding initiation, progression and clinical treatment of vascular disease. Patient-specific structural models are highlighted to provide diagnosis, virtual planning of therapy and prognosis when realistic patient-specific geometries and material properties of diseased vessels can be acquired by advanced imaging techniques.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Constitutive model, atherosclerosis, elastin, collagen, smooth muscle cells', 'title': u'Microstructure-Based Biomechanics of Coronary Arteries in Health and Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5536768/', 'p': u'Coronary artery anomaly (CAA) is a remarkable etiological factor for sudden cardiac death in young adults. The incidence of CAA is unknown, with most reliable data available based on postmortem/angiography investigations. This study aimed to assess the prevalence of different forms of coronary anomalies, and to investigate the relationships between demographic data and occurrence of CAA.A total of 2401 consecutive patients (1805 men; mean age, 56\u2009\xb1\u200911.7 years), who were referred between January 2005 and December 2008 for noninvasive multi-slice computed tomography (MSCT) imaging, were retrospectively analysed.', 'kwd': u'64-slice computed tomography, coronary anomalies, coronary anatomy, multi-slice computed tomography', 'title': u'Prevalence of congenital coronary artery anomalies as shown by multi-slice computed tomography coronary angiography: a single-centre study from Turkey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4919754/', 'p': u'The aim of this review is to give a comprehensive and concise overview of coronary embryology and normal coronary anatomy, describe common variants of normal and summarize typical patterns of anomalous coronary artery anatomy. Extensive iconography supports the text, with particular attention to images obtained in vivo using non-invasive imaging. We have divided this article into three groups, according to their frequency in the general population: Normal, normal variant and anomaly. Although congenital coronary artery anomalies are relatively uncommon, they are the second most common cause of sudden cardiac death among young athletes and therefore warrant detailed review. Based on the functional relevance of each abnormality, coronary artery anomalies can be classified as anomalies with obligatory ischemia, without ischemia or with exceptional ischemia. The clinical symptoms may include chest pain, dyspnea, palpitations, syncope, cardiomyopathy, arrhythmia, myocardial infarction and sudden cardiac death. Moreover, it is important to also identify variants and anomalies without clinical relevance in their own right as complications during surgery or angioplasty can occur.Defining what anatomy of the coronary arteries is normal can be challenging. Some normal features can be defined in numerical terms (for example, the number of coronary ostia), while in some other cases a more qualitative description is required. Angelini and coworkers proposed to classify \u201cnormal\u201d as every feature with > 1% of frequency in an unselected general population. According to this approach, a CAA can be defined as a coronary pattern or feature that is encountered in less than 1% of the general population. In summary, we can divide the coronary feature in two groups: (1) Normal coronary anatomy, defined as any morphological characteristics seen in > 1% of unselected sample. This group also includes normal anatomical variants, defined as alternative and relatively unusual morphological feature observed in > 1% of the population; and (2) Anomalous coronary anatomy, defined as morphological features found in < 1% of the population[8-10].', 'kwd': u'Coronary arteries, Anomalies, Variants, Anatomy, Heart', 'title': u'Coronary artery anomalies overview: The normal and the abnormal'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684738/', 'p': u'Coronary computed tomography angiography (CTA) can be used to detect and quantitatively assess high-risk plaque features.To validate the ROMICAT score, which was derived using semi-automated quantitative measurements of high-risk plaque features, for the prediction of ACS.', 'kwd': u'coronary computed tomography angiography, acute coronary syndrome, coronary atherosclerotic plaque, acute chest pain, risk score', 'title': u'Computed Tomography-Based High-Risk Coronary Plaque Score to Predict Acute Coronary Syndrome Among Patients With Acute Chest Pain \u2013 Results from the ROMICAT II Trial'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3545010/', 'p': u'Mounting evidence suggests that the pulsatile character of blood pressure and flow within large arteries plays a particularly important role as a mechano-biological stimulus for wall growth and remodeling. Nevertheless, understanding better the highly coupled interactions between evolving wall geometry, structure, and properties and the hemodynamics will require significantly more experimental data. Computational fluid\u2013solid-growth models promise to aid in the design and interpretation of such experiments and to identify candidate mechanobiological mechanisms for the observed arterial adaptations. Motivated by recent aortic coarctation models in animals, we used a computational fluid\u2013solid interaction model to study possible local and systemic effects on the hemodynamics within the thoracic aorta and coronary, carotid, and cerebral arteries due to a distal aortic coarctation and subsequent spatial variations in wall adaptation. In particular, we studied an initial stage of acute cardiac compensation (i.e., maintenance of cardiac output) followed by early arterial wall remodeling (i.e., spatially varying wall thickening and stiffening). Results suggested, for example, that while coarctation increased both the mean and pulse pressure in the proximal vessels, the locations nearest to the coarctation experienced the greatest changes in pulse pressure. In addition, after introducing a spatially varying wall adaptation, pressure, left ventricular work, and wave speed all increased. Finally, vessel wall strain similarly experienced spatial variations consistent with the degree of vascular wall adaptation.A thoracic aortic coarctation was modeled by introducing a narrowing in the aorta just above the diaphragm, consistent with both the location and the degree of a surgically induced coarctation in mini-pigs in studies that provide information on temporal and spatial changes in arterial wall composition (Hu et al. 2008; Hayenga 2010). This narrowing was created by scaling the original segmented vessel cross-sections to create a 75% reduction in diameter relative to the normal aortic diameter proximal to the location of the narrowing.', 'kwd': u'FSI, Arterial adaptation, Aortic banding, Pulse pressure, Hypertension', 'title': u'Computational simulations of hemodynamic changes within thoracic, coronary, and cerebral arteries following early wall remodeling in response to distal aortic coarctation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534347/', 'p': '-', 'kwd': '-', 'title': u'B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756468/', 'p': u'Advances in atherosclerosis imaging technology and research have provided a range of diagnostic tools to characterize high-risk plaque in vivo; however, these important vascular imaging methods additionally promise great scientific and translational applications beyond this quest. When combined with conventional anatomic- and hemodynamic-based assessments of disease severity, cross-sectional multimodal imaging incorporating molecular probes and other novel noninvasive techniques can add detailed interrogation of plaque composition, activity, and overall disease burden. In the catheterization laboratory, intravascular imaging provides unparalleled access to the world beneath the plaque surface, allowing tissue characterization and measurement of cap thickness with micrometer spatial resolution. Atherosclerosis imaging captures key data that reveal snapshots into underlying biology, which can test our understanding of fundamental research questions and shape our approach toward patient management. Imaging can also be used to quantify response to therapeutic interventions and ultimately help predict cardiovascular risk. Although there are undeniable barriers to clinical translation, many of these hold-ups might soon be surpassed by rapidly evolving innovations to improve image acquisition, coregistration, motion correction, and reduce radiation exposure. This article provides a comprehensive review of current and experimental atherosclerosis imaging methods and their uses in research and potential for translation to the clinic.Within the arterial wall, innate and adaptive immune responses triggered largely by clinical cardiovascular risk factors are major determinants of atherosclerotic progression and plaque rupture. Macrophages direct proinflammatory cell signaling cascades underlying high-risk plaque morphology, thus, presenting an attractive molecular imaging target to track vascular inflammation.', 'kwd': u'atherosclerosis, coronary artery disease, molecular imaging, multimodal imaging, risk factors', 'title': u'Imaging Atherosclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3903591/', 'p': u'Conceived and designed the experiments: CHY KP. Performed the experiments: CHY XQL. Analyzed the data: CHY XQL KP. Contributed reagents/materials/analysis tools: KP. Wrote the paper: CHY XQL KP.Abnormal fluid mechanical environment in the pre-natal cardiovascular system is hypothesized to play a significant role in causing structural heart malformations. It is thus important to improve our understanding of the prenatal cardiovascular fluid mechanical environment at multiple developmental time-points and vascular morphologies. We present such a study on fetal great arteries on the wildtype mouse from embryonic day 14.5 (E14.5) to near-term (E18.5).', 'kwd': '-', 'title': u'Characterizaton of the Vessel Geometry, Flow Mechanics and Wall Shear Stress in the Great Arteries of Wildtype Prenatal Mouse'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5249211/', 'p': u'\nThe objectives were to compare the performance of a segmentation algorithm, based on the minimization of an uncertainty function, to delineate contours of external elastic membrane and lumen of human coronary arteries imaged with 40 and 60 MHz IVUS, and to use values of this function to delineate portions of contours with highest uncertainty. For 8 patients, 40 and 60 MHz IVUS coronary data acquired pre- and post-interventions were used, for a total of 68,516 images. Manual segmentations of contours (on 2312 images) performed by experts at three core laboratories were the gold-standards. Inter-expert variability was highest on contour points with largest values of the uncertainty function (p < 0.001). Inter-expert variability was lower at 60 than 40 MHz for external elastic membrane (p = 0.013) and lumen (p = 0.024). Average differences in plaque (and atheroma) burden between algorithmic contours and experts\u2019 contours were within inter-expert variability (p < 0.001).', 'kwd': '-', 'title': u'Assessment of Inter-Expert Variability and of an Automated Segmentation Method of 40 and 60 MHz IVUS Images of Coronary Arteries'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3533624/', 'p': '-', 'kwd': '-', 'title': u'ECR 2011 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3063948/', 'p': u'To define and evaluate coronary contrast opacification gradients using prospectively ECG-gated single heart beat 320-detector row coronary angiography (CTA).Thirty-six patients with normal coronary arteries determined by 320 \xd7 0.5 mm detector row coronary CTA were retrospectively evaluated with customized image post-processing software to measure Hounsfield Units (HU) at 1 mm intervals orthogonal to the artery center line. Linear regression determined correlation between mean HU and distance from the coronary ostium (regression slope defined as the distance gradient Gd), lumen cross-sectional area (Ga), and lumen short axis diameter (Gs). For each gradient, differences between the three coronary arteries were analyzed with ANOVA. Linear regression determined correlations between measured gradients, heart rate, body-mass index (BMI), and cardiac phase. To determine feasibility in lesions, all three gradients were evaluated in 22 consecutive patients with left anterior descending artery lesions greater than or equal to 50% stenosis. For all 3 coronary arteries in all patients, the gradients Ga and Gs were significantly different from zero (p<0.0001), highly linear (Pearson r values 0.77-0.84), and had no significant difference between the LAD, LCx, and RCA (p>0.503). The distance gradient Gd demonstrated nonlinearities in a small number of vessels and was significantly smaller in the RCA when compared to the left coronary system (p<0.001). Gradient variations between cardiac phases, heart rates, BMI, and readers were low. Gradients in patients with lesions were significantly different (p<0.021) than in patients considered normal by CTA.', 'kwd': u'computed tomography, coronary vessels, imaging', 'title': u'Iodinated Contrast Opacification Gradients in Normal Coronary Arteries Imaged with Prospectively ECG-Gated Single Heart Beat 320-Detector Row Computed Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4496886/', 'p': u'For free-breathing cardiovascular magnetic resonance (CMR), the self-navigation technique recently emerged, which is expected to deliver high-quality data with a high success rate. The purpose of this study was to test the hypothesis that self-navigated 3D-CMR enables the reliable assessment of cardiovascular anatomy in patients with congenital heart disease (CHD) and to define factors that affect image quality.CHD patients \u22652 years-old and referred for CMR for initial assessment or for a follow-up study were included to undergo a free-breathing self-navigated 3D CMR at 1.5T. Performance criteria were: correct description of cardiac segmental anatomy, overall image quality, coronary artery visibility, and reproducibility of great vessels diameter measurements. Factors associated with insufficient image quality were identified using multivariate logistic regression.', 'kwd': u'Congenital heart disease, Cardiovascular magnetic resonance, Self-navigation, Navigator, Free-breathing', 'title': u'Single centre experience of the application of self navigated 3D whole heart cardiovascular magnetic resonance for the assessment of cardiac anatomy in congenital heart disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3042771/', 'p': u'The writing committee consisted of acknowledged experts in the field of CMR, as well as a liaison from the ACCF Task Force on Clinical ECDs, the oversight group for this document. In addition to 2 ACCF members, the writing committee included 1 representative from the American Academy of Pediatrics (AAP) and 2 representatives from the ACR, AHA, NASCI, and the SCMR. Representation by an outside organization does not necessarily imply endorsement.At its first meeting, each member of the writing committee reported all relationships with industry and other entities relevant to this document topic. This information was updated, if applicable, at the beginning of all subsequent meetings and full committee conference calls. As noted in the Preamble, relevant relationships with industry and other entities of writing committee members are published in Appendix 1.', 'kwd': u'ACCF/AHA Expert Consensus Document, cardiovascular magnetic resonance, cardiovascular disease, magnetic resonance imaging, safety', 'title': u'ACCF/ACR/AHA/NASCI/SCMR 2010 Expert Consensus Document on Cardiovascular Magnetic Resonance'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3034132/', 'p': u'The writing committee consisted of acknowledged experts in the field of CMR, as well as a liaison from the ACCF Task Force on Clinical ECDs, the oversight group for this document. In addition to 2 ACCF members, the writing committee included 1 representative from the American Academy of Pediatrics (AAP) and 2 representatives from the ACR, AHA, NASCI, and the SCMR. Representation by an outside organization does not necessarily imply endorsement.At its first meeting, each member of the writing committee reported all relationships with industry and other entities relevant to this document topic. This information was updated, if applicable, at the beginning of all subsequent meetings and full committee conference calls. As noted in the Preamble, relevant relationships with industry and other entities of writing committee members are published in Appendix 1.', 'kwd': u'ACCF/AHA Expert Consensus Document, cardiovascular magnetic resonance, cardiovascular disease, magnetic resonance imaging, safety', 'title': u'ACCF/ACR/AHA/NASCI/SCMR 2010 Expert Consensus Document on Cardiovascular Magnetic Resonance A Report of the American College of Cardiology Foundation Task Force on Expert Consensus Documents'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4499869/', 'p': u'The initial course of atherosclerotic disease is thought to begin in early adulthood. In young adults lesions in the arterial vessel wall have been observed surprisingly frequently13 but the prognostic relevance of early, adaptive or reversible changes like \u201cfatty streak\u201d or intimal thickening remains a matter of debate. Pathology studies seek to integrate autopsy findings from various stages of atherosclerosis to provide a putative sequence of events4. In brief, intimal thickening is observed early in the disease process. The early atherosclerotic lesion is composed of smooth muscle cells and is affected by increased macrophage and lipid influx. If this process continues, a necrotic core is formed and the lesion progresses to a fibrous cap atheroma. The necrotic core contains lipids and apoptotic macrophages. A stable fibrous cap may prevent rupture of the lesion. If the fibrous cap loses matrix proteins and smooth muscle cells, a thin cap atheroma can result. Intraplaque hemorrhage is also seen frequently in this entity, leading to further enlargement of the lipid core. The risk of plaque rupture is increased as the fibrous cap thins and the lipid core enlarges14. The \u201cfibrocalcific plaque\u201d is considered to be a feature of more stable plaque, although the processes involved in calcification are not fully understood.It is generally conceived that therapeutic intervention for atherosclerosisis most effective when started at an early stage of the progressive disease process 15. Imaging tools have provided a substantial database of knowledge regarding disease burden. Imaging of the larger surface vessels (carotid or femoral arteries) has been extensively used to detect early systemic vascular pathology 16. Calcium detection using non-contrast CT provides a direct approach to assessing coronary atherosclerosis burden. The coronary artery calcium score has strong predictive power for cardiovascular events in asymptomatic subjects17. However, calcium deposition is felt to be a late event in the formation of atherosclerotic plaque. The relevance of non-calcified plaque is emphasized by prospective IVUS studies that show coronary fibroatheroma without significant calcification confers an elevated risk for myocardial infarction 18. Non-calcified plaque is more common than calcified plaque in asymptomatic individuals younger than 45 years. The ability to noninvasively image non-calcified plaque or wall thickening of the coronary arteries using MRI or CT enables the detection of earlier stages of atherosclerotic disease 19, 20.', 'kwd': u'imaging, coronary disease, plaque, atherosclerosis', 'title': u'Noninvasive Imaging of Atherosclerotic Plaque Progression: Status of Coronary CT Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4341907/', 'p': u"Studies in human and non-human primates have confirmed the compensatory enlargement or positive remodeling (Glagov phenomenon) of coronary vessels in the presence of focal stenosis. To our knowledge, this is the first study to document arterial enlargement in a metabolic syndrome animal model with diffuse coronary artery disease (DCAD) in the absence of severe focal stenosis. Two different groups of Ossabaw miniature pigs were fed a high fat atherogenic diet for 4\xa0months (Group I) and 12\xa0months (Group II), respectively. Group I (6 pigs) underwent contrast enhanced computed tomographic angiography (CCTA) and intravascular ultrasound (IVUS) at baseline and after 4\xa0months of high fat diet, whereas Group II (7 pigs) underwent only IVUS at 12\xa0months of high fat diet. IVUS measurements of the left anterior descending (LAD), left circumflex (LCX) and right coronary (RCA) arteries in Group I showed an average increase in their lumen cross-sectional areas (CSA) of 25.8%, 11.4%, and 43.4%, respectively, as compared to baseline. The lumen CSA values of LAD in Group II were found to be between the baseline and 4\xa0month values in Group I. IVUS and CCTA measurements showed a similar trend and positive correlation. Fractional flow reserve (FFR) was 0.91\xa0\xb1\xa00.07 at baseline and 0.93\xa0\xb1\xa00.05 at 4\xa0months with only 2.2%, 1.6% and 1% stenosis in the LAD, LCX and RCA, respectively. The relation between percent stenosis and lumen CSA shows a classical Glagov phenomenon in this animal model of DCAD.The data were expressed as means\xa0\xb1\xa01SD. Statistical significance of the results was assessed using SigmaStat software (Systat Software, Point Richmond, CA). The differences were evaluated with student's t-test or two-way ANOVA, where appropriate. The Siegel\u2013Tukey test (non-parametric test) was also used to confirm the statistical analysis. p\xa0<\xa00.05 was considered statistically significant.", 'kwd': u'Diffuse coronary artery disease, Positive remodeling, Glagov phenomenon, FFR, Percent stenosis', 'title': u'Compensatory enlargement of Ossabaw miniature swine coronary arteries in diffuse atherosclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422837/', 'p': u'During the last decade coronary computed tomography angiography (CTA) has become the preeminent non-invasive imaging modality to detect coronary artery disease (CAD) with high accuracy. However, CTA has a limited value in assessing the hemodynamic significance of a given stenosis due to a modest specificity and positive predictive value. In recent years, different CT techniques for detecting myocardial ischemia have emerged, such as CT-derived fractional flow reserve (FFR-CT), transluminal attenuation gradient (TAG), and myocardial CT perfusion (CTP) imaging. Myocardial CTP imaging can be performed with a single static scan during first pass of the contrast agent, with monoenergetic or dual-energy acquisition, or as a dynamic, time-resolved scan during stress by using coronary vasodilator agents (adenosine, dipyridamole, or regadenoson). A number of CTP techniques are available, which can assess myocardial perfusion in both a qualitative, semi-quantitative or quantitative manner. Once used primarily as research tools, these modalities are increasingly being used in routine clinical practice. All these techniques offer the substantial advantage of combining anatomical and functional evaluation of flow-limiting coronary stenosis in the same examination that would be beneficial for clinical decision-making. This review focuses on the state-of the-art and future trends of these evolving imaging modalities in the field of cardiology for the physiologic assessments of CAD.The FFR-CT method allows the extraction of \u2018stress induced\u2019 quantitative functional information from an anatomic CTA of at least moderate quality acquired at rest without adenosine infusion. The method uses computational fluid dynamics with simulated hyperemia to calculate the FFR measurement at any point in the vascular tree (39). The concept of coronary FFR, defined as the ratio of the mean coronary pressure distal to a coronary stenosis to the mean aortic pressure during maximal coronary blood flow, has evolved into an accepted functional measure of stenosis severity since first proposed 15 years ago (22). FFR has now become the invasive gold standard for assessing lesion-specific ischemia. A FFR value less than 0.80 or less than 0.75 identifies hemodynamic significance of coronary stenosis (7,8). FFR-CT correlates well with invasive-derived FFR measurements in patients with suspected or known CAD (17,39,40). One of the temporary drawbacks is that FFR-CT needs extreme computational ability and analysis time, which hampers widespread dissemination.', 'kwd': u'Cardiac computed tomography, coronary artery disease (CAD), stress imaging, myocardial perfusion imaging, myocardial blood flow quantification', 'title': u'Myocardial blood flow quantification for evaluation of coronary artery disease by computed tomography'}], 'Angiography AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877137/', 'p': u'Quantitative analysis of SPECT and PET has become a major part of nuclear\ncardiology practice. Current software tools can automatically segment the left\nventricle, quantify function, establish myocardial perfusion maps and estimate\nglobal and local measures of stress/rest perfusion \u2013 all with minimal\nuser input. State-of-the-art automated techniques have been shown to offer high\ndiagnostic accuracy for detecting coronary artery disease, as well as predict\nprognostic outcomes. This chapter briefly reviews these techniques, highlights\nseveral challenges and discusses the latest developments.The first step in quantification of perfusion and function is\nsegmentation of the LV from both gated and static reconstructed data.\nSegmentation of the myocardium may sometimes be challenging due to possible\nlarge perfusion defects, extra-cardiac activity, and image noise. Typically, the\nmost common sources of incorrect automated contours are gut activity and\nincorrect definition of the valve plane (Figure\n1). Nonetheless, current software tools allow accurate automatic\ndefinition of LV contours in up to 90%.2 Incorrect segmentation in the minority of cases\ncan result in spurious defects mimicking perfusion abnormalities, and therefore,\nsome supervision by an experienced observer is still required during this step.\nHowever, this can be accomplished by an experienced technologist, prior to scan\ninterpretation. Furthermore, recent software developments, which are discussed\nin this review, can be used to check automated LV contours, allowing readers to\ntarget manual adjustment only to those studies flagged by the algorithm for\npotential errors.', 'kwd': u'SPECT, PET, automated quantitation, myocardial function, left ventricular ejection fraction, myocardial perfusion, total perfusion deficit, ischemia', 'title': u'Automated Quantitative Nuclear Cardiology Methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480986/', 'p': u'Disease staging involves the assessment of disease severity or progression and is used for treatment selection. In diabetic retinopathy, disease staging using a wide area is more desirable than that using a limited area. We investigated if deep learning artificial intelligence (AI) could be used to grade diabetic retinopathy and determine treatment and prognosis.The retrospective study analyzed 9,939 posterior pole photographs of 2,740 patients with diabetes. Nonmydriatic 45\xb0 field color fundus photographs were taken of four fields in each eye annually at Jichi Medical University between May 2011 and June 2015. A modified fully randomly initialized GoogLeNet deep learning neural network was trained on 95% of the photographs using manual modified Davis grading of three additional adjacent photographs. We graded 4,709 of the 9,939 posterior pole fundus photographs using real prognoses. In addition, 95% of the photographs were learned by the modified GoogLeNet. Main outcome measures were prevalence and bias-adjusted Fleiss\u2019 kappa (PABAK) of AI staging of the remaining 5% of the photographs.', 'kwd': '-', 'title': u'Applying artificial intelligence to disease staging: Deep learning for improved staging of diabetic retinopathy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643767/', 'p': u'The continuous uninterrupted feedback system is the essential part of any well-organized system. We propose aLYNX concept that is a possibility to use an artificial intelligence algorithm or a neural network model in decision-making system so as to avoid possible mistakes and to remind the doctors to review tactics once more in selected cases.aLYNX system includes: registry with significant factors, decisions and results; machine learning process based on this registry data; the use of the machine learning results as the adviser. We show a possibility to build a computer adviser with a neural network model for making a choice between coronary aortic bypass surgery (CABG) and percutaneous coronary intervention (PCI) in order to achieve a higher 5-year survival rate in patients with angina based on the experience of 5107 patients.', 'kwd': u'Coronary artery bypass grafting, Percutaneous coronary intervention, Artificial intelligence, Decision making', 'title': u'Artificial intelligence: Neural network model as the multidisciplinary team member in clinical decision support to avoid medical mistakes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3176251/', 'p': u'Coronary angiography is an important tool in diagnosis of cardiovascular diseases. However, it is the administration is relatively stressful and emotionally traumatic for the subjects. The aim of this study is to evaluate psychophysiological responses induced by the coronary angiography instead of subjective methods such as a questionnaire. We have also evaluated the influence of the tranquilizer on the psychophysiological responses.Electrocardiography (ECG), Blood Volume Pulse (BVP), and Galvanic Skin Response (GSR) of 34 patients who underwent coronary angiography operation were recorded. Recordings were done at three phases: "1 hour before," "during," and "1 hour after" the coronary angiography test. Total of 5 features obtained from the physiological signals were compared across these three phases. Sixteen of the patients were administered 5 mg of a tranquilizer (Diazepam) before the operation and remaining 18 were not.', 'kwd': '-', 'title': u'Analysis of coronary angiography related psychophysiological responses'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3820488/', 'p': u'Recently introduced high-efficiency (HE) SPECT cameras with solid-state CZT detectors have been shown to decrease imaging time and reduce radiation exposure to patients. An automated, computer derived quantification of HE MPI has been shown to correlate well with coronary angiography on one HE SPECT camera system (D-SPECT), but has not been compared to visual interpretation on any of the HE SPECT platforms.Patients undergoing a clinically indicated Tc-99m sestamibi HE SPECT (GE Discovery 530c with supine and prone imaging) study over a one year period followed by a coronary angiogram within 2 months were included. Only patients with a history of CABG surgery were excluded. Both MPI studies and coronary angiograms were reinterpreted by blinded readers. One hundred and twenty two very low (risk of CAD < 5%) or low (risk of CAD < 10%) likelihood subjects with normal myocardial perfusion were used to create normal reference limits. Computer derived quantification of the total perfusion deficit (TPD) at stress and rest was obtained with QPS software. The visual and automated MPI quantification were compared to coronary angiography (\u2265 70% luminal stenosis) by receiver operating curve (ROC) analysis.', 'kwd': u'CZT SPECT MPI, High-Efficiency SPECT MPI, Automated Quantification, Coronary Angiography', 'title': u'High-Efficiency SPECT MPI: Comparison of Automated Quantification, Visual Interpretation, and Coronary Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3995408/', 'p': u"The goal of artificial intelligence, expert systems, decision support systems and computer assisted diagnosis (CAD) in imaging is the development and implementation of software to assist in the detection and evaluation of abnormalities, to alert physicians to cognitive biases, to reduce intra and inter-observer variability and to facilitate the interpretation of studies at a faster rate and with a higher level of accuracy. These developments are needed to meet the challenges resulting from a rapid increase in the volume of diagnostic imaging studies coupled with a concurrent increase in the number and complexity of images in each patient data. The convergence of an expanding knowledge base and escalating time constraints increases the likelihood of physician errors. Errors are even more likely when physicians interpret low volume studies such as 99mTc-MAG3 diuretic scans where imagers may have had limited training or experience. Decision support systems include neural networks, case-based reasoning, expert systems and statistical systems. iRENEX (renal expert) is an expert system for diuretic renography that uses a set of rules obtained from human experts to analyze a knowledge base of both clinical parameters and quantitative parameters derived from the renogram. Initial studies have shown that the interpretations provided by iRENEX are comparable to the interpretations of a panel of experts. iRENEX provides immediate patient specific feedback at the time of scan interpretation, can be queried to provide the reasons for its conclusions and can be used as an educational tool to teach trainees to better interpret renal scans. iRENEX also has the capacity to populate a structured reporting module and generate a clear and concise impression based on the elements contained in the report; adherence to the procedural and data entry components of the structured reporting module assures and documents procedural competency. Finally, although the focus is CAD applied to diuretic renography, this review offers a window into the rationale, methodology and broader applications of computer assisted diagnosis in medical imaging.The two major components of clinical decision consist of making a diagnosis and devising a treatment plan based on that diagnosis; if the diagnosis is incorrect, there is a much greater likelihood that the treatment plan will also be incorrect or suboptimal. Although many factors contribute to diagnostic error, a principal cause consists of cognitive errors where the problem lies not in a lack of knowledge but with the clinician's thinking process (6). Our minds are quite vulnerable to cognitive biases, logical fallacies and false assumptions. Cognitive failures are best understood in the context of how our brains manage and process information. The two principal modes can be categorized as System 1 and System 2 (7). System 1 operates automatically, intuitively and quickly, effortlessly originating impressions and feelings while System 2 requires mental effort, focused concentration and analysis. Cognitive errors are much more likely to occur when the scan, radiograph or clinical problem are analyzed by System 1. Examples of System 1 errors consist of reports that are internally contradictory or reports where diuretic renal scan interpretations are primarily based on a simple heuristic rule governing the T1/2 (If the T1/2 is greater than 20 min, the kidney is obstructed). The cognitive error rate may be even higher in areas where training, experience or expertise is compromised.", 'kwd': '-', 'title': u'Computer assisted diagnosis in renal nuclear medicine: rationale, methodology and interpretative criteria for diuretic renography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4859156/', 'p': u'We aimed to investigate if early revascularization in patients with suspected coronary artery disease (CAD) can be effectively predicted by integrating clinical data and quantitative image features derived from perfusion SPECT (MPS) by machine learning (ML) approach.713 rest 201Thallium/stress 99mTechnetium MPS studies with correlating invasive angiography (372 revascularization events (275 PCI / 97 CABG) within 90 days after MPS (91% within 30 days) were considered. Transient ischemic dilation (TID), stress combined supine/prone total perfusion deficit (TPD), quantitative rest and stress TPD, exercise ejection fraction, and end-systolic volume along with clinical parameters including patient gender, history of hypertension and diabetes mellitus, ST-depression on baseline ECG, ECG and clinical response during stress, and post-ECG probability by boosted ensemble ML algorithm (LogitBoost) to predict revascularization events. These features were selected using an automated feature selection algorithm from all available clinical and quantitative data (33 parameters). 10-fold cross-validation was utilized to train and test the prediction model. The prediction of revascularization by ML algorithm was compared to standalone measures of perfusion and visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data.', 'kwd': u'Machine Learning, Coronary Artery Disease, Myocardial Perfusion SPECT, Revascularization, Total Perfusion Deficit', 'title': u'Prediction of Revascularization after Myocardial Perfusion SPECT by Machine Learning in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3293488/', 'p': '-', 'kwd': u'Mode decomposition, Evolution equations, High order PDE transform, Anisotropic diffusion, Total variation, High-pass filter, Partial differential equation', 'title': u'Mode decomposition evolution equations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4757170/', 'p': '-', 'kwd': '-', 'title': u'WFITN 2015 Abstracts Oral Abstracts'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615055/', 'p': u'957 rest/stress 99mtechnetium gated MPS NC studies from 623 consecutive patients with correlating invasive coronary angiography and 334 with low likelihood of CAD (LLK < 5% ) were assessed. Patients with stenosis \u2265 50% in left main or \u2265 70% in all other vessels were considered abnormal. Total perfusion deficit (TPD) was computed automatically. In addition, ischemic changes (ISCH) and ejection fraction changes (EFC) between stress and rest were derived by quantitative software. The SVM was trained using a group of 125 pts (25 LLK, 25 0-, 25 1-, 25 2- and 25 3-vessel CAD) using above quantitative variables and second order polynomial fitting. The remaining patients (N = 832) were categorized based on probability estimates, with CAD defined as (probability estimate \u2265 0.50). The diagnostic accuracy of SVM was also compared to visual segmental scoring by two experienced readers.Sensitivity of SVM (84%) was significantly better than ISCH (75%, p < 0.05) and EFC (31%, p < 0.05). Specificity of SVM (88%) was significantly better than that of TPD (78%, p < 0.05) and EFC (77%, p < 0.05). Diagnostic accuracy of SVM (86%) was significantly better than TPD (81%), ISCH (81%), or EFC (46%) (p < 0.05 for all). The Receiver-operator-characteristic area-under-the-curve (ROC-AUC) for SVM (0.92) was significantly better than TPD (0.90), ISCH (0.87), and EFC (0.60) (p < 0.001 for all). Diagnostic accuracy of SVM was comparable to the overall accuracy of both visual readers (85% vs. 84%, p < 0.05). ROC-AUC for SVM (0.92) was significantly better than that of both visual readers (0.87 and 0.88, p < 0.03).', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit, Support Vector Machines, Machine Learning', 'title': u'Improved Accuracy of Myocardial Perfusion SPECT for the Detection of Coronary Artery Disease by Utilizing a Support Vector Machines Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563765/', 'p': u'995 rest/stress 99mTc-sestamibi MPS studies, [650 consecutive cases with coronary angiography and 345 with likelihood of CAD < 5% (LLk)] were obtained by MPS with AC. Total perfusion deficit (TPD) for AC and NC data were compared to the visual summed stress and rest scores of 2 experienced readers. Visual reads were performed in 4 consecutive steps with the following information progressively revealed: NC data, AC+NC data, computer results, all clinical information.The diagnostic accuracy of TPD for detection of CAD was similar to both readers (NC: 82% vs. 84%, AC: 86% vs. 85\u201387% p = NS) with the exception of second reader when using clinical information (89%, p < 0.05). The Receiver-Operator-Characteristics Areas-Under-Curve (ROC-AUC) for TPD were significantly better than visual reads for NC (0.91 vs. 0.87 and 0.89, p < 0.01) and AC (0.92 vs. 0.90, p < 0.01), and it was comparable to visual reads incorporating all clinical information. Per-vessel accuracy of TPD was superior to one reader for NC (81% vs. 77%, p < 0.05) and AC (83% vs. 78%, p < 0.05) and equivalent to second reader [NC (79%) and AC (81%)]. Per-vessel ROC-AUC for NC (0.83) and AC (0.84) for TPD were better than (0.78\u20130.80 p < 0.01), and comparable to second reader (0.82\u20130.84, p = NS), for all steps.', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit', 'title': u'Comparison of Fully Automated Computer Analysis and Visual Scoring for Detection of Coronary Artery Disease from Myocardial Perfusion SPECT in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2837185/', 'p': '-', 'kwd': u'Uncertainty, reporting, data mining', 'title': u'Uncovering and Improving Upon the Inherent Deficiencies of Radiology Reporting through Data Mining'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2759696/', 'p': u'To determine the performance of an artificial neural network in transcranial color-coded duplex sonography (TCCS) diagnosis of middle cerebral artery (MCA) spasm. TCCS was prospectively acquired within 2 h prior to routine cerebral angiography in 100 consecutive patients (54M:46F, median age 50 years). Angiographic MCA vasospasm was classified as mild (<25% of vessel caliber reduction), moderate (25\u201350%), or severe (>50%). A Learning Vector Quantization neural network classified MCA spasm based on TCCS peak-systolic, mean, and end-diastolic velocity data. During a four-class discrimination task, accurate classification by the network ranged from 64.9% to 72.3%, depending on the number of neurons in the Kohonen layer. Accurate classification of vasospasm ranged from 79.6% to 87.6%, with an accuracy of 84.7% to 92.1% for the detection of moderate-to-severe vasospasm. An artificial neural network may increase the accuracy of TCCS in diagnosis of MCA spasm.Following a 15-min rest period in supine position, TCCS was performed on a sonographic scanner (Toshiba Applio, Toshiba Medical System, Tokyo, Japan) equipped with a 2.5 MHz 90\xb0 phased-array probe with B-mode and Doppler imaging. Proximal segments of the basal cerebral arteries were insonated via a transtemporal and were identified on grayscale and color imaging in relation to intracranial structures (Krejza et al. 2000). A 3-mm wide sample volume was placed on the color image of the proximal MCA (M1) about 10 mm distal to the terminal carotid or at the site of the highest flow velocity acceleration indicated by aliasing phenomenon. A linear marker was placed under visual guidance on the color image of the insonated vascular segment along the long axis of the vessel to determine the angle of insonation. The angle between this linear marker and the ultrasound beam, displayed automatically on the screen of the scanner, was considered a two-dimensional approximation of the angle of insonation. A typical TCCS image of MCA spasm is shown on Fig. 1. Angle-corrected peak systolic (VPS), mean (VMEAN), and end diastolic (VED) blood flow velocities were subsequently obtained. Automated blood flow velocity determinations were used, although manual tracing of the maximum frequency envelope of the Doppler waveform was used to obtain these values when a weak Doppler signal was noted. An expert radiologist reviewed the TCCS data for quality purposes and rejected 42 waveforms due to inferior quality. Further analyses were based on the remaining 158 data sets, including waveforms of the left and right MCA.', 'kwd': u'Ultrasound, Cerebral blood vessels, Vasospasm, Artificial neural networks, Transcranial Doppler, Diagnosis, Blood velocity, Velocimetry, Brain arteries, Ischemia, Stroke', 'title': u'Learning Vector Quantization Neural Networks Improve Accuracy of Transcranial Color-coded Duplex Sonography in Detection of Middle Cerebral Artery Spasm\u2014Preliminary Report'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3548409/', 'p': u'Endocardial surfaces at the end-diastolic and end-systolic frames for post-stress and rest studies were registered automatically to each other by matching ventricular surfaces. Myocardial MTCs were computed and normal limits of change were determined as the mean and standard deviation for each polar sample. Normal limits were utilized to quantify the MTCs for each map and the accumulated sample values were used for abnormality assessments in segmental regions. A hybrid method was devised by combining the Total Perfusion Deficit (TPD) and MTC for each vessel territory. Normal limits were obtained from 100 subjects with low likelihood (LLK) of coronary artery disease (CAD). For validation, 623 subjects with correlating invasive angiography were studied. All subjects had a stress/rest 99mTc-sestamibi exercise or adenosine test, and all had coronary angiography within 3 months of MPS. All MTC and TPD measurements were derived automatically. The diagnostic accuracy for detection of coronary artery disease for MTC+TPD was compared to TPD alone.Segmental normal values for motion change were between \u22121.3 and \u22124.1 mm and between \u221230.1% and \u22129.8% for thickening change. MTC combined with TPD achieved 61% sensitivity for 3-vessel disease (3VD), 63% for 2-vessel disease (2VD), and 90% for 1-vessel disease (1VD) detection vs. 32% for 3VD (P <0.0001), 53% for 2VD (P < 0.001), and 90% for 1VD (P = 1.0) detection with TPD alone method. The specificity for the combined method was 71% for 3VD, 72% for 2VD, and 47% for 1 VD detection vs. 90% for 3VD (P < 0.0001), 80% for 2VD (P <0.001), and 50% for 1VD detection (P=0.0625) for TPD alone method. The accuracy of 3VD detection by MTC+TPD was higher (69%) than the accuracy of TPD + change in ejection fraction (63%), (P< 0.004).', 'kwd': u'Surface registration, Motion, Thickening, 3 vessel disease, Coronary artery disease, SPECT', 'title': u'Direct Quantification of Post-Stress-Rest Left Ventricular Motion and Thickening Changes for Myocardial Perfusion SPECT'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3473729/', 'p': u'The huge amount of information that needs to be assimilated in order to keep pace with the continued advances in modern medical practice can form an insurmountable obstacle to the individual clinician. Within radiology, the recent development of quantitative imaging techniques, such as perfusion imaging, and the development of imaging-based biomarkers in modern therapeutic assessment has highlighted the need for computer systems to provide the radiological community with support for academic as well as clinical/translational applications. This article provides an overview of the underlying design and functionality of radiological decision support systems with examples tracing the development and evolution of such systems over the past 40 years. More importantly, we discuss the specific design, performance and usage characteristics that previous systems have highlighted as being necessary for clinical uptake and routine use. Additionally, we have identified particular failings in our current methodologies for data dissemination within the medical domain that must be overcome if the next generation of decision support systems is to be implemented successfully.The huge amount of information that needs to be assimilated in order to keep pace with continued advances in modern medical practice can form an insurmountable obstacle to the individual clinician. In medical applications decision quality is of crucial importance, whilst human decision-making performance can be suboptimal and deteriorate as the complexity of the problem increases. For these reasons, the development of medical DSS is becoming increasingly important [1] and the routine uptake of these \u201cintelligent\u201d systems is becoming more common [2]. One of the earliest rule-based expert systems, DENDRAL [3], was implemented in the 1960s and was designed to provide support to organic chemists. This was further developed over the early 1970s by the same team at Stanford University into arguably the first rule-based medical DSS, MYCIN [4]. This system attempted to identify bacteria causing severe infections and recommend appropriate antibiotics. From these early DSS and the subsequent development of knowledge engineering, we now have DSS based on established architectures.', 'kwd': '-', 'title': u'Decision support systems for clinical radiological practice \u2014 towards the next generation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4659889/', 'p': u'Monitoring heart failure patients through continues assessment of sign and symptoms by information technology tools lead to large reduction in re-hospitalization. Agent technology is one of the strongest artificial intelligence areas; therefore, it can be expected to facilitate, accelerate, and improve health services especially in home care and telemedicine. The aim of this article is to provide an agent-based model for chronic heart failure (CHF) follow-up management.This research was performed in 2013-2014 to determine appropriate scenarios and the data required to monitor and follow-up CHF patients, and then an agent-based model was designed.', 'kwd': u'Health Information Systems, Heart Failure, Artificial Intelligence, Multi-agent Systems', 'title': u'Chronic Heart Failure Follow-up Management Based on Agent Technology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3372692/', 'p': '-', 'kwd': u'survey, radiology, machine learning, image registration, image segmentation, computer aided detection and diagnosis, functional MRI, content-based image retrieval, computed tomography, magnetic resonance imaging', 'title': u'Machine Learning and Radiology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4757185/', 'p': u'K Lobotesis, R Veltkamp, I Carpenter and R HodgsonR Kabra, J-L Saw, TJ Phillips, C Phatouros, T Singh, GJ Hankey, D Blacker, D Ghia, D Prentice and W McAuliffe', 'kwd': '-', 'title': u'WFITN 2015 Abstracts: Oral Expositions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680364/', 'p': u'Artificial neural networks (ANN) might help to diagnose coronary artery disease. This study aimed to determine whether the diagnostic accuracy of an ANN-based diagnostic system and conventional quantitation are comparable.The ANN was trained to classify potentially abnormal areas as true or false based on the nuclear cardiology expert interpretation of 1001 gated stress/rest 99mTc-MIBI images at 12 hospitals. The diagnostic accuracy of the ANN was compared with 364 expert interpretations that served as the gold standard of abnormality for the validation study. Conventional summed stress/rest/difference scores (SSS/SRS/SDS) were calculated and compared with receiver operating characteristics (ROC) analysis.', 'kwd': u'Artificial intelligence, Diagnostic imaging, Coronary artery disease, Nuclear cardiology, Computer-aided diagnosis', 'title': u'Diagnostic accuracy of an artificial neural network compared with statistical quantitation of myocardial perfusion images: a Japanese multicenter study'}], 'Angiography AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669887/', 'p': u'Through a marriage of spiral computed tomography (CT) and graphical volumetric image processing, CT angiography was born 20 years ago. Fueled by a series of technical innovations in CT and image processing, over the next 5\u201315 years, CT angiography toppled conventional angiography, the undisputed diagnostic reference standard for vascular disease for the prior 70 years, as the preferred modality for the diagnosis and characterization of most cardiovascular abnormalities. This review recounts the evolution of CT angiography from its development and early challenges to a maturing modality that has provided unique insights into cardiovascular disease characterization and management. Selected clinical challenges, which include acute aortic syndromes, peripheral vascular disease, aortic stent-graft and transcatheter aortic valve assessment, and coronary artery disease, are presented as contrasting examples of how CT angiography is changing our approach to cardiovascular disease diagnosis and management. Finally, the recently introduced capabilities for multispectral imaging, tissue perfusion imaging, and radiation dose reduction through iterative reconstruction are explored with consideration toward the continued refinement and advancement of CT angiography.The rapid evolution of fast CT scanners is congruent with Moore\u2019s law, which predicts the doubling of the density of transistors on integrated circuits approximately every 2 years. In addition to effecting the acquisition circuitry, Moore\u2019s law is also responsible for the rapid increase in computer performance/price ratios, without which the expense and time to reconstruct these high-resolution cone-beam volume acquisitions, consisting of hundreds to thousands of sections, would not be clinically practical. Moore\u2019s law is also directly responsible for the final enabler of clinical CT angiography: Because section-by-section inspection of CT angiographic images is neither efficient nor intuitive, visualization of CT angiography studies employs shaded surface displays, maximum intensity projections, and volume rendering (Fig 1b, 1c). While early datasets consisted of only tens of cross-sections, each of the many desired view directions required many seconds to compute on expensive workstations. Today, many vendors provide software that runs on inexpensive computers and is capable of interactive high-resolution volume rendering with advanced lighting effects based on thousands of contrast-enhanced CT sections with additional capabilities such as automated bone removal and curved planar reformatting. The analysis of CT angiographic datasets has evolved to the point where review of the transverse reconstructions is a secondary analysis to tailored visualization and quantitation tasks using application-specific postprocessing solutions.', 'kwd': '-', 'title': u'CT Angiography after 20 Years'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5617355/', 'p': u'Optical coherence tomography angiography (OCTA) is a functional extension of OCT that provides information on retinal and choroidal circulations without the need for dye injections. With the recent development of high-speed OCT systems and efficient algorithms, OCTA has become clinically feasible. In this review article, we discuss the technical principles of OCTA, including image processing and artifacts, and its clinical applications in ophthalmology. We summarize recent studies which qualitatively or quantitatively assess disease presentation, progression, and/or response to treatment.The generation of structural OCT images depends on the backscattering of light from tissue structures. The OCT signal can be reduced globally by media opacity, pupil vignetting, and defocusing of the light beam. In such cases, low-quality images will be generated, obscuring inner eye components and producing unreliable flow information. Using light sources with longer wavelengths can theoretically overcome mild opacities. Proper OCT beam focusing and centering, as well as pupil dilation, can enhance the quality of an OCT scan. Focal OCT signal weakening or loss can also be encountered beneath large vessels or hyperreflective retinal lesions, obscuring flow signal and producing shadow artifacts [Figure \u200b[Figure1b1b and \u200bandc,c, white arrows].', 'kwd': u'Choroidal neovascularization, diabetic retinopathy, glaucoma, optic disc, optical coherence tomography angiography, retina', 'title': u'Optical coherence tomography angiography: Technical principles and clinical applications in ophthalmology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4725949/', 'p': u'Conceived and designed the experiments: SS JT BF DH RW AH. Performed the experiments: SS AH. Analyzed the data: SS JT AH FY. Contributed reagents/materials/analysis tools: SS JT AH DH RW. Wrote the paper: SS JT AH BF DH RW FY.Trabecular meshwork (TM) bypass surgeries attempt to enhance aqueous humor outflow (AHO) to lower intraocular pressure (IOP). While TM bypass results are promising, inconsistent success is seen. One hypothesis for this variability rests upon segmental (non-360 degrees uniform) AHO. We describe aqueous angiography as a real-time and physiologic AHO imaging technique in model eyes as a way to simulate live AHO imaging.', 'kwd': '-', 'title': u'Aqueous Angiography: Real-Time and Physiologic Aqueous Humor Outflow Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4674198/', 'p': u'Invasive coronary angiography (ICA) was performed by femoral or radial approach. The MLD was measured in projections showing the most severe narrowing of three main coronary arteries (LAD, LCx, and RCA) by a radiologist with more than 15 years of experience in cardiac imaging. Similarly, 3 consecutive measurements of the MLD within the same lesion were obtained, and the mean value was averaged.Statistical analyses were performed using SPSS 21.0 (SPSS, Inc, Chicago, IL). All continuous variables were expressed as the mean\u200a\xb1\u200astandard deviation, while categorical variables were presented as frequencies or percentages. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) for the detection of significant stenosis (>50%) on CCTA were calculated for individual three main coronary arteries with ICA as the gold standard. Receiver-operating characteristic (ROC) curve analysis was used to evaluate the diagnostic value of CCTA using different image postprocessing approaches in the measurement of MLD and detection of coronary stenosis compared to ICA. The areas under the ROC curves (AUCs) were compared among these three methods. A difference with P-value of <0.05 was considered statistically significant.', 'kwd': '-', 'title': u'Coronary CT Angiography in Heavily Calcified Coronary Arteries: Improvement of Coronary Lumen Visualization and Coronary Stenosis Assessment With Image Postprocessing Methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479873/', 'p': u"Coronary CT angiography has been increasingly used in the diagnosis of coronary artery disease owing to rapid technological developments, which are reflected in the improved spatial and temporal resolution of the images. High diagnostic accuracy has been achieved with multislice CT scanners (64 slice and higher), and in selected patients coronary CT angiography is regarded as a reliable alternative to invasive coronary angiography. With high-quality coronary CT imaging increasingly being performed, patients can benefit from an imaging modality that provides a rapid and accurate diagnosis while avoiding an invasive procedure. Despite the tremendous contributions of coronary CT angiography to cardiac imaging, study results reported in the literature should be interpreted with caution as there are some limitations existing within the study design or related to patient risk factors. In addition, some attention must be given to the potential health risks associated with the ionising radiation received during cardiac CT examinations. Radiation dose associated with coronary CT angiography has raised serious concerns in the literature, as the risk of developing malignancy is not negligible. Various dose-saving strategies have been implemented, with some of the strategies resulting in significant dose reduction. The aim of this review is to present an overview of the role of coronary CT angiography on cardiac imaging, with focus on coronary artery disease in terms of the diagnostic and prognostic value of coronary CT angiography. Various approaches for dose reduction commonly recommended in the literature are discussed. Limitations of coronary CT angiography are identified. Finally, future directions and challenges with the use of coronary CT angiography are highlighted.There is no doubt that, with increasing technological improvements, coronary CT angiography will continue to play an important role in the detection and diagnosis of CAD. Justification is a shared responsibility between requesting physicians and radiologists. For cardiac imaging exposures, the primary tasks of the medical imaging specialists are to collaborate with referring cardiologists to direct patients to the most appropriate imaging modality for the required diagnostic task and to ensure that all technical aspects of the examination are optimised so that the acquired image quality is diagnostic while keeping the doses as low as possible [97]. This is particularly important for young individuals, especially women, for whom alternative diagnostic modalities that do not involve the use of ionising radiation should be considered, such as stress electrocardiography, echocardiography or MRI. The benefit-to-risk ratio for imaging patients suspected of CAD must be driven by the benefit and appropriateness of the CCTA examination requested by the physicians. The American College of Radiology's appropriateness criteria provide evidence-based guidelines to help physicians in recommending an appropriate imaging test [98]. Similarly, the European Commission's guidelines and UK's Royal College of Radiologists' referral guidelines for imaging also provide a detailed overview of clinical indications for imaging examinations including CT [99]. Physicians need to follow guidelines like national diagnostic reference levels for reducing radiation dosages.", 'kwd': '-', 'title': u'Coronary CT angiography: current status and continuing challenges'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4881033/', 'p': u'Optical coherence tomography (OCT)-based angiography is increasingly becoming a clinically useful and important imaging technique due to its ability to provide volumetric microvascular networks innervating tissue beds in vivo without a need for exogenous contrast agent. Numerous OCT angiography algorithms have recently been proposed for the purpose of contrasting microvascular networks. A general literature review is provided on the recent progress of OCT angiography methods and algorithms. The basic physics and mathematics behind each method together with its contrast mechanism are described. Potential directions for future technical development of OCT based angiography is then briefly discussed. Finally, by the use of clinical data captured from normal and pathological subjects, the imaging performance of vascular networks delivered by the most recently reported algorithms is evaluated and compared, including optical microangiography, speckle variance, phase variance, split-spectrum amplitude decorrelation angiography, and correlation mapping. It is found that the method that utilizes complex OCT signal to contrast retinal blood flow delivers the best performance among all the algorithms in terms of image contrast and vessel connectivity. The purpose of this review is to help readers understand and select appropriate OCT angiography algorithm for use in specific applications.OCT-based angiography, as one of the most attractive functional extensions to the current popular OCT imaging modality, has been reviewed. The review divided the various OCT-based angiography algorithms into three categories by considering the data type employed for imaging, including the complete (i.e., complex), the intensity and the phase of available OCT signals. The principle of each algorithm has been described with special attention given to the most recently reported and popular ones, including OMAG, speckle variance, phase variance, SSADA, and correlation mapping. A comparative study was performed, which assessed the performances of these methods using the clinical datasets captured from a normal and a pathological retina. From this study, we conclude that among the compared algorithms, complex OCT data based approaches, such as OMAG, may provide the best visual result of retinal microvascular networks in terms of imaging contrast, vessel connectivity, and SNR, most probably owning to the full use of OCT signals. We hope that this review has achieved its purpose to give a general survey on the reported OCT angiography technologies with their blood flow contrast mechanism for those readers, who are interested in the OCT-based angiography.', 'kwd': u'optical coherence tomography, optical coherence tomography-based angiography, optical microangiography, retinal imaging', 'title': u'Methods and algorithms for optical coherence tomography-based angiography: a review and comparison'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4722855/', 'p': u'In low-resolution phase contrast magnetic resonance angiography, the maximum intensity projected channel images will be blurred with consequent loss of vascular details. The channel images are enhanced using a stabilized deblurring filter, applied to each channel prior to combining the individual channel images. The stabilized deblurring is obtained by the addition of a nonlocal regularization term to the reverse heat equation, referred to as nonlocally stabilized reverse diffusion filter. Unlike reverse diffusion filter, which is highly unstable and blows up noise, nonlocal stabilization enhances intensity projected parallel images uniformly. Application to multichannel vessel enhancement is illustrated using both volunteer data and simulated multichannel angiograms. Robustness of the filter applied to volunteer datasets is shown using statistically validated improvement in flow quantification. Improved performance in terms of preserving vascular structures and phased array reconstruction in both simulated and real data is demonstrated using structureness measure and contrast ratio.The NLM based regularization term has the potential to suppress noise while preserving edges and other details more effectively than conventional regularization techniques. A common image restoration approach is to minimize a cost function such as ', 'kwd': u'nonlocal reverse diffusion, multichannel parallel images, maximum intensity projection, phased array reconstruction, phase contrast magnetic resonance angiography', 'title': u'Improved image reconstruction of low-resolution multichannel phase contrast angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4679245/', 'p': u'This article provides an overview of advanced image processing for three dimensional (3D) optical coherence tomographic (OCT) angiography of macular diseases, including age-related macular degeneration (AMD) and diabetic retinopathy (DR). A fast automated retinal layers segmentation algorithm using directional graph search was introduced to separates 3D flow data into different layers in the presence of pathologies. Intelligent manual correction methods are also systematically addressed which can be done rapidly on a single frame and then automatically propagated to full 3D volume with accuracy better than 1 pixel. Methods to visualize and analyze the abnormalities including retinal and choroidal neovascularization, retinal ischemia, and macular edema were presented to facilitate the clinical use of OCT angiography.The OCT angiography data was acquired using a commercial spectral domain OCT instrument (RTVue-XR; Optovue). It has a center wavelength of 840 nm with a full-width half-maximum bandwidth of 45 nm and an axial scan rate of 70 kHz. Volumetric macular scans consisted of a 3 \xd7 3 mm or 6 \xd7 6 mm area with a 1.6 mm depth (304 \xd7 304 \xd7 512 pixels). In the fast transverse scanning direction, 304 A-scans were sampled. Two repeated B-scans were captured at a fixed position before proceeding to the next location. A total of 304 locations along a 3 mm or 6 mm distance in the slow transverse direction were sampled to form a 3D data cube. The SSADA algorithm split the spectrum into 11 sub-spectra and detected blood flow by calculating the signal amplitude-decorrelation between two consecutive B-scans of the same location. All 608 B-scans in each data cube were acquired in 2.9 seconds. Two volumetric raster scans, including one x-fast scan and one y-fast scan, were obtained and registered [20].', 'kwd': u'(110.4500) Optical coherence tomography, (100.0100) Image processing, (100.2960) Image analysis, (170.4470) Ophthalmology', 'title': u'Advanced image processing for optical coherence tomographic angiography of macular diseases'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4528352/', 'p': u'This paper presents a new procedure for automatic extraction of the blood vessels and optic disk (OD) in fundus fluorescein angiogram (FFA). In order to extract blood vessel centerlines, the algorithm of vessel extraction starts with the analysis of directional images resulting from sub-bands of fast discrete curvelet transform (FDCT) in the similar directions and different scales. For this purpose, each directional image is processed by using information of the first order derivative and eigenvalues obtained from the Hessian matrix. The final vessel segmentation is obtained using a simple region growing algorithm iteratively, which merges centerline images with the contents of images resulting from modified top-hat transform followed by bit plane slicing. After extracting blood vessels from FFA image, candidates regions for OD are enhanced by removing blood vessels from the FFA image, using multi-structure elements morphology, and modification of FDCT coefficients. Then, canny edge detector and Hough transform are applied to the reconstructed image to extract the boundary of candidate regions. At the next step, the information of the main arc of the retinal vessels surrounding the OD region is used to extract the actual location of the OD. Finally, the OD boundary is detected by applying distance regularized level set evolution. The proposed method was tested on the FFA images from angiography unit of Isfahan Feiz Hospital, containing 70 FFA images from different diabetic retinopathy stages. The experimental results show the accuracy more than 93% for vessel segmentation and more than 87% for OD boundary extraction.Source of Support: Nil', 'kwd': u'Diabetic retinopathy, fast discrete curvelet transform, fundus fluorescein angiography, Hessian matrix, level set method', 'title': u'Analysis of Fundus Fluorescein Angiogram Based on the Hessian Matrix of Directional Curvelet Sub-bands and Distance Regularized Level Set Evolution'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3736499/', 'p': u'We address the problem of motion artifact reduction in digital subtraction angiography (DSA) using image registration techniques. Most of registration algorithms proposed for application in DSA, have been designed for peripheral and cerebral angiography images in which we mainly deal with global rigid motions. These algorithms did not yield good results when applied to coronary angiography images because of complex nonrigid motions that exist in this type of angiography images. Multiresolution and iterative algorithms are proposed to cope with this problem, but these algorithms are associated with high computational cost which makes them not acceptable for real-time clinical applications. In this paper we propose a nonrigid image registration algorithm for coronary angiography images that is significantly faster than multiresolution and iterative blocking methods and outperforms competing algorithms evaluated on the same data sets. This algorithm is based on a sparse set of matched feature point pairs and the elastic registration is performed by means of multilevel B-spline image warping. Experimental results with several clinical data sets demonstrate the effectiveness of our approach.The algorithm here is a summary of the operations involved in the registration of two images of a digital angiographic image sequence, presented and discussed in the previous section. Given a mask image and a live image from an angiographic image sequence, the registration is accomplished by carrying out the following steps.', 'kwd': '-', 'title': u'Nonrigid Image Registration in Digital Subtraction Angiography Using Multilevel B-Spline'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4866465/', 'p': u'We compared the performance of three OCT angiography (OCTA) methods: speckle variance, amplitude decorrelation and phase variance for imaging of the human retina and choroid. Two averaging methods, split spectrum and volume averaging, were compared to assess the quality of the OCTA vascular images. All data were acquired using a swept-source OCT system at 1040 nm central wavelength, operating at 100,000 A-scans/s. We performed a quantitative comparison using a contrast-to-noise (CNR) metric to assess the capability of the three methods to visualize the choriocapillaris layer. For evaluation of the static tissue noise suppression in OCTA images we proposed to calculate CNR between the photoreceptor/RPE complex and the choriocapillaris layer. Finally, we demonstrated that implementation of intensity-based OCT imaging and OCT angiography methods allows for visualization of retinal and choroidal vascular layers known from anatomic studies in retinal preparations. OCT projection imaging of data flattened to selected retinal layers was implemented to visualize retinal and choroidal vasculature. User guided vessel tracing was applied to segment the retinal vasculature. The results were visualized in a form of a skeletonized 3D model.Imaging of human subjects was performed under a protocol approved by the UC Davis Institutional Review Board. Three normal subjects (N1, N2, N3, ages: 63, 35, and 29 years), a patient diagnosed with geographic atrophy (GA, age 66), and a patient with age-related macular degeneration (AMD, age 64) were imaged with a swept-source OCT system. Subject preparation included instillation of eye drops: 1% Tropicamide and 2.5% phenylephrine for pupil dilation and cycloplegia. During imaging, head position was stabilized with a forehead rest and bite bar. The imaged retinal location was selected by guiding the gaze with a white fixation cross displayed on a liquid crystal display (LCD). Light exposures were at the level of 1.3mW, which is below the maximum of the ANSI laser safety standards [65, 66].', 'kwd': u'(110.0110) Imaging systems, (110.4500) Optical coherence tomography, (110.2960) Image analysis, (170.3880) Medical and biological imaging, (170.4470) Ophthalmology, (280.2490) Flow diagnostics', 'title': u'Comparison of amplitude-decorrelation, speckle-variance and phase-variance OCT angiography methods for imaging the human retina and choroid'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4882875/', 'p': u'Interventional left ventricular (LV) procedures integrating static 3D anatomy visualization are subject to mismatch with dynamic catheter movements due to prominent LV motion. We aimed to evaluate the accuracy of a recently developed acquisition and post-processing protocol for low radiation dose LV multi-phase rotational angiography (4DRA) in patients.4DRA image acquisition of the LV was performed as investigational acquisition in patients undergoing left-sided ablation (11 men; BMI = 24.7 \xb1 2.5 kg/m\xb2). Iodine contrast was injected in the LA, while pacing from the RA at a cycle length of 700 ms. 4DRA acquisition and reconstruction were possible in all 11 studies. Reconstructed images were post-processed using streak artefact reduction algorithms and an interphase registration-based filtering method, increasing contrast-to-noise ratio by a factor 8.2 \xb1 2.1. This enabled semi-automatic segmentation, yielding LV models of five equidistant phases per cardiac cycle. For evaluation, off-line 4DRA fluoroscopy registration was performed, and the 4DRA LV contours of the different phases were compared with the contours of five corresponding phases of biplane LV angiography, acquired in identical circumstances. Of the distances between these contours, 95% were <4 mm in both incidences. Effective radiation dose for 4DRA, calculated by patient-specific Monte-Carlo simulation, was 5.1 \xb1 1.1 mSv.', 'kwd': u'angiography, catheter ablation, imaging, tomography, ventricle', 'title': u'Multi-phase rotational angiography of the left ventricle to assist ablations: feasibility and accuracy of novel imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4347263/', 'p': u'The development of cardiac CT has provided a non-invasive alternative to echocardiography, exercise electrocardiogram, and invasive angiography and cardiac CT continues to develop at an exponential speed even now. The appropriate use of cardiac CT may lead to improvements in the medical performances of physicians and can reduce medical costs which eventually contribute to better public health. However, until now, there has been no guideline regarding the appropriate use of cardiac CT in Korea. We intend to provide guidelines for the appropriate use of cardiac CT in heart diseases based on scientific data. The purpose of this guideline is to assist clinicians and other health professionals in the use of cardiac CT for diagnosis and treatment of heart diseases, especially in patients at high risk or suspected of heart disease.This work was developed through collaboration between the Korean Society of Radiology and the Korean Society of Cardiology and has been published jointly by invitation and consent in Journal of the Korean Society of Radiology and Korean Journal of Radiology.', 'kwd': u'Guideline, Appropriateness criteria, Cardiac computed tomography', 'title': u'Korean Guidelines for the Appropriate Use of Cardiac CT'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5430594/', 'p': u'To study the impact of image quality on quantitative measurements and the frequency of segmentation error with optical coherence tomography angiography (OCTA).Seventeen eyes of 10 healthy individuals were included in this study. OCTA was performed using a swept-source device (Triton, Topcon). Each subject underwent three scanning sessions 1\u20132\xa0min apart; the first two scans were obtained under standard conditions and for the third session, the image quality index was reduced using application of a topical ointment. En face OCTA images of the retinal vasculature were generated using the default segmentation for the superficial and deep retinal layer (SRL, DRL). Intraclass correlation coefficient (ICC) was used as a measure for repeatability. The frequency of segmentation error, motion artifact, banding artifact and projection artifact was also compared among the three sessions.', 'kwd': u'OCT angiography, Image quality, Quantitative analysis, Image artefacts', 'title': u'Impact of image quality on OCT angiography based quantitative measurements'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017267/', 'p': u'To assess the ability of trabecular micro-bypass stents to improve aqueous humor outflow (AHO) in regions initially devoid of AHO as assessed by aqueous angiography.Enucleated human eyes (14 total from 7 males and 3 females [ages 52\u201384]) were obtained from an eye bank within 48 hours of death. Eyes were oriented by inferior oblique insertion, and aqueous angiography was performed with indocyanine green (ICG; 0.4%) or fluorescein (2.5%) at 10 mm Hg. With an angiographer, infrared and fluorescent images were acquired. Concurrent anterior segment optical coherence tomography (OCT) was performed, and fixable fluorescent dextrans were introduced into the eye for histologic analysis of angiographically positive and negative areas. Experimentally, some eyes (n = 11) first received ICG aqueous angiography to determine angiographic patterns. These eyes then underwent trabecular micro-bypass sham or stent placement in regions initially devoid of angiographic signal. This was followed by fluorescein aqueous angiography to query the effects.', 'kwd': u'minimally invasive glaucoma surgery, aqueous angiography, imaging', 'title': u'Aqueous Angiography\u2013Mediated Guidance of Trabecular Bypass Improves Angiographic Outflow in Human Enucleated Eyes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4819503/', 'p': u'We enhance intravascular ultrasound virtual histology (VH) tissue characterization by fully automatic quantification of the acoustic shadow behind calcified plaque. VH is unable to characterize atherosclerosis located behind calcifications. In this study, the quantified acoustic shadows are considered calcified to approximate the real dense calcium (DC) plaque volume. In total, 57 patients with 108 coronary lesions were included. A novel post-processing step is applied on the VH images to quantify the acoustic shadow and enhance the VH results. The VH and enhanced VH results are compared to quantitative computed tomography angiography (QTA) plaque characterization as reference standard. The correlation of the plaque types between enhanced VH and QTA differs significantly from the correlation with unenhanced VH. For DC, the correlation improved from 0.733 to 0.818. Instead of an underestimation of DC in VH with a bias of 8.5\xa0mm3, there was a smaller overestimation of 1.1\xa0mm3 in the enhanced VH. Although tissue characterization within the acoustic shadow in VH is difficult, the novel algorithm improved the DC tissue characterization. This algorithm contributes to accurate assessment of calcium on VH and could be applied in clinical studies.A novel automatic method was developed to combine VH tissue characterization with an acoustic shadow detection method in order to quantify calcified plaque behind the acoustic shadow. This is performed in five automatic steps as shown in Fig.\xa01:', 'kwd': u'Quantitative CT angiography, Plaque constitution, Tissue characterization, Acoustic shadow', 'title': u'Enhanced characterization of calcified areas in intravascular ultrasound virtual histology images by quantification of the acoustic shadow: validation against computed tomography coronary angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478169/', 'p': u'Contrast-enhanced MR angiography (CE-MRA) was first introduced for clinical studies approximately 20 years ago. Early work provided 3 to 4 mm spatial resolution with acquisition times in the 30 sec range. Since that time there has been continuing effort to provide improved spatial resolution with reduced acquisition time, allowing high resolution three-dimensional (3D) time-resolved studies. The purpose of this work is to describe how this has been accomplished. Specific technical enablers have been: improved gradients allowing reduced repetition times, improved k-space sampling and reconstruction methods, parallel acquisition particularly in two directions, and improved and higher count receiver coil arrays. These have collectively made high resolution time-resolved studies readily available for many anatomic regions. Depending on the application, approximate 1 mm isotropic resolution is now possible with frame times of several seconds. Clinical applications of time-resolved CE-MRA are briefly reviewed.Imaging of the carotid arteries and the intracranial vasculature is a major application of MRA, and multiple kinds of sequences can be used. Applications of time-resolved CE-MRA include capturing a clear arterial phase of the carotid (25,83-85) and intracranial arteries (24,86-90). Time-resolved acquisitions can simplify existing clinical protocols by avoiding need for a timing bolus (91). Large field of view (FOV) time-resolved acquisitions of the supraaortic vasculature have been developed for covering the great vessel origins and neurovascular anatomy in a single scan (92-94) but sometimes with a need to reduce A/P coverage or resolution to permit adequate temporal or spatial resolution. Single-phase CE-MRA using 2D SENSE with R = 9 and 16 has been demonstrated using a 16-element coil (95). A common target for time-resolved CE-MRA is to capture the rapid transit of contrast material in arterial-venous malformations (AVMs) (24,33,76,86,96-101), as shown with the HYPR technique in Figure 9.', 'kwd': u'MRA, contrast-enhanced MRA, time-resolved studies, fast imaging, parallel imaging', 'title': u'Recent Advances in 3D Time-Resolved Contrast-Enhanced MR Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3184392/', 'p': u'To evaluate the diagnostic performance of 320-slice computed tomography coronary angiography (CTA) in the evaluation of patients with prior coronary artery bypass grafting (CABG). Invasive coronary angiography (ICA) served as the standard of reference, using a quantitative approach.CTA studies were performed using CT equipment with 320 detector-rows, each 0.5\xa0mm wide, and a gantry rotation time of 0.35\xa0s. All grafts, recipient and nongrafted vessels were deemed interpretable or uninterpretable. The presence of significant (\u226550%) stenosis and occlusion were determined on vessel and patient basis. Results were compared to ICA using quantitative coronary angiography.', 'kwd': u'Multidetector computed tomography, Non-invasive coronary angiography, Coronary artery bypass grafts, Cardiac imaging, Cardiac-gated imaging techniques', 'title': u'Diagnostic performance of 320-slice multidetector computed tomography coronary angiography in patients after coronary artery bypass grafting'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3360862/', 'p': u'Coronary computed tomographic angiography (CCTA) is a non-invasive imaging modality for the visualization of the heart and coronary arteries. To fully exploit the potential of the CCTA datasets and apply it in clinical practice, an automated coronary artery extraction approach is needed. The purpose of this paper is to present and validate a fully automatic centerline extraction algorithm for coronary arteries in CCTA images. The algorithm is based on an improved version of Frangi\u2019s vesselness filter which removes unwanted step-edge responses at the boundaries of the cardiac chambers. Building upon this new vesselness filter, the coronary artery extraction pipeline extracts the centerlines of main branches as well as side-branches automatically. This algorithm was first evaluated with a standardized evaluation framework named Rotterdam Coronary Artery Algorithm Evaluation Framework used in the MICCAI Coronary Artery Tracking challenge 2008 (CAT08). It includes 128 reference centerlines which were manually delineated. The average overlap and accuracy measures of our method were 93.7% and 0.30\xa0mm, respectively, which ranked at the 1st and 3rd place compared to five other automatic methods presented in the CAT08. Secondly, in 50 clinical datasets, a total of 100 reference centerlines were generated from lumen contours in the transversal planes which were manually corrected by an expert from the cardiology department. In this evaluation, the average overlap and accuracy were 96.1% and 0.33\xa0mm, respectively. The entire processing time for one dataset is less than 2\xa0min on a standard desktop computer. In conclusion, our newly developed automatic approach can extract coronary arteries in CCTA images with excellent performances in extraction ability and accuracy.According to the processing pipeline displayed in Fig.\xa01, our automatic coronary tree extraction algorithm can be divided into several successive steps, i.e. pre-processing, improved Frangi\u2019s vesselness filtering, centerline extraction, branch searching and centerline refinement. These steps will be described in the next paragraphs.', 'kwd': u'Coronary computed tomographic angiography (CCTA), Coronary artery, Centerline, Automatic extraction', 'title': u'Automatic centerline extraction of coronary arteries in coronary computed tomographic angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4963017/', 'p': u'Over the past 25 years, optical coherence tomography (OCT) and adaptive optics (AO) ophthalmoscopy have revolutionised our ability to non-invasively observe the living retina. The purpose of this review is to highlight the techniques and human clinical applications of recent advances in OCT and adaptive optics scanning laser/light ophthalmoscopy (AOSLO) ophthalmic imaging.Optical coherence tomography retinal and optic nerve head (ONH) imaging technology allows high resolution in the axial direction resulting in cross-sectional visualisation of retinal and ONH lamination. Complementary AO ophthalmoscopy gives high resolution in the transverse direction resulting in en face visualisation of retinal cell mosaics. Innovative detection schemes applied to OCT and AOSLO technologies (such as spectral domain OCT, OCT angiography, confocal and non-confocal AOSLO, fluorescence, and AO-OCT) have enabled high contrast between retinal and ONH structures in three dimensions and have allowed in vivo retinal imaging to approach that of histological quality. In addition, both OCT and AOSLO have shown the capability to detect retinal reflectance changes in response to visual stimuli, paving the way for future studies to investigate objective biomarkers of visual function at the cellular level. Increasingly, these imaging techniques are being applied to clinical studies of the normal and diseased visual system.', 'kwd': u'adaptive optics ophthalmoscopy, angiography, optical coherence tomography, photoreceptors, retinal and choroidal vasculature, scanning laser ophthalmoscopy', 'title': u'The fundus photo has met its match: optical coherence tomography and adaptive optics ophthalmoscopy are here to stay'}], 'Risk Stratification AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4433206/', 'p': u'Conceived and designed the experiments: ZS XC. Performed the experiments: YL JL. Analyzed the data: YL JL Xudong Liu WK. Contributed reagents/materials/analysis tools: YL Xiaoyong Liu WK XZ FW. Wrote the paper: YL JL XC.Exfoliative cytology has been widely used for early diagnosis of oral squamous cell carcinoma (OSCC). Test outcome is reported as \u201cnegative\u201d, \u201catypical\u201d (defined as abnormal epithelial changes of uncertain diagnostic significance), and \u201cpositive\u201d (defined as definitive cellular evidence of epithelial dysplasia or carcinoma). The major challenge is how to properly manage the \u201catypical\u201d patients in order to diagnose OSCC early and prevent OSCC. In this study, we collected exfoliative cytology data, histopathology data, and clinical data of normal subjects (n=102), oral leukoplakia (OLK) patients (n=82), and OSCC patients (n=93), and developed a data analysis procedure for quantitative risk stratification of OLK patients. This procedure involving a step called expert-guided data transformation and reconstruction (EdTAR) which allows automatic data processing and reconstruction and reveals informative signals for subsequent risk stratification. Modern machine learning techniques were utilized to build statistical prediction models on the reconstructed data. Among the several models tested using resampling methods for parameter pruning and performance evaluation, Support Vector Machine (SVM) was found to be optimal with a high sensitivity (median>0.98) and specificity (median>0.99). With the SVM model, we constructed an oral cancer risk index (OCRI) which may potentially guide clinical follow-up of OLK patients. One OLK patient with an initial OCRI of 0.88 developed OSCC after 40 months of follow-up. In conclusion, we have developed a statistical method for qualitative risk stratification of OLK patients. This method may potentially improve cost-effectiveness of clinical follow-up of OLK patients, and help design clinical chemoprevention trial for high-risk populations.', 'kwd': '-', 'title': u'Quantitative Risk Stratification of Oral Leukoplakia with Exfoliative Cytology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4780431/', 'p': u'Great effort has been devoted in recent years to the development of sudden cardiac risk predictors as a function of electric cardiac signals, mainly obtained from the electrocardiogram (ECG) analysis. But these prediction techniques are still seldom used in clinical practice, partly due to its limited diagnostic accuracy and to the lack of consensus about the appropriate computational signal processing implementation. This paper addresses a three-fold approach, based on ECG indices, to structure this review on sudden cardiac risk stratification. First, throughout the computational techniques that had been widely proposed for obtaining these indices in technical literature. Second, over the scientific evidence, that although is supported by observational clinical studies, they are not always representative enough. And third, via the limited technology transfer of academy-accepted algorithms, requiring further meditation for future systems. We focus on three families of ECG derived indices which are tackled from the aforementioned viewpoints, namely, heart rate turbulence (HRT), heart rate variability (HRV), and T-wave alternans. In terms of computational algorithms, we still need clearer scientific evidence, standardizing, and benchmarking, siting on advanced algorithms applied over large and representative datasets. New scenarios like electronic health recordings, big data, long-term monitoring, and cloud databases, will eventually open new frameworks to foresee suitable new paradigms in the near future.This set of indices aims to improve the robustness of the HRV measurements in RR tachograms, and for this purpose, they distribute the series of observed RR intervals by following a specific geometric pattern, based on the probability density function of normal RR intervals or their first difference, or on the sampling distribution density of normal RR interval durations. Emerging patterns are then measured and classified in different categories and measuring the range or the geometric figure scatter. For instance, when trying to match a given RR histogram with a triangle pattern shape, the parameters better approximating the histogram provide with a measurement of the scatter by means of the triangle basis. The most usual geometrical methods are the the triangular index, the differential index, and the logarithmic index (see Malik et al., 1996 for further details).', 'kwd': u'sudden cardiac death, risk stratification, computational algorithms, scientific evidence, technology transfer, heart rate variability, heart rate turbulence, T\u2013wave alternans', 'title': u'Sudden Cardiac Risk Stratification with Electrocardiographic Indices - A Review on Computational Processing, Technology Transfer, and Scientific Evidence'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3925797/', 'p': u'Prostate biopsy parameters are commonly used to attribute cancer risk. A targeted approach to lesions found on imaging may have an impact on the risk attribution given to a man.To evaluate whether, based on computer simulation, targeting of lesions during biopsy results in reclassification of cancer risk when compared with transrectal ultrasound (TRUS) guided biopsy.', 'kwd': u'Prostate, Biopsy, Simulation, Risk', 'title': u'Prostate Cancer Risk Inflation as a Consequence of Image-targeted Biopsy of the Prostate: A Computer Simulation Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2816803/', 'p': u'Cardiac magnetic resonance (CMR) imaging enables accurate and reproducible quantification of measurements of global and regional ventricular function, blood flow, perfusion at rest and stress as well as myocardial injury. Recent advances in MR hardware and software have resulted in significant improvements in image quality and a reduction in imaging time. Methods for automated and robust assessment of the parameters of cardiac function, blood flow and morphology are being developed. This article reviews the recent advances in image acquisition and quantitative image analysis in CMR.The diagnosis and management of cardiac disease requires a precise assessment of the parameters of cardiac morphology and function. Cardiac magnetic resonance (CMR) imaging has shown to be a versatile non-invasive imaging modality providing accurate and reproducible assessment of global and ventricular regional function, blood flow, myocardial perfusion and myocardial scar. In addition to enhancing clinical decision making, the accuracy and reproducibility of the CMR quantitative measures of cardiac function and morphology allow research studies to be carried out with fewer subjects enhancing cost effectiveness. Significant recent advances have been made in the generation of new CMR acquisition protocols as well as MR hardware enabling more rapid image acquisition. Despite these advances, the quantitative analysis of the images often still relies on manual tracing of the contours in many images, a time-consuming process. Reliable automated or semi-automated image segmentation and analysis software allowing for reproducible and rapid quantification are under development. In this paper an overview is provided on some of the recent work that has been carried out on image acquisition, computerized quantitative image analysis methods and semi-automated contour detection software for CMR imaging. The emerging clinical applications of quantitative CMR parameters are highlighted.', 'kwd': u'Cardiac MRI, Quantification', 'title': u'Quantification in cardiac MRI: advances in image acquisition and processing'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481086/', 'p': u'Breast cancer is the most frequently diagnosed cancer in women. However, the exact cause(s) of breast cancer still remains unknown. Early detection, precise identification of women at risk, and application of appropriate disease prevention measures are by far the most effective way to tackle breast cancer. There are more than 70 common genetic susceptibility factors included in the current non-image-based risk prediction models (e.g., the Gail and the Tyrer-Cuzick models). Image-based risk factors, such as mammographic densities and parenchymal patterns, have been established as biomarkers but have not been fully incorporated in the risk prediction models used for risk stratification in screening and/or measuring responsiveness to preventive approaches. Within computer aided mammography, automatic mammographic tissue segmentation methods have been developed for estimation of breast tissue composition to facilitate mammographic risk assessment. This paper presents a comprehensive review of automatic mammographic tissue segmentation methodologies developed over the past two decades and the evidence for risk assessment/density classification using segmentation. The aim of this review is to analyse how engineering advances have progressed and the impact automatic mammographic tissue segmentation has in a clinical environment, as well as to understand the current research gaps with respect to the incorporation of image-based risk factors in non-image-based risk prediction models.Clinical evaluation has indicated that SFM and FFDM are similar in their ability to detect cancer [107]; however, FFDM is more effective at finding cancer in certain groups of the population, such as women who are premenopausal or perimenopausal, under the age of 50, and have dense breasts. This indicates that in this subgroup some anatomical regions are better visualised by FFDM than SFM. In particular, FFDM demonstrated improved image quality with significantly better depiction of the nipple, skin, pectoral muscle, and especially contrast in parenchymal and fatty tissue [108]. Note that digital mammography imaging generates two types of images for analysis, raw (\u201cfor processing\u201d) and vendor postprocessed (\u201cfor presentation\u201d), of which postprocessed images are commonly used in clinical practice.', 'kwd': '-', 'title': u'A Review on Automatic Mammographic Density and Parenchymal Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5313641/', 'p': u'Conventional cytogenetics can categorize patients with acute myeloid leukemia (AML) into favorable, intermediate, and unfavorable\u2010risk groups; however, patients with intermediate\u2010risk cytogenetics represent the major population with variable outcomes. Because molecular profiling can assist with AML prognosis and next\u2010generation sequencing allows simultaneous sequencing of many target genes, we analyzed 260 genes in 112 patients with de novo \nAML who received standard treatment. Multivariate analysis showed that karyotypes and mutation status of TET2,PHF6,KIT, and NPM1\nmutation/FLT3\u2010 internal tandem duplication (ITD)negative were independent prognostic factors for the entire cohort. Among patients with intermediate\u2010risk cytogenetics, patients with mutations in CEBPA\ndouble mutation, IDH2, and NPM1 in the absence of FLT3\u2010ITD were associated with improved Overall survival (OS), similar to those with favorable\u2010risk cytogenetics; patients with mutations in TET2,RUNX1,ASXL1, and DNMT3A were associated with reduced OS, similar to those with unfavorable\u2010risk cytogenetics. We concluded that integration of cytogenetic and molecular profiling improves prognostic stratification of patients into three groups with more distinct prognoses (P\xa0<\xa00.001) and significantly reduces the number of patients classified as intermediate risk. In addition, our study demonstrates that next\u2010generation sequencing (NGS)\u2010based multi\u2010gene sequencing is clinically applicable in establishing an accurate risk stratification system for guiding therapeutic decisions.The diagnosis of AML was based on the definition of World Health Organization. All of the enrolled patients received standard chemotherapy with or without allogeneic HSCT as previously described. The diagnosis of AML was based on the World Health Organization definition, and all of the enrolled patients received standard chemotherapy with or without allogeneic HSCT as described 16. The mononuclear cells of each bone marrow sample were also collected and cryopreserved in the biobank after the patients had signed informed consent. This study was approved by the Institutional Review Board of China Medical University Hospital (DMR101\u2010IRB2\u2010020).', 'kwd': u'Acute myeloid leukemia, gene mutations, next\u2010generation sequencing, precision medicine, prognosis', 'title': u'A targeted next\u2010generation sequencing in the molecular risk stratification of adult acute myeloid leukemia: implications for clinical practice'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4552341/', 'p': u"Migration and stratification are increasingly intertwined. One day soon it will be impossible to understand one without the other. Both focus on life chances. Stratification is about differential life chances - who gets what and why - and migration is about improving life chances - getting more of the good things of life. To examine the interconnections of migration and stratification, we address a mix of old and new questions, carrying out analyses newly enabled by a unique new data set on recent legal immigrants to the United States (the New Immigrant Survey). We look at immigrant processing and lost documents, depression due to the visa process, presentation of self, the race-ethnic composition of an immigrant cohort (made possible by the data for the first time since 1961), black immigration from Africa and the Americas, skin-color diversity among couples formed by U.S. citizen sponsors and immigrant spouses, and English fluency among children age 8\u201312 and their immigrant parents. We find, inter alia, that children of previously illegal parents are especially more likely to be fluent in English, that native-born U.S. citizen women tend to marry darker, that immigrant applicants who go through the visa process while already in the United States are more likely to have their documents lost and to suffer visa depression, and that immigration, by introducing accomplished black immigrants from Africa (notably via the visa lottery), threatens to overturn racial and skin color associations with skill. Our analyses show the mutual embeddedness of migration and stratification in the unfolding of the immigrants' and their children's life chances and the impacts on the stratification structure of the United States.Most family and employment immigrants require a sponsor. In the case of family immigrants, the sponsor is the relative who is already a citizen or legal permanent resident of the United States (for example, the U.S. citizen spouse or parent of a prospective immigrant). In the case of employment immigrants, the sponsor is the employing individual or firm. The sponsor files the initial petition that establishes the prospective immigrant's eligibility and starts the visa process. The requirement for a sponsor may be waived in certain cases. In the family visa classes, the sponsor requirement may be waived for the widow(er) and child of a deceased U.S. citizen (in a marriage that had existed for at least two years before the U.S. citizen's death) or for the spouse and child of an abusive citizen or LPR. In the case of employment visas, the sponsor requirement may be waived for certain classes of immigrants, including investors as well as immigrants of great renown. Sponsored immigrant cases thus require both the sponsor's petition and the prospective immigrant's application; nonsponsored immigrant cases in general require only the prospective immigrant's application.6", 'kwd': '-', 'title': u'Migration and stratification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5483837/', 'p': u'Metastatic clear cell renal cell cancer (mccRCC) portends a poor prognosis and urgently requires better clinical tools for prognostication as well as for prediction of response to treatment. Considerable investment in molecular risk stratification has sought to overcome the performance ceiling encountered by methods restricted to traditional clinical parameters. However, replication of results has proven challenging, and intratumoural heterogeneity (ITH) may confound attempts at tissue-based stratification.We investigated the influence of confounding ITH on the performance of a novel molecular prognostic model, enabled by pathologist-guided multiregion sampling (n\u2009=\u2009183) of geographically separated mccRCC cohorts from the SuMR trial (development, n\u2009=\u200922) and the SCOTRRCC study (validation, n\u2009=\u200922). Tumour protein levels quantified by reverse phase protein array (RPPA) were investigated alongside clinical variables. Regularised wrapper selection identified features for Cox multivariate analysis with overall survival as the primary endpoint.', 'kwd': u'Cancer, Tumour heterogeneity, Prognostic markers, Renal cell carcinoma, Tumour biomarkers', 'title': u'Overcoming intratumoural heterogeneity for reproducible molecular risk stratification: a case study in advanced kidney cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4224958/', 'p': u'Optimization of prostate biopsy requires addressing the shortcomings of standard systematic transrectal ultrasound guided biopsy, including false-negative rates, incorrect risk stratification, detection of clinically insignificant disease and the need for repeat biopsy. Magnetic resonance imaging is an evolving noninvasive imaging modality that increases the accurate localization of prostate cancer at the time of biopsy, and thereby enhances clinical risk assessment and improves the ability to appropriately counsel patients regarding therapy. In this review we 1) summarize the various sequences that comprise a prostate multiparametric magnetic resonance imaging examination along with its performance characteristics in cancer detection, localization and reporting standards; 2) evaluate potential applications of magnetic resonance imaging targeting in prostate biopsy among men with no previous biopsy, a negative previous biopsy and those with low stage cancer; and 3) describe the techniques of magnetic resonance imaging targeted biopsy and comparative study outcomes.A bibliographic search covering the period up to October 2013 was conducted using MEDLINE\xae/PubMed\xae. Articles were reviewed and categorized based on which of the 3 objectives of this review was addressed. Data were extracted, analyzed and summarized.', 'kwd': u'prostate, image-guided biopsy, magnetic resonance imaging, prostatic neoplasms, risk assessment', 'title': u'Optimization of Prostate Biopsy: the Role of Magnetic Resonance Imaging Targeted Biopsy in Detection, Localization and Risk Assessment'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4714781/', 'p': u'Selection of patients for abdominal aortic aneurysm (AAA) repair is currently based on aneurysm size, growth rate and symptoms. Molecular imaging of biological processes associated with aneurysm growth and rupture, e.g., inflammation and matrix remodeling, could improve patient risk stratification and lead to a reduction in AAA morbidity and mortality. 18F-fluorodeoxyglucose (FDG) positron emission tomography (PET) and ultrasmall superparamagnetic particles of iron oxide (USPIO) magnetic resonance imaging are two novel approaches to AAA imaging evaluated in clinical trials. A variety of other tracers, including those that target inflammatory cells and proteolytic enzymes (e.g., integrin \u03b1v\u03b23 and matrix metalloproteinases), have proven effective in preclinical models of AAA and show great potential for clinical translation.Inflammation, matrix remodeling with elastin degradation and compensatory collagen deposition, and medial smooth muscle cell rarefaction are the main pathological features of AAA (Figure 3). Both local and systemic (e.g., smoking) factors play a role in aneurysm development. Traditionally, AAA is thought to be associated with atherosclerosis, but the protective role of diabetes in AAA and differences in the predilection sites point to a more complex picture. Single gene mutations of matrix proteins which are major players in thoracic aortic aneurysm, appear to be less prevalent in AAA where focal vessel wall inflammation is associated with upregulation of proteolytic pathways, smooth muscle cell apoptosis, enhanced oxidative stress and neovascularization.1 The combination of extracellular matrix and more specifically elastin breakdown, in part mediated by matrix metalloproteinases (MMPs), and smooth muscle cell loss is responsible for the thinning of the media and aortic dilation. Intraluminal thrombus, present particularly in patients with advanced AAA is associated with increased local inflammation and proteolysis.8 Aneurysm expansion is a consequence of the interaction of local hemodynamic forces with a weakened vessel wall. Rupture occurs when wall stress exceeds tensile strength. The sites of low tensile strength are associated with inflammation, matrix remodeling and neovascularization.9, 10 These issues have been summarized in an excellent recent review.1', 'kwd': u'Abdominal aortic aneurysm, Aorta, Molecular imaging, Inflammation, Remodeling, PET, MRI, FDG', 'title': u'Novel Molecular Imaging Approaches to Abdominal Aortic Aneurysm Risk Stratification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5161439/', 'p': u'High-risk primary biliary cholangitis (PBC), defined by inadequate response at one year to Ursodeoxycholic acid (UDCA), is associated with disease progression and liver transplantation. Stratifying high-risk patients early would facilitate improved approaches to care. Using long-term follow-up data to define risk at presentation, 6 high-risk PBC patients and 8 low-risk patients were identified from biopsy, transplant and biochemical archival records. Formalin-fixed paraffin-embedded (FFPE) liver biopsies taken at presentation were graded (Scheuer and Nakanuma scoring) and gene expression analysed using the NanoString\xae nCounter PanCancer Immunity 770-gene panel. Principle component analysis (PCA) demonstrated discrete gene expression clustering between controls and high- and low-risk PBC. High-risk PBC was characterised by up-regulation of genes linked to T-cell activation and apoptosis, INF-\u03b3 signalling and leukocyte migration and down-regulation of those linked to the complement pathway. CDKN1a, up-regulated in high-risk PBC, correlated with significantly increased expression of its gene product, the senescence marker p21WAF1/Cip, by biliary epithelial cells. Our findings suggest high- and low-risk PBC are biologically different from disease outset and senescence an early feature in high-risk disease. Identification of a high-risk \u2018signal\u2019 early from standard FFPE tissue sections has clear clinical utility allowing for patient stratification and second-line therapeutic intervention.The concept behind this study was to identify patients from a historic PBC cohort who had had a routine diagnostic liver biopsy performed and progressed to have either a positive outcome (UDCA response sustained over at least 15\xa0years, alive and in full response at the study point (defined as low-risk patients)) or a poor outcome in terms of their PBC development (non-response to UDCA and progression leading to transplantation (defined as high-risk patients)), and to investigate any significant differences in the molecular characteristics of their liver tissue at the outset of disease (at a point where stratified second-line therapy to alter disease trajectory might be possible in the future). Transcriptomic profiling of archived formaldehyde-fixed paraffin embedded (FFPE) liver tissue was piloted using archive biopsy material from the pre-defined high and low risk patients. Ethical approval for this work is covered by the UK-PBC project (REC 14/NW/1146), a national cohort study.', 'kwd': u'PBC, UDCA, Prognosis, Stratification, NanoString\xae nCounter PanCancer Immunity Panel', 'title': u'Early Molecular Stratification of High-risk Primary Biliary Cholangitis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4653820/', 'p': u'Childhood trauma was assessed using a self-report questionnaire and an interview. We administered the Childhood Trauma Questionnaire (CTQ) (Bernstein et al. 1997), a well-validated 28 item measure of abuse experienced during childhood and adolescence, with separate subscales for physical, sexual, and emotional abuse (Bernstein et al. 1994, 1997). These subscales had good internal consistency (\u03b1=0.75 for physical abuse, \u03b1=0.94 for sexual abuse, and \u03b1=0.86 for emotional abuse). The Childhood Experiences of Care and Abuse (CECA) interview (Bifulco et al. 1994; 1997) was also administered to assess exposure to physical and sexual abuse during childhood and adolescence. Interrater reliability for maltreatment reports is excellent, and multiple validation studies suggest high agreement between siblings on reports of caregiver behaviors and maltreatment (Bifulco et al. 1994, 1997). Participants who reported physical or sexual abuse during the CECA interview or who had a score on any of the three CTQ abuse subscales (physical, sexual, and emotional abuse) above a previously identified threshold (Walker et al. 1999) were classified as having experienced childhood trauma. No participant was currently experiencing maltreatment, and the proper authorities were contacted in cases that raised safety concerns.As noted, we covaried white/nonwhite racial/ethnic status in all analyses given the racial/ethnic diversity of our sample. To further address concerns regarding population stratification (i.e., the presence of systematic differences in allele frequencies as a function of subpopulations in the sample; e.g., Pritchard and Rosenberg 1999), we inferred the underlying population structure from 40 ancestry-informative markers in a subset of participants (n=46). We reran analyses covarying the first two principal components from a principal components analysis of the ancestry-informative markers, along with the other covariates included in the models described. Our overall findings remained highly similar in effect size despite the smaller sample: rs4765914 genotype was still associated with amygdala volume (b=\u2212405.82, SE=145.45, \u03b2=\u22120.39, p=0.008), and rs1006737 genotype was still associated with amygdala activation during the emotional reactivity (b=6.71, SE=3.63, \u03b2=0.30, p=0.072) and emotion regulation (b=\u22127.79, SE=3.04, \u03b2=\u22120.43, p=0.014) trials. These findings suggested that the associations between CACNA1C genotype and amygdala structure and function were not solely the result of confounding effects of population structure.', 'kwd': '-', 'title': u'Variation in CACNA1C is Associated with Amygdala Structure and Function in Adolescents'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3621376/', 'p': u'T2-weighted cardiovascular magnetic resonance (CMR) is clinically-useful for imaging the ischemic area-at-risk and amount of salvageable myocardium in patients with acute myocardial infarction (MI). However, to date, quantification of oedema is user-defined and potentially subjective.We describe a highly automatic framework for quantifying myocardial oedema from bright blood T2-weighted CMR in patients with acute MI. Our approach retains user input (i.e. clinical judgment) to confirm the presence of oedema on an image which is then subjected to an automatic analysis. The new method was tested on 25 consecutive acute MI patients who had a CMR within 48\xa0hours of hospital admission. Left ventricular wall boundaries were delineated automatically by variational level set methods followed by automatic detection of myocardial oedema by fitting a Rayleigh-Gaussian mixture statistical model. These data were compared with results from manual segmentation of the left ventricular wall and oedema, the current standard approach.', 'kwd': u'Myocardial oedema, Bright blood T2-weighted CMR, Rayleigh-Gaussian mixture model, Level set', 'title': u'Highly automatic quantification of myocardial oedema in patients with acute myocardial infarction using bright blood T2-weighted CMR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5481887/', 'p': u'The tremendous clinical and aetiological diversity among individuals with autism spectrum disorder (ASD) has been a major obstacle to the development of new treatments, as many may only be effective in particular subgroups. Precision medicine approaches aim to overcome this challenge by combining pathophysiologically based treatments with stratification biomarkers that predict which treatment may be most beneficial for particular individuals. However, so far, we have no single validated stratification biomarker for ASD. This may be due to the fact that most research studies primarily have focused on the identification of mean case-control differences, rather than within-group variability, and included small samples that were underpowered for stratification approaches. The EU-AIMS Longitudinal European Autism Project (LEAP) is to date the largest multi-centre, multi-disciplinary observational study worldwide that aims to identify and validate stratification biomarkers for ASD.LEAP includes 437 children and adults with ASD and 300 individuals with typical development or mild intellectual disability. Using an accelerated longitudinal design, each participant is comprehensively characterised in terms of clinical symptoms, comorbidities, functional outcomes, neurocognitive profile, brain structure and function, biochemical markers and genomics. In addition, 51 twin-pairs (of which 36 had one sibling with ASD) are included to identify genetic and environmental factors in phenotypic variability.', 'kwd': u'Biomarkers, Cognition, Neuroimaging, MRI, EEG, Eye-tracking, Genetics', 'title': u'The EU-AIMS Longitudinal European Autism Project (LEAP): design and methodologies to identify and validate stratification biomarkers for autism spectrum disorders'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493967/', 'p': u'Given the low incidence of pancreatic cancer in the general population, screening of pancreatic cancer in the general population using invasive modalities is not feasible. Combination of invasive screening with noninvasive biomarkers for pancreatic cancer and its precancerous lesions has the potential to reduce mortality due to pancreatic cancer. In this review, we focus on biomarkers found in the blood that can indicate early-stage pancreatic cancer, and we discuss current strategies for screening for pancreatic cancer. We recently identified a unique alteration in apolipoprotein A2 isoforms in pancreatic cancer and its precancerous lesions, and we describe its clinical usefulness as a potential biomarker for the early detection and risk stratification of pancreatic cancer.Carcinoembryonic antigen (CEA) is the second most common serum biomarker used clinically to detect pancreatic cancer\xa0[37]. It was initially discovered as a biomarker for the detection of colorectal cancer. CEA reacts not only with colorectal cancer, but also with several other cancers such as pancreatic cancer. A recent compendium reported median CEA estimates of 54% sensitivity and 79% specificity in the detection of pancreatic cancer in an analysis of 13 studies reporting CEA values in a total of 1323 cases\xa0[43].', 'kwd': u'apolipoprotein A2 isoforms, early detection for pancreatic cancer, National Cancer Institute Early Detection Research Network, NCI EDRN, plasma/serum biomarker', 'title': u'Potential usefulness of apolipoprotein A2 isoforms for screening and risk stratification of pancreatic cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4193692/', 'p': u'Considering the diverse clinical presentation and likely polygenic etiology of schizophrenia, this investigation examined the effect of polygenic risk on a well-established intermediate phenotype for schizophrenia. We hypothesized that a measure of cumulative genetic risk based on additive effects of many genetic susceptibility loci for schizophrenia would predict prefrontal cortical inefficiency during working memory, a brain-based biomarker for the disorder. The present study combined imaging, genetic and behavioral data obtained by the Mind Clinical Imaging Consortium study of schizophrenia (n = 255). For each participant, we derived a polygenic risk score (PGRS), which was based on over 600 nominally significant single nucleotide polymorphisms, associated with schizophrenia in a separate discovery sample comprising 3322 schizophrenia patients and 3587 control participants. Increased polygenic risk for schizophrenia was associated with neural inefficiency in the left dorsolateral prefrontal cortex after covarying for the effects of acquisition site, diagnosis, and population stratification. We also provide additional supporting evidence for our original findings using scores based on results from the Psychiatric Genomics Consortium study. Gene ontology analysis of the PGRS highlighted genetic loci involved in brain development and several other processes possibly contributing to disease etiology. Our study permits new insights into the additive effect of hundreds of genetic susceptibility loci on a brain-based intermediate phenotype for schizophrenia. The combined impact of many common genetic variants of small effect are likely to better reveal etiologic mechanisms of the disorder than the study of single common genetic variants.Imaging, genetic and behavioral data from 255 participants of the Mind Clinical Imaging Consortium (MCIC) study of schizophrenia from 4 participating sites (the University of New Mexico [UNM], the University of Minnesota [UMN], Massachusetts General Hospital [MGH], and the University of Iowa [UI]) were used to determine genetic polymorphisms in cryo-conserved blood samples and to analyze whole-brain neural activity during a WkM task. All subjects gave written informed consent prior to study enrolment. The human subjects research committees at each of the 4 sites approved the study protocol. Out of a total of 248 participants, who passed genetic quality control procedures (see below), imaging data of 241 participants were available for genetic analysis, resulting in a final dataset of 92 schizophrenia patients and 114 healthy controls after imaging quality control steps (see below). Patients had a Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) diagnosis of schizophrenia (n = 88), schizophreniform disorder (n = 3), or schizoaffective disorder (n = 1), established using a Structured Clinical Interview for DSM disorders (SCID)32 and a review of case files by trained clinicians. In the initial cohort, controls were matched to the patient group for age, gender, and parental education and were excluded if they had a history of a medical or Axis I psychiatric diagnosis. The majority of participants were of Caucasian descent (102 healthy controls and 73 patients). For additional details about the participants and clinical measures, see Ehrlich et al19. For the replication analyses, we used 2 additional datasets from the International Schizophrenia Consortium (ISC) and from the Psychiatric Genomics Consortium (see below).', 'kwd': u'schizophrenia, DLPFC, working memory,  intermediate phenotype, fMRI, genetic risk score', 'title': u'Prefrontal Inefficiency Is Associated With Polygenic Risk for Schizophrenia'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537099/', 'p': u'Pretreatment risk stratification is key for personalized medicine. While many physicians rely on an \u201ceyeball test\u201d to assess whether patients will tolerate major surgery or chemotherapy, \u201ceyeballing\u201d is inherently subjective and difficult to quantify. The concept of morphometric age derived from cross-sectional imaging has been found to correlate well with outcomes such as length of stay, morbidity, and mortality. However, the determination of the morphometric age is time intensive and requires highly trained experts. In this study, we propose a fully automated deep learning system for the segmentation of skeletal muscle cross-sectional area (CSA) on an axial computed tomography image taken at the third lumbar vertebra. We utilized a fully automated deep segmentation model derived from an extended implementation of a fully convolutional network with weight initialization of an ImageNet pre-trained model, followed by post processing to eliminate intramuscular fat for a more accurate analysis. This experiment was conducted by varying window level (WL), window width (WW), and bit resolutions in order to better understand the effects of the parameters on the model performance. Our best model, fine-tuned on 250 training images and ground truth labels, achieves 0.93\xa0\xb1\xa00.02 Dice similarity coefficient (DSC) and 3.68\xa0\xb1\xa02.29% difference between predicted and ground truth muscle CSA on 150 held-out test cases. Ultimately, the fully automated segmentation system can be embedded into the clinical environment to accelerate the quantification of muscle and expanded to volume analysis of 3D datasets.We reformatted the manually tuned muscle segmentation maps created by domain experts as described previously into acceptable input for convolutional neural networks (CNN). As shown in Fig. \u200bFig.1,1, the axial images and their corresponding color-coded images served as original input data and ground truth labels, respectively. The main challenge for muscle segmentation is the accurate differentiation of muscle tissue from neighboring organs due to their overlapping HU ranges. We manually drew a boundary between organs and muscle, setting the inside region as additional segmentation class (\u201cInside\u201d) in an effort to train the neural network to learn distinguishing features of muscle for a precise segmentation from adjacent organs. The color-coded label images were assigned to pre-defined label indices, including 0 (black) for \u201cBackground\u201d, 1 (red) for \u201cMuscle\u201d, and 2 (green) for \u201cInside\u201d, before passing through CNNs for training as presented in Fig. \u200bFig.11.\n', 'kwd': u'Muscle segmentation, Convolutional neural networks, Computer-aided diagnosis (CAD), Computed tomography, Artificial intelligence, Deep learning', 'title': u'Pixel-Level Deep Segmentation: Artificial Intelligence Quantifies Muscle on Computed Tomography for Body Morphometric Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3424733/', 'p': u'We tested whether an assessment of myocardial scarring by cardiac magnetic resonance (CMR) would improve risk stratification in patients evaluated for implantable cardioverter-defibrillator implantation.Current SCD risk stratification emphasizes left-ventricular ejection fraction (LVEF), however the majority of patients suffering SCD have a preserved LVEF and many with poor LVEF do not benefit from ICD prophylaxis.', 'kwd': u'cardiovascular magnetic resonance, implantable cardioverter-defibrillator, myocardial scarring', 'title': u'Assessment of Myocardial Scarring Improves Risk Stratification in Patients Evaluated for Cardiac Defibrillator Implantation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3841469/', 'p': u'Coronary artery disease (CAD) is one of the leading causes of death in the US and a substantial health-care burden in all industrialized societies. In recent years we have witnessed a constant strive towards the development and the clinical application of novel or improved detection methods as well as therapies. Particularly, noninvasive imaging is a decisive component in the cardiovascular field. Image fusion is the ability of combining into a single integrated display the anatomical as well as the physiological data retrieved by separated modalities. Clinical evidence suggests that it represents a promising strategy in CAD assessment and risk stratification by significantly improving the diagnostic power of each modality independently considered and of the traditional side-by-side interpretation. Numerous techniques and approaches taken from the image registration field have been implemented and validated in the context of CAD assessment and management. Although its diagnostic power is widely accepted, additional technical developments are still needed to become a routinely used clinical tool.Clinical evidence suggests that image fusion can be a reliable and useful tool in the hands of clinicians for a more accurate diagnosis of CAD, specifically for ambiguous and borderline cases. Additional technical developments in the extraction of anatomical information are still in order for the whole procedure to become fully automated and enter the clinical practice.', 'kwd': u'image fusion, CAD diagnosis, computed tomography angiography, nuclear imaging', 'title': u'Multimodality image fusion for diagnosing coronary artery disease'}], 'Risk Score AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4668034/', 'p': u'Conceived and designed the experiments: M\xc5 MN. Performed the experiments: MT M\xc5. Wrote the paper: SR M\xc5 MN. Manually segmented the image data: MT M\xc5. Developed the segmentation methodology: SR LG JN. Implemented the methodology and analysed the data: SR. Contributed to the analysis and drafting of the paper: LG JN MT.Organotypic, three dimensional (3D) cell culture models of epithelial tumour types such as prostate cancer recapitulate key aspects of the architecture and histology of solid cancers. Morphometric analysis of multicellular 3D organoids is particularly important when additional components such as the extracellular matrix and tumour microenvironment are included in the model. The complexity of such models has so far limited their successful implementation. There is a great need for automatic, accurate and robust image segmentation tools to facilitate the analysis of such biologically relevant 3D cell culture models. We present a segmentation method based on Markov random fields (MRFs) and illustrate our method using 3D stack image data from an organotypic 3D model of prostate cancer cells co-cultured with cancer-associated fibroblasts (CAFs). The 3D segmentation output suggests that these cell types are in physical contact with each other within the model, which has important implications for tumour biology. Segmentation performance is quantified using ground truth labels and we show how each step of our method increases segmentation accuracy. We provide the ground truth labels along with the image data and code. Using independent image data we show that our segmentation method is also more generally applicable to other types of cellular microscopy and not only limited to fluorescence microscopy.', 'kwd': '-', 'title': u'Segmentation of Image Data from Complex Organotypic 3D Models of Cancer Tissues with Markov Random Fields'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5677362/', 'p': u'The segmentation of a high spatial resolution remote sensing image is a critical step in geographic object-based image analysis (GEOBIA). Evaluating the performance of segmentation without ground truth data, i.e., unsupervised evaluation, is important for the comparison of segmentation algorithms and the automatic selection of optimal parameters. This unsupervised strategy currently faces several challenges in practice, such as difficulties in designing effective indicators and limitations of the spectral values in the feature representation. This study proposes a novel unsupervised evaluation method to quantitatively measure the quality of segmentation results to overcome these problems. In this method, multiple spectral and spatial features of images are first extracted simultaneously and then integrated into a feature set to improve the quality of the feature representation of ground objects. The indicators designed for spatial stratified heterogeneity and spatial autocorrelation are included to estimate the properties of the segments in this integrated feature set. These two indicators are then combined into a global assessment metric as the final quality score. The trade-offs of the combined indicators are accounted for using a strategy based on the Mahalanobis distance, which can be exhibited geometrically. The method is tested on two segmentation algorithms and three testing images. The proposed method is compared with two existing unsupervised methods and a supervised method to confirm its capabilities. Through comparison and visual analysis, the results verified the effectiveness of the proposed method and demonstrated the reliability and improvements of this method with respect to other methods.A novel unsupervised method is proposed for evaluating the segmentation quality of VHR remote sensing images. This method uses a multidimensional spectral\u2013spatial feature set as the feature image, which is captured from a raw image using a bilateral filter and a Gabor wavelet filter. Based on this integrated feature set, q\xa0and MI, which respectively denote the spatial stratified heterogeneity and spatial autocorrelation, are computed to indicate the property of each segmentation result from different aspects. These two indicators are then combined into a single overall metric dM using a strategy of measuring the Mahalanobis distance of the quality points in the MI\xa0\u2212\xa0q\xa0space to reveal the segmentation quality. Evaluations of reference segmentation of two synthetic images and three remote sensing images indicate that applying the proposed method to a feature enhanced image yields superior results relative to the original image. The MRS and MSS segmentation algorithms with different parameters were applied to the three remote sensing images to produce multiple segmentation results for evaluation. The experimental results show that indicators q\xa0 and MI appropriately reflect the changes at different segmentation scales, and the combined metric dM clearly reveals the segmentation quality when applied to different algorithms and different parameters. The effectiveness of the combined metric, dM,\xa0 is further demonstrated by comparing two existing unsupervised measures and one supervised method. The results demonstrate the superior potential and robust performance of the proposed method.', 'kwd': u'high spatial resolution remote sensing, image segmentation, unsupervised segmentation evaluation, spatial stratified heterogeneity, statistical features', 'title': u'A Novel Unsupervised Segmentation Quality Evaluation Method for Remote Sensing Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756595/', 'p': u'We propose new methods for automatic segmentation of images based on an atlas of manually labeled scans and contours in the image. First, we introduce a Bayesian framework for creating initial label maps from manually annotated training images. Within this framework, we model various registration- and patch-based segmentation techniques by changing the deformation field prior. Second, we perform contour-driven regression on the created label maps to refine the segmentation. Image contours and image parcellations give rise to non-stationary kernel functions that model the relationship between image locations. Setting the kernel to the covariance function in a Gaussian process establishes a distribution over label maps supported by image structures. Maximum a posteriori estimation of the distribution over label maps conditioned on the outcome of the atlas-based segmentation yields the refined segmentation. We evaluate the segmentation in two clinical applications: the segmentation of parotid glands in head and neck CT scans and the segmentation of the left atrium in cardiac MR angiography images.Given a novel image I, we aim to infer its segmentation S based on an atlas that contains training images \u2110 =\xa0{I1,\xa0\u2026,\xa0In} with segmentations \U0001d4ae =\xa0{S1,\xa0\u2026,\xa0Sn}. A probabilistic label map \u2112 =\xa0{L1,\xa0\u2026,\xa0L\u03b7} specifies the likelihood for each label l \u2208 {1, ..., \u03b7} ', 'kwd': u'Image segmentation, atlas, patch, spectral clustering, Gaussian process, left atrium, parotid glands', 'title': u'Contour-Driven Atlas-Based Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5683197/', 'p': u'The MRBrainS131 (Mendrik et al., 2015) framework consists of MR images and manual segmentations from 20 patients (age, mean \xb1 standard deviation: 71 \xb1 4 years; 10 male, 10 female). The MR images were manually segmented in eight classes: white matter (WM), cortical grey matter (cGM), basal ganglia and thalami (BGT), cerebellum (CB), brain stem (BS), lateral ventricular cerebrospinal fluid (lvCSF), peripheral cerebrospinal fluid (pCSF), and WMH. Note that the MRBrainS13 challenge only includes evaluation of three combined tissue classes: white matter (including WMH), grey matter (including BGT) and CSF (pCSF and lvCSF) instead of all eight classes.Patients with type 2 diabetes mellitus and healthy controls were included from the Utrecht Diabetic Encephalopathy Study part 2 (UDES2) (Reijmer et al., 2013). The images used in MRBrainS13 were selected from the UDES2 cohort. From the UDES2 cohort we analysed images from 96 additional patients (age, mean \xb1 standard deviation: 71 \xb1 5 years; 58 male, 38 female; 51 with type 2 diabetes mellitus and 45 healthy controls). Reference segmentations of WMH were performed by manual outlining on the FLAIR images using relatively strict criteria (Brundel et al., 2014).', 'kwd': u'Brain MRI, Segmentation, White matter hyperintensities, Deep learning, Convolutional neural networks, Motion artefacts, Brain atrophy', 'title': u'Evaluation of a deep learning approach for the segmentation of brain tissues and white matter hyperintensities of presumed vascular origin in\xa0MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4000401/', 'p': u'In a preprocessing step, all 18 available atlases (17 for right parotid gland) were pre-aligned to one reference data-set in order to bring all atlas volumes into one common reference system. Using normalized mutual information metric22 all data-sets were registered rigidly onto one (randomly chosen) data-set.In pairwise registration, each image of the whole set of prealigned atlas images IS is registered nonrigidly to a test image I using a multiresolution, cubic B-Spline registration approach. In general, B-spline registration methods use cubic B-splines to define a displacement field that maps the voxels in the moving image to those in a reference image (also referred to as fixed image). In combination with a metric that quantifies the similarity between the fixed and moving images, the registration problem can be solved using a gradient descent optimization approach. In order to improve the speed of the registration a highly-efficient multicore implementation of multiresolution B-Spline registration algorithm presented by Sharp et al.23 was used in this project. In this approach, a grid alignment scheme is used in order to speed up necessary B-Spline interpolation and gradient computation.23 In this project, B-Spline registration was used in combination with a quasi-Newton optimizer.24 As proposed by Han et al.,5 a metric based on mutual information25 was used for pairwise registration in order to deal with potential changes in image contrast and to add robustness in the presence of image artifacts.', 'kwd': u'radiation therapy planning, atlas-based segmentation, geodesic active contours, InShape models', 'title': u'Automatic segmentation of head and neck CT images for radiotherapy treatment planning using multiple atlases, statistical appearance models, and geodesic active contours'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5609357/', 'p': u'Image segmentation pipelines often are sensitive to algorithm input parameters. Algorithm parameters optimized for a set of images do not necessarily produce good-quality-segmentation results for other images. Even within an image, some regions may not be well segmented due to a number of factors, including multiple pieces of tissue with distinct characteristics, differences in staining of the tissue, normal versus tumor regions, and tumor heterogeneity. Evaluation of quality of segmentation results is an important step in image analysis. It is very labor intensive to do quality assessment manually with large image datasets because a whole-slide tissue image may have hundreds of thousands of nuclei. Semi-automatic mechanisms are needed to assist researchers and application developers to detect image regions with bad segmentations efficiently.Our goal is to develop and evaluate a machine-learning-based semi-automated workflow to assess quality of nucleus segmentation results in a large set of whole-slide tissue images.', 'kwd': u'Classification, nuclei segmentation quality assessment, texture feature', 'title': u'A Methodology for Texture Feature-based Quality Assessment in Nucleus Segmentation of Histopathology Image'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4673359/', 'p': u'Glaucoma is the second leading cause of loss of vision in the world. Examining the head of optic nerve (cup-to-disc ratio) is very important for diagnosing glaucoma and for patient monitoring after diagnosis. Images of optic disc and optic cup are acquired by fundus camera as well as Optical Coherence Tomography. The optic disc and optic cup segmentation techniques are used to isolate the relevant parts of the retinal image and to calculate the cup-to-disc ratio. The main objective of this paper is to review segmentation methodologies and techniques for the disc and cup boundaries which are utilized to calculate the disc and cup geometrical parameters automatically and accurately to help the professionals in the glaucoma to have a wide view and more details about the optic nerve head structure using retinal fundus images. We provide a brief description of each technique, highlighting its classification and performance metrics. The current and future research directions are summarized and discussed.The Structured Analysis of Retina (STARE) dataset [22] is funded by the US National Institutes of Health. The project has 400 fundus images. Each image is diagnosis. The blood vessels are annotated in 40 images. The ONH is localized in 80 images. A TopCon TRV-50 fundus camera with 35\xb0 field of view was used to capture the images.', 'kwd': '-', 'title': u'Optic Disc and Optic Cup Segmentation Methodologies for Glaucoma Image Detection: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2939190/', 'p': u'We present and evaluate a new method for automatically labeling the subfields of the hippocampal formation in focal 0.4\xd70.5\xd72.0mm3 resolution T2-weighted magnetic resonance images that can be acquired in the routine clinical setting with under 5 min scan time. The method combines multi-atlas segmentation, similarity-weighted voting, and a novel learning-based bias correction technique to achieve excellent agreement with manual segmentation. Initial partitioning of MRI slices into hippocampal \u2018head\u2019, \u2018body\u2019 and \u2018tail\u2019 slices is the only input required from the user, necessitated by the nature of the underlying segmentation protocol. Dice overlap between manual and automatic segmentation is above 0.87 for the larger subfields, CA1 and dentate gyrus, and is competitive with the best results for whole-hippocampus segmentation in the literature. Intraclass correlation of volume measurements in CA1 and dentate gyrus is above 0.89. Overlap in smaller hippocampal subfields is lower in magnitude (0.54 for CA2, 0.62 for CA3, 0.77 for subiculum and 0.79 for entorhinal cortex) but comparable to overlap between manual segmentations by trained human raters. These results support the feasibility of subfield-specific hippocampal morphometry in clinical studies of memory and neurodegenerative disease.The imaging data for this study was collected by the Center for Imaging of Neurodegenerative Diseases (CIND) at the San Francisco Veterans Administration Medical Center. Our experiments use imaging data from 32 subjects, who participated in imaging studies at CIND. These subjects fall into three categories: control, mild cognitive impairment (MCI) of the AD type, and \u201ccognitively impaired, non-demented (CIND).\u201d Control (n=21) means cognitively intact control subject. Subjects in the MCI group (n=4) meet the diagnostic criteria in (Petersen et al., 1999) and also have a clinical diagnosis of MCI based on the consensus opinion of experienced neurologists. Subjects in the CIND group (n=7) have memory or executive deficits that are severe enough so that the referring clinicians suspected these subjects to be at risk for developing AD, but they do not fulfill the Research Criteria for MCI of the AD type or executive MCI. The subjects were between 38 and 82 years of age at the time of the scan, with average age 64.8\xb111.8 years. 18 subjects are male and 14 are female.', 'kwd': '-', 'title': u'Nearly Automatic Segmentation of Hippocampal Subfields in In Vivo Focal T2-Weighted MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440179/', 'p': u'An important image processing step in spinal cord magnetic resonance imaging is the ability to reliably and accurately segment grey and white matter for tissue specific analysis. There are several semi- or fully-automated segmentation methods for cervical cord cross-sectional area measurement with an excellent performance close or equal to the manual segmentation. However, grey matter segmentation is still challenging due to small cross-sectional size and shape, and active research is being conducted by several groups around the world in this field. Therefore a grey matter spinal cord segmentation challenge was organised to test different capabilities of various methods using the same multi-centre and multi-vendor dataset acquired with distinct 3D gradient-echo sequences. This challenge aimed to characterize the state-of-the-art in the field as well as identifying new opportunities for future improvements. Six different spinal cord grey matter segmentation methods developed independently by various research groups across the world and their performance were compared to manual segmentation outcomes, the present gold-standard. All algorithms provided good overall results for detecting the grey matter butterfly, albeit with variable performance in certain quality-of-segmentation metrics. The data have been made publicly available and the challenge web site remains open to new submissions. No modifications were introduced to any of the presented methods as a result of this challenge for the purposes of this publication.Image quality assessments were performed for each site at subject level. Signal-to-noise ratio within WM and contrast-to-noise ratio between GM and WM (Grussu et al., 2015, Yiannakas et al., 2016) were computed as: SNRWM =\xa0\u03bcWM/\u03c3WM and CNR=|\u03bcWM\u2212\u03bcGM|/\u03c3WM2+\u03c3GM2.', 'kwd': u'Spinal cord, Grey matter, Segmentation, MRI, Challenge, Evaluation metrics', 'title': u'Spinal cord grey matter segmentation challenge'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3970427/', 'p': u'The study of neural circuit reconstruction, i.e., connectomics, is a challenging problem in neuroscience. Automated and semi-automated electron microscopy (EM) image analysis can be tremendously helpful for connectomics research. In this paper, we propose a fully automatic approach for intra-section segmentation and inter-section reconstruction of neurons using EM images. A hierarchical merge tree structure is built to represent multiple region hypotheses and supervised classification techniques are used to evaluate their potentials, based on which we resolve the merge tree with consistency constraints to acquire final intra-section segmentation. Then, we use a supervised learning based linking procedure for the inter-section neuron reconstruction. Also, we develop a semi-automatic method that utilizes the intermediate outputs of our automatic algorithm and achieves intra-segmentation with minimal user intervention. The experimental results show that our automatic method can achieve close-to-human intra-segmentation accuracy and state-of-the-art inter-section reconstruction accuracy. We also show that our semi-automatic method can further improve the intra-segmentation accuracy.Our intra-section segmentation method uses pixel-wise membrane detection probability maps as input. A membrane detection map \u212c = {bi} is a probability image with each pixel intensity bi \u2208 [0, 1] as shown in Figure 1(b). Such membrane detection maps are commonly obtained using machine learning algorithms, such as (Jain et al., 2007; Laptev et al., 2012; Ciresan et al., 2012; Seyedhosseini et al., 2013). We can obtain a segmentation of the cells in the image by simply thresholding the probability map. With this approach, however, a few false negatives can cause significant under-segmentation errors. To address this problem, we propose a region-based method for 2D segmentation. Our method is independent of how the membrane detection probability maps are obtained. In Section 3, we experiment with the membrane detection probability maps learned with cascaded hierarchical models (CHM) (Seyedhosseini et al., 2013) and deep neural networks (DNN) (Ciresan et al., 2012), and we show that our method can significantly improve the segmentation accuracy over thresholding either of these probability maps.', 'kwd': u'Image segmentation, Electron microscopy, Hierarchical segmentation, Semi-automatic segmentation, Neuron reconstruction', 'title': u'A Modular Hierarchical Approach to 3D Electron Microscopy Image Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5686877/', 'p': u'Spinal diseases are very common; for example, the risk of osteoporotic fracture is 40% for White women and 13% for White men in the United States during their lifetime. Hence, the total number of surgical spinal treatments is on the rise with the aging population, and accurate diagnosis is of great importance to avoid complications and a reappearance of the symptoms. Imaging and analysis of a vertebral column is an exhausting task that can lead to wrong interpretations. The overall goal of this contribution is to study a cellular automata-based approach for the segmentation of vertebral bodies between the compacta and surrounding structures yielding to time savings and reducing interpretation errors.To obtain the ground truth, T2-weighted magnetic resonance imaging acquisitions of the spine were segmented in a slice-by-slice procedure by several neurosurgeons. Subsequently, the same vertebral bodies have been segmented by a physician using the cellular automata approach GrowCut.', 'kwd': u'Segmentation, vertebral body, GrowCut, magnetic resonance imaging, Dice Score', 'title': u'Vertebral body segmentation with GrowCut: Initial experience, workflow and practical application'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5423555/', 'p': u'The proposed framework was evaluated on a retrospective study comprising 15 prostate cancer patients treated with fixed-field intensity-modulated therapy (prescribed dose range, 67\u201382 Gy). All patients included in this study had given consent for their data to be used for research purposes. Each subject had a T2-weighted MR image (3 T, 2D spin echo; TE/TR: 80/2500\u2009ms; 1.46\xa0\xd7\xa01.46\xa0\xd7\xa05\xa0mm3), a T1-weighted MR image (3 T, 2D spin echo; TE/TR: 10/400\u2009ms; 1.64\xa0\xd7\xa01.64\xa0\xd7\xa05\xa0mm3), and a CT image (140 kVp, voxel size 0.98\xa0\xd7\xa00.98\xa0\xd7\xa01.5\xa0mm3), all acquired the same day. Delineations of the organs were performed manually by a qualified clinician for each modality independently. Note that a different couch was used for the MR (curved couch) and CT (flat couch) imaging sessions. Image preprocessing consisted of resampling the MR images to isotropic resolution using a cubic spline interpolation, and performing intensity non-uniformity correction (Tustison et al\n2010).The initial atlas database was composed of three image-segmentation pairs (ISPs) per subject, one for each data type (T1, T2, CT). The T1 and CT ISPs were registered to the T2 ISP using an affine followed by a multi-channel non-rigid registration (Modat et al\n2012). The similarity measure used to non-rigidly register two ISPs was defined as the LNCC over the intensity data and KLD over the segmentations, thus aligning both imaging and segmentation data, similarly to Dowling et al (2015). All the non-rigid registrations were performed with a pyramidal approach with three levels. The finer lattice of control points had a spacing of 2.5\u2009mm along each axis for the T1 to T2 registrations and 7.5\u2009mm along each axis for the CT to T2 registrations. A linear interpolation was used during the optimisation. After all registrations, the three ISPs per subject were aligned to each other.', 'kwd': u'segmentation, image synthesis, atlas-based methods, pseudo CT, MRI-only RTP', 'title': u'Iterative framework for the joint segmentation and CT synthesis of MR images: application to MRI-only radiotherapy treatment planning'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5158033/', 'p': '-', 'kwd': '-', 'title': u'Identification of Alfalfa Leaf Diseases Using Image Recognition Technology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4949270/', 'p': u"Recent advances in multi-atlas based algorithms address many of the previous limitations in model-based and probabilistic segmentation methods. However, at the label fusion stage, a majority of algorithms focus primarily on optimizing weight-maps associated with the atlas library based on a theoretical objective function that approximates the segmentation error. In contrast, we propose a novel method\u2014Autocorrecting Walks over Localized Markov Random Fields (AWoL-MRF)\u2014that aims at mimicking the sequential process of manual segmentation, which is the gold-standard for virtually all the segmentation methods. AWoL-MRF begins with a set of candidate labels generated by a multi-atlas segmentation pipeline as an initial label distribution and refines low confidence regions based on a localized Markov random field (L-MRF) model using a novel sequential inference process (walks). We show that AWoL-MRF produces state-of-the-art results with superior accuracy and robustness with a small atlas library compared to existing methods. We validate the proposed approach by performing hippocampal segmentations on three independent datasets: (1) Alzheimer's Disease Neuroimaging Database (ADNI); (2) First Episode Psychosis patient cohort; and (3) A cohort of preterm neonates scanned early in life and at term-equivalent age. We assess the improvement in the performance qualitatively as well as quantitatively by comparing AWoL-MRF with majority vote, STAPLE, and Joint Label Fusion methods. AWoL-MRF reaches a maximum accuracy of 0.881 (dataset 1), 0.897 (dataset 2), and 0.807 (dataset 3) based on Dice similarity coefficient metric, offering significant performance improvements with a smaller atlas library (< 10) over compared methods. We also evaluate the diagnostic utility of AWoL-MRF by analyzing the volume differences per disease category in the ADNI1: Complete Screening dataset. We have made the source code for AWoL-MRF public at: https://github.com/CobraLab/AWoL-MRF.MAGeT-Brain (https://github.com/CobraLab/MAGeTbrain)\u2014a segmentation pipeline previously developed by our group, is used as a baseline method for comparison (Pipitone et al., 2014). MAGeT-Brain uses multiple manually labeled anatomical atlases and a bootstrapping method to generate a large set of candidate labels (votes) for each voxel for a given target image to be segmented. These labels are generated by first randomly selecting a subset of target images, which is referred as a template library. Then the atlas segmentations are propagated to the template library via transformations estimated by nonlinear image registration. Subsequently, these template library segmentations are propagated to each target image and these candidate labels are fused using a label fusion method. The number of candidate labels is dependent on the number of available atlases and number of templates. In a default MAGeT-Brain configuration, the candidate labels are fused by a majority vote. In previous investigations by our group (Chakravarty et al., 2013; Pipitone et al., 2014), we observed no improvements when we used cross correlation and normalized mutual information based weighted voting (Studholme et al., 1999). For the purposes of this manuscript, candidate labels generated using MAGeT-Brain will be used to serve as the input to AWoL-MRF, STAPLE, and the default majority vote label fusion methods. The use of candidate labels is non-trivial in the case of label fusion with JLF, as this method requires coupled atlas image and label pairs as input. The permutations in MAGeT-Brain pipeline generate candidate labels totaling to number of atlases \xd7 number of templates. These candidate labels no longer have unique corresponding intensity images associated with them. The use of identical atlas (or template) library images as proxies is likely to deteriorate the performance of JLF, as it models the joint probability of two atlases making a segmentation error based on intensity similarity between a pair of atlases and the target image (Wang et al., 2012). Therefore, no template library is used during JLF evaluation. Note that even though MAGeT-Brain is used as a baseline method for the performance validation in this work, AWoL-MRF is a generic label fusion algorithm that can be used with any multi-atlas segmentation pipeline that produces a set of candidate labels.", 'kwd': u"MR Imaging, segmentation, multi-atlas label fusion, hippocampus, Alzheimer's disease, first-episode-psychosis, premature birth and neonates", 'title': u'Manual-Protocol Inspired Technique for Improving Automated MR Image Segmentation during Label Fusion'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5505987/', 'p': u'The anatomical location of imaging features is of crucial importance for accurate diagnosis in many medical tasks. Convolutional neural networks (CNN) have had huge successes in computer vision, but they lack the natural ability to incorporate the anatomical location in their decision making process, hindering success in some medical image analysis tasks. In this paper, to integrate the anatomical location information into the network, we propose several deep CNN architectures that consider multi-scale patches or take explicit location features while training. We apply and compare the proposed architectures for segmentation of white matter hyperintensities in brain MR images on a large dataset. As a result, we observe that the CNNs that incorporate location information substantially outperform a conventional segmentation method with handcrafted features as well as CNNs that do not integrate location information. On a test set of 50 scans, the best configuration of our networks obtained a Dice score of 0.792, compared to 0.805 for an independent human observer. Performance levels of the machine and the independent human observer were not statistically significantly different (p-value\u2009=\u20090.06).Subjects for the RUN DMC study were selected at baseline based on the following inclusion criteria1: (a) aged between 50 and 85 years (b) cerebral SVD on neuroimaging (appearance of WMHs and/or lacunes). Exclusion criteria comprised: presence of (a) dementia (b) parkinson(-ism) (c) intracranial hemorrhage (d) life expectancy less than six months (e) intracranial space occupying lesion (f) (psychiatric) disease interfering with cognitive testing or follow-up (g) recent or current use of acetylcholine-esterase inhibitors, neuroleptic agents, L-dopa or dopa-a(nta)gonists (h) non-SVD related WMH (e.g. MS) (i) prominent visual or hearing impairment (j) language barrier and (k) MRI contraindications. Based on these criteria, MRI scans of 503 patients were taken at baseline.', 'kwd': '-', 'title': u'Location Sensitive Deep Convolutional Neural Networks for Segmentation of White Matter Hyperintensities'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5263212/', 'p': u'We propose a fully automated method for detection and segmentation of the abnormal tissue associated with brain tumour (tumour core and oedema) from Fluid- Attenuated Inversion Recovery (FLAIR) Magnetic Resonance Imaging (MRI).The method is based on superpixel technique and classification of each superpixel. A number of novel image features including intensity-based, Gabor textons, fractal analysis and curvatures are calculated from each superpixel within the entire brain area in FLAIR MRI to ensure a robust classification. Extremely randomized trees (ERT) classifier is compared with support vector machine (SVM) to classify each superpixel into tumour and non-tumour.', 'kwd': u'Brain tumour segmentation, Extremely randomized trees, Feature selection, Magnetic resonance imaging, Superpixels, Textons', 'title': u'Automated brain tumour detection and segmentation using superpixel-based extremely randomized trees in FLAIR MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3570946/', 'p': u"This paper overviews one of the most important, interesting, and challenging problems in oncology, the problem of lung cancer diagnosis. Developing an effective computer-aided diagnosis (CAD) system for lung cancer is of great clinical importance and can increase the patient's chance of survival. For this reason, CAD systems for lung cancer have been investigated in a huge number of research studies. A typical CAD system for lung cancer diagnosis is composed of four main processing steps: segmentation of the lung fields, detection of nodules inside the lung fields, segmentation of the detected nodules, and diagnosis of the nodules as benign or malignant. This paper overviews the current state-of-the-art techniques that have been developed to implement each of these CAD processing steps. For each technique, various aspects of technical issues, implemented methodologies, training and testing databases, and validation methods, as well as achieved performances, are described. In addition, the paper addresses several challenges that researchers face in each implementation step and outlines the strengths and drawbacks of the existing approaches for lung cancer CAD systems. Several challenges and aspects have been facing CAD systems for lung cancer. These challenges can be summarized as follows.", 'kwd': '-', 'title': u'Computer-Aided Diagnosis Systems for Lung Cancer: Challenges and Methodologies'}], 'Risk Score AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5219634/', 'p': u'Amyotrophic lateral sclerosis (ALS) is a progressive neuromuscular disease, with large variation in survival between patients. Currently, it remains rather difficult to predict survival based on clinical parameters alone. Here, we set out to use clinical characteristics in combination with MRI data to predict survival of ALS patients using deep learning, a machine learning technique highly effective in a broad range of big-data analyses. A group of 135 ALS patients was included from whom high-resolution diffusion-weighted and T1-weighted images were acquired at the first visit to the outpatient clinic. Next, each of the patients was monitored carefully and survival time to death was recorded. Patients were labeled as short, medium or long survivors, based on their recorded time to death as measured from the time of disease onset. In the deep learning procedure, the total group of 135 patients was split into a training set for deep learning (n\xa0=\xa083 patients), a validation set (n\xa0=\xa020) and an independent evaluation set (n\xa0=\xa032) to evaluate the performance of the obtained deep learning networks. Deep learning based on clinical characteristics predicted survival category correctly in 68.8% of the cases. Deep learning based on MRI predicted 62.5% correctly using structural connectivity and 62.5% using brain morphology data. Notably, when we combined the three sources of information, deep learning prediction accuracy increased to 84.4%. Taken together, our findings show the added value of MRI with respect to predicting survival in ALS, demonstrating the advantage of deep learning in disease prognostication.Each of the 135 patients was categorized according to the true survival time (i.e. time between disease onset and death): short survivors with survival up to 25\xa0months after disease onset, medium survivors with survival between 25 and 50\xa0months after disease onset, and long survivors living over 50\xa0months after disease onset (Elamin et al., 2015). The group of long survivors consisted of patients who either died after a disease duration of at least 50\xa0months or were still alive and had a disease duration of at least 50\xa0months at time of analysis.', 'kwd': u'Deep learning, Neural network, Amyotrophic lateral sclerosis, White matter connectivity, Survival, Prediction', 'title': u'Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4491543/', 'p': u'Rewards are crucial objects that induce learning, approach behavior, choices, and emotions. Whereas emotions are difficult to investigate in animals, the learning function is mediated by neuronal reward prediction error signals which implement basic constructs of reinforcement learning theory. These signals are found in dopamine neurons, which emit a global reward signal to striatum and frontal cortex, and in specific neurons in striatum, amygdala, and frontal cortex projecting to select neuronal populations. The approach and choice functions involve subjective value, which is objectively assessed by behavioral choices eliciting internal, subjective reward preferences. Utility is the formal mathematical characterization of subjective value and a prime decision variable in economic choice theory. It is coded as utility prediction error by phasic dopamine responses. Utility can incorporate various influences, including risk, delay, effort, and social interaction. Appropriate for formal decision mechanisms, rewards are coded as object value, action value, difference value, and chosen value by specific neurons. Although all reward, reinforcement, and decision variables are theoretical constructs, their neuronal signals constitute measurable physical implementations and as such confirm the validity of these concepts. The neuronal reward signals provide guidance for behavior while constraining the free will to act.Rewards are attractive. They are motivating and make us exert an effort. We want rewards; we do not usually remain neutral when we encounter them. Rewards induce approach behavior, also called appetitive or preparatory behavior, and consummatory behavior. We want to get closer when we encounter them, and we prepare to get them. We cannot get the meal, or a mating partner, if we do not approach them. Rewards usually do not come alone, and we often can choose between different rewards. We find some rewards more attractive than others and select the best reward. Thus we value rewards and then decide between them to get the best value. Then we consume them. So, rewards are attractive and elicit approach behavior that helps to consume the reward. Thus any stimulus, object, event, activity, or situation that has the potential to make us approach and consume it is by definition a reward.', 'kwd': '-', 'title': u'Neuronal Reward and Decision Signals: From Theories to Data'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5601479/', 'p': u'Translating the vast data generated by genomic platforms into accurate predictions of clinical outcomes is a fundamental challenge in genomic medicine. Many prediction methods face limitations in learning from the high-dimensional profiles generated by these platforms, and rely on experts to hand-select a small number of features for training prediction models. In this paper, we demonstrate how deep learning and Bayesian optimization methods that have been remarkably successful in general high-dimensional prediction tasks can be adapted to the problem of predicting cancer outcomes. We perform an extensive comparison of Bayesian optimized deep survival models and other state of the art machine learning methods for survival analysis, and describe a framework for interpreting deep survival models using a risk backpropagation technique. Finally, we illustrate that deep survival models can successfully transfer information across diseases to improve prognostic accuracy. We provide an open-source software implementation of this framework called SurvivalNet that enables automatic training, evaluation and interpretation of deep survival models.An overview of the SurvivalNet framework is presented in Fig.\xa01. SurvivalNet is implemented as an open-source Python module (https://github.com/CancerDataScience/SurvivalNet) using Theano and is available as a pre-built Docker software container. A deep survival model uses the Cox partial log likelihood to train the weights of neural network to transform molecular features into explanatory factors that explain survival. The partial log likelihood serves as a feedback signal to train the model weights using backpropagation. Deep neural networks have many hyperparameters that impact prediction accuracy including the number of layers, number and type of activation functions in each layer, and choices for optimization/regularization procedures. The time needed to train a deep survival model prohibits exhaustive hyperparameter search, and so SurvivalNet employs a Bayesian optimization strategy to identify hyperparameters that optimize prediction accuracy including the number of network layers, the number of elements in each layer, the activation function, and the dropout fraction. Bayesian optimization enables users who lack experience tuning neural networks to optimize model designs automatically, and results in considerable savings in time and effort as previously reported19. Data is first split into training (60%), validation (20%), and testing (20%) sets. Training samples are used to train the model weights with backpropagation using the network design suggested by Bayesian optimization. The prediction accuracy of the trained deep survival model is then estimated using the validation samples, and is used to maintain a probabilistic model of performance as a function of hyperparamters. Based on the probabilistic model, the design with the best expected accuracy is inferred as the next design to test. After the Bayesian optimization process is finished (typically after a prescribed number of experiments), the best network design is used to re-train a deep survival model using the training\u2009and \u2009validation samples, and the accuracy of this best model is reported using the held-out testing samples.', 'kwd': '-', 'title': u'Predicting clinical outcomes from large scale cancer genomic profiles with deep survival models'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5661078/', 'p': u'Molecular imaging enables the visualization and quantitative analysis of the alterations of biological procedures at molecular and/or cellular level, which is of great significance for early detection of cancer. In recent years, deep leaning has been widely used in medical imaging analysis, as it overcomes the limitations of visual assessment and traditional machine learning techniques by extracting hierarchical features with powerful representation capability. Research on cancer molecular images using deep learning techniques is also increasing dynamically. Hence, in this paper, we review the applications of deep learning in molecular imaging in terms of tumor lesion segmentation, tumor classification, and survival prediction. We also outline some future directions in which researchers may develop more powerful deep learning models for better performance in the applications in cancer molecular imaging.The papers on diverse applications of deep learning in different molecular imaging of cancer published from 2014 onwards were included. This review contains 25 papers and is organized according to the application of deep learning in cancer molecular imaging, including tumor lesion segmentation, cancer classification, and prediction of patient survival.  Table 1 summarizes the 13 different studies on tumor lesion segmentation, while Table 2 summarizes the 10 different studies on cancer classification. Two interesting papers on prediction of patient survival are also reviewed (Table 3). To our best knowledge, there is no previous work making such a comprehensive review on this issue. In this regard, we believe this survey can present radiologists and physicians with the application status of advanced artificial intelligent techniques in molecular images analysis and hence inspire more applications in clinical practice. Biomedical engineering researchers may also benefit from this survey by acquiring the state of the art in this field or inspiration for better models/methods in future research.', 'kwd': '-', 'title': u'Application of Deep Learning in Automated Analysis of Molecular Images in Cancer: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431941/', 'p': u'Precision medicine approaches rely on obtaining precise knowledge of the true state of health of an individual patient, which results from a combination of their genetic risks and environmental exposures. This approach is currently limited by the lack of effective and efficient non-invasive medical tests to define the full range of phenotypic variation associated with individual health. Such knowledge is critical for improved early intervention, for better treatment decisions, and for ameliorating the steadily worsening epidemic of chronic disease. We present proof-of-concept experiments to demonstrate how routinely acquired cross-sectional CT imaging may be used to predict patient longevity as a proxy for overall individual health and disease status using computer image analysis techniques. Despite the limitations of a modest dataset and the use of off-the-shelf machine learning methods, our results are comparable to previous \u2018manual\u2019 clinical methods for longevity prediction. This work demonstrates that radiomics techniques can be used to extract biomarkers relevant to one of the most widely used outcomes in epidemiological and clinical research \u2013 mortality, and that deep learning with convolutional neural networks can be usefully applied to radiomics research. Computer image analysis applied to routinely collected medical images offers substantial potential to enhance precision medicine initiatives.We created a predictive model using multivariable survival analysis (Cox regression). The model was informed by the top 5 covariates selected by minimum redundancy-maximum relevance feature selection50. These covariates were standardised, and the resulting risk score was dichotomised at the mean to create high-risk and low-risk phenotypes. Table\xa02 shows the 5-year mortality rate for high and low risk phenotypes, and the related Kaplan-Meier curves are presented in Fig.\xa02. The difference between the survival curves for of the high and low risk phenotypes is highly significant (p\u2009<\u20090.00005). The distribution of the raw mortality phenotype scores among cases and controls are presented in a box and whisker plot in Supplemental Figure\xa02.\n\n', 'kwd': '-', 'title': u'Precision Radiology: Predicting longevity using feature engineering and deep learning methods in a radiomics framework'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4364461/', 'p': '-', 'kwd': '-', 'title': u'Abstracts for the 15th International Congress on Schizophrenia Research (ICOSR)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5001212/', 'p': '-', 'kwd': '-', 'title': u'25th Annual Computational Neuroscience Meeting: CNS-2016'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5489441/', 'p': u'In recent years, research in artificial neural networks has resurged, now under the deep-learning umbrella, and grown extremely popular. Recently reported success of DL techniques in crowd-sourced QSAR and predictive toxicology competitions has showcased these methods as powerful tools in drug-discovery and toxicology research. The aim of this work was dual, first large number of hyper-parameter configurations were explored to investigate how they affect the performance of DNNs and could act as starting points when tuning DNNs and second their performance was compared to popular methods widely employed in the field of cheminformatics namely Na\xefve Bayes, k-nearest neighbor, random forest and support vector machines. Moreover, robustness of machine learning methods to different levels of artificially introduced noise was assessed. The open-source Caffe deep-learning framework and modern NVidia GPU units were utilized to carry out this study, allowing large number of DNN configurations to be explored.We show that feed-forward deep neural networks are capable of achieving strong classification performance and outperform shallow methods across diverse activity classes when optimized. Hyper-parameters that were found to play critical role are the activation function, dropout regularization, number hidden layers and number of neurons. When compared to the rest methods, tuned DNNs were found to statistically outperform, with p value <0.01 based on Wilcoxon statistical test. DNN achieved on average MCC units of 0.149 higher than NB, 0.092 than kNN, 0.052 than SVM with linear kernel, 0.021 than RF and finally 0.009 higher than SVM with radial basis function kernel. When exploring robustness to noise, non-linear methods were found to perform well when dealing with low levels of noise, lower than or equal to 20%, however when dealing with higher levels of noise, higher than 30%, the Na\xefve Bayes method was found to perform well and even outperform at the highest level of noise 50% more sophisticated methods across several datasets.', 'kwd': u'Deep learning, SARs, Cheminformatics, Machine-learning, Data-mining, Random forest, kNN, Support vector machines, Na\xefve Bayes', 'title': u'Deep-learning: investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4429500/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 37th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4567804/', 'p': u'Progress Tests (PTs) draw on a common question bank to assess all students in a programme against graduate outcomes. Theoretically PTs drive deep approaches to learning and reduce assessment-related stress. In 2013, PTs were introduced to two year groups of medical students (Years 2 and 4), whereas students in Years 3 and 5 were taking traditional high-stakes assessments. Staged introduction of PTs into our medical curriculum provided a time-limited opportunity for a comparative study. The main purpose of the current study was to compare the impact of PTs on undergraduate medical students\u2019 approaches to learning and perceived stress with that of traditional high-stakes assessments. We also aimed to investigate the associations between approaches to learning, stress and PT scores.Undergraduate medical students (N\u2009=\u2009333 and N\u2009=\u2009298 at Time 1 and Time 2 respectively) answered the Revised Study Process Questionnaire (R-SPQ-2F) and the Perceived Stress Scale (PSS) at two time points to evaluate change over time. The R-SPQ-2F generated a surface approach and a deep approach score; the PSS generated an overall perceived stress score.', 'kwd': '-', 'title': u'Progress testing in the medical curriculum: students\u2019 approaches to learning and perceived stress'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5087483/', 'p': u'Big sensor data provide significant potential for chemical fault diagnosis, which involves the baseline values of security, stability and reliability in chemical processes. A deep neural network (DNN) with novel active learning for inducing chemical fault diagnosis is presented in this study. It is a method using large amount of chemical sensor data, which is a combination of deep learning and active learning criterion to target the difficulty of consecutive fault diagnosis. DNN with deep architectures, instead of shallow ones, could be developed through deep learning to learn a suitable feature representation from raw sensor data in an unsupervised manner using stacked denoising auto-encoder (SDAE) and work through a layer-by-layer successive learning process. The features are added to the top Softmax regression layer to construct the discriminative fault characteristics for diagnosis in a supervised manner. Considering the expensive and time consuming labeling of sensor data in chemical applications, in contrast to the available methods, we employ a novel active learning criterion for the particularity of chemical processes, which is a combination of Best vs. Second Best criterion (BvSB) and a Lowest False Positive criterion (LFP), for further fine-tuning of diagnosis model in an active manner rather than passive manner. That is, we allow models to rank the most informative sensor data to be labeled for updating the DNN parameters during the interaction phase. The effectiveness of the proposed method is validated in two well-known industrial datasets. Results indicate that the proposed method can obtain superior diagnosis accuracy and provide significant performance improvement in accuracy and false positive rate with less labeled chemical sensor data by further active learning compared with existing methods.The experimental data used here are from the UCI machine learning repository, and are provided by a real sensor signal. We employed the proposed method to validate the superiority of active DNN framework in model performance. In the dataset, features are extracted from electric current drive signals by empirical mode decomposition (EMD). The drive has intact and defective components. This results in 11 different classes with different conditions. Each condition measured several times using 12 different operating conditions, that is, by different speeds, load moments and load forces. The current signals are measured with a current probe and an oscilloscope on two phases. Six classes of the current conditions are used in this study to test the performance of the proposed method. Type A corresponds to the normal condition whereas B-F corresponds to the different fault types caused by defective components. A total of 5319 samples for each health condition were used. Twenty thousand samples were selected as unlabeled data and used in pre-training, whereas 500 samples were selected as initial labeled data for fine-tuning. The other samples were selected as test data. ', 'kwd': u'fault diagnosis, deep learning, deep neural network, active learning, big sensor data', 'title': u'Fault Diagnosis Based on Chemical Sensor Data with an Active Deep Neural Network'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405523/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 38th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5380996/', 'p': u'Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.The dataset consisted of 74 whole-slide images of breast tumour resection samples which either retrieved from the AstraZeneca BioBank or acquired from a commercial provider (Dako Denmark A/S). Slides were obtained by cutting formalin-fixed, paraffin embedded human breast cancer samples into 4\u2009\u03bcm-thick sections, stained by IHC for HER2 demonstration (monoclonal Rabbit Anti-Human HER2 antibody, Dako Denmark A/S) and counterstained with haematoxylin using a Dako Autostainer Link48 (Dako Denmark A/S). Slides were digitized with an Aperio ScanScope whole-slide imaging microscope (Aperio, Leica Biosystems Imaging, Inc.) at a resolution of 0.49\u2009\u03bcm/pixel. The slides were reviewed to confirm the presence of invasive carcinoma and a total of 71 invasive carcinoma cases were selected for the study.', 'kwd': '-', 'title': u'Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333206/', 'p': u'Exponential surge in health care data, such as longitudinal data from electronic health records (EHR), sensor data from intensive care unit (ICU), etc., is providing new opportunities to discover meaningful data-driven characteristics and patterns ofdiseases. Recently, deep learning models have been employedfor many computational phenotyping and healthcare prediction tasks to achieve state-of-the-art performance. However, deep models lack interpretability which is crucial for wide adoption in medical research and clinical decision-making. In this paper, we introduce a simple yet powerful knowledge-distillation approach called interpretable mimic learning, which uses gradient boosting trees to learn interpretable models and at the same time achieves strong prediction performance as deep learning models. Experiment results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed method not only outperforms state-of-the-art approaches for morality and ventilator free days prediction tasks but can also provide interpretable models to clinicians.EHR data from ICU contains both static variables such as general descriptors (demographic information collected during admission) and temporal variables, which possibly come from different modalities, such as injury markers, ventilator settings, blood gas values, etc. We use X to represent all the input variables, and a binary label y\u2208 {0,1} to represent the prediction task outcome such as ICU mortality or Ventilator free days (VFD). We also use xt to denote the temporal variables observed at time t. Our goal is to learn an effective and interpretable function F() which can be used to predict the value of y given the input X.', 'kwd': '-', 'title': u'Interpretable Deep Models for ICU Outcome Prediction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3654146/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 36th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3500667/', 'p': u'We previously developed a model of the pre-assessment learning effects of consequential assessment and started to validate it. The model comprises assessment factors, mechanism factors and learning effects. The purpose of this study was to continue the validation process. For stringency, we focused on a subset of assessment factor\u2013learning effect associations that featured least commonly in a baseline qualitative study. Our aims were to determine whether these uncommon associations were operational in a broader but similar population to that in which the model was initially derived.A cross-sectional survey of 361 senior medical students at one medical school was undertaken using a purpose-made questionnaire based on a grounded theory and comprising pairs of written situational tests. In each pair, the manifestation of an assessment factor was varied. The frequencies at which learning effects were selected were compared for each item pair, using an adjusted alpha to assign significance. The frequencies at which mechanism factors were selected were calculated.', 'kwd': '-', 'title': u'Modelling the pre-assessment learning effects of assessment: evidence in the validity chain'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4212306/', 'p': '-', 'kwd': '-', 'title': u'UEG Week 2014 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5381785/', 'p': u'\nEvolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.', 'kwd': '-', 'title': u'Multiagent cooperation and competition with deep reinforcement learning'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4749830/', 'p': u'Unexpected outcomes can reflect noise in the environment or a change in the current rules. We should ignore noise but shift strategy after rule changes. How we learn to do this is unclear, but one possibility is that it relies on learning to learn in uncertain environments. We propose that acquisition of latent task structure during learning to learn, even when not necessary, is crucial. We report results consistent with this hypothesis. Macaque monkeys acquired adaptive responses to feedback while learning to learn serial stimulus-response associations with probabilistic feedback. Monkeys learned well, decreasing their errors to criterion, but they also developed an apparently nonadaptive reactivity to unexpected stochastic feedback, even though that unexpected feedback never predicted problem switch. This surprising learning trajectory permitted the same monkeys, na\xefve to relearning about previously learned stimuli, to transfer to a task of stimulus-response remapping at immediately asymptotic levels. Our results suggest that learning new problems in a stochastic environment promotes the acquisition of performance rules from latent task structure, providing behavioral flexibility. Learning to learn in a probabilistic and volatile environment thus appears to induce latent learning that may be beneficial to flexible cognition.The structure of a single trial and a single problem was always the same, regardless of the form of the task. Monkeys initiated each trial by touching and holding a lever item, represented by a white square at the bottom of the screen (Fig. 1A). A fixation point (FP) appeared. After a delay period, a stimulus was displayed at the top of the screen (Stim ON signal), and was followed after a delay by the appearance in the middle of the screen of three targets (Targets ON signal). Stimuli consisted of square bitmap images of either an abstract picture or a photograph, of size 65 \xd7 65mm. Targets were three empty gray squares, of the same size as the stimulus. After a further delay all targets turned white, providing the GO signal following which monkeys were permitted to make their choice by touching a target. Monkeys maintained touch on the chosen target for a fixed amount of time in order to receive visual feedback on that choice. Feedback consisted of horizontal (positive) or vertical (negative) bars within each of the three targets. A positive feedback was followed by the delivery of \u223c1.8 mL of 50% apple juice. After the completion of a trial, a new stimulus was picked within the set of two stimuli and monkeys were allowed to begin a new trial. Timing for each event gradually increased across learning to progressively train monkeys to hold their hand on the screen without moving after each action.', 'kwd': '-', 'title': u'Learning to learn about uncertain feedback'}], 'Coronary Artery Disease AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123729/', 'p': u'Despite advances in the pharmacologic and interventional treatment of coronary artery disease (CAD), atherosclerosis remains the leading cause of death in Western societies. X-ray coronary angiography has been the modality of choice for diagnosing the presence and extent of CAD. However, this technique is invasive and provides limited information on the composition of atherosclerotic plaque. Coronary computed tomography angiography (CCTA) and cardiac magnetic resonance (CMR) have emerged as promising non-invasive techniques for the clinical imaging of CAD. Hereby, CCTA allows for visualization of coronary calcification, lumen narrowing and atherosclerotic plaque composition. In this regard, data from the CONFIRM Registry recently demonstrated that both atherosclerotic plaque burden and lumen narrowing exhibit incremental value for the prediction of future cardiac events. However, due to technical limitations with CCTA, resulting in false positive or negative results in the presence of severe calcification or motion artifacts, this technique cannot entirely replace invasive angiography at the present time. CMR on the other hand, provides accurate assessment of the myocardial function due to its high spatial and temporal resolution and intrinsic blood-to-tissue contrast. Hereby, regional wall motion and perfusion abnormalities, during dobutamine or vasodilator stress, precede the development of ST-segment depression and anginal symptoms enabling the detection of functionally significant CAD. While CT generally offers better spatial resolution, the versatility of CMR can provide information on myocardial function, perfusion, and viability, all without ionizing radiation for the patients. Technical developments with these 2 non-invasive imaging tools and their current implementation in the clinical imaging of CAD will be presented and discussed herein.The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.', 'kwd': u'coronary artery disease, atherosclerotic plaque, coronary computed tomography, cardiac magnetic resonance, risk stratification', 'title': u'Cardiac magnetic resonance and computed tomography angiography for clinical imaging of stable coronary artery disease. Diagnostic classification and risk stratification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4922813/', 'p': u'After a decade of clinical use of coronary computed tomographic angiography (CCTA) to evaluate the anatomic severity of coronary artery disease, new methods of deriving functional information from CCTA have been developed. These methods utilize the anatomic information provided by CCTA in conjunction with computational fluid dynamics to calculate fractional flow reserve (FFR) values from CCTA image data sets. Computed tomography-derived FFR (CT-FFR) enables the identification of lesion-specific drop noninvasively. A three-dimensional CT-FFR modeling technique, which provides FFR values throughout the coronary tree (HeartFlow FFRCT analysis), has been validated against measured FFR and is now approved by the US Food and Drug Administration for clinical use. This technique requires off-site supercomputer analysis. More recently, a one-dimensional computational analysis technique (Siemens cFFR), which can be performed on on-site workstations, has been developed and is currently under investigation. This article reviews CT-FFR technology and clinical evidence for its use in stable patients with suspected coronary artery disease.Intermediate degrees of stenosis (30%\u201370%) present the greatest challenge in the diagnosis of CAD. Since hemodynamically significant lesions are occasionally observed in intermediate lesions with <70% stenosis,13 the use of invasive FFR is recommended to evaluate the function of intermediate coronary lesions as a class IIa indication.6 However, given the relatively lower prevalence of lesion-specific pressure drop caused by intermediate stenosis compared to that of severe stenosis in the FAME study,13 CT-derived FFR would be of great use for assessing the functional significance of intermediate lesions to avoid unnecessary ICA and help in treatment decision making. Table 2 provides a summary of the studies of FFRCT and cFFR. Similar to the overall diagnostic accuracy of CT-derived FFR, all studies demonstrated high diagnostic performance for intermediate stenosis, with the highest accuracy and specificity for FFRCT.24,31,38,39', 'kwd': u'fractional flow reserve, coronary computed tomographic angiography, FFRCT, cFFR', 'title': u'Noninvasive FFR derived from coronary CT angiography in the management of coronary artery disease: technology and clinical update'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5329750/', 'p': u'Coronary artery disease (CAD) is a leading cause of death and disability worldwide. Cardiovascular magnetic resonance (CMR) is established in clinical practice guidelines with a growing evidence base supporting its use to aid the diagnosis and management of patients with suspected or established CAD. CMR is a multi-parametric imaging modality that yields high spatial resolution images that can be acquired in any plane for the assessment of global and regional cardiac function, myocardial perfusion and viability, tissue characterisation and coronary artery anatomy, all within a single study protocol and without exposure to ionising radiation. Advances in technology and acquisition techniques continue to progress the utility of CMR across a wide spectrum of cardiovascular disease, and the publication of large scale clinical trials continues to strengthen the role of CMR in daily cardiology practice. This article aims to review current practice and explore the future directions of multi-parametric CMR imaging in the investigation of stable CAD.Although 1.5T is remains the standard field strength used in clinical CMR, imaging at a higher field strength of 3.0T offers increased signal to noise and contrast to noise ratios thereby giving improved spatial and temporal enhancement[27]. Consequently the diagnostic accuracy of perfusion imaging at 3.0T may be improved, and in a small direct comparison of CMR perfusion at 1.5T, 3.0T (n = 61) showed greater diagnostic accuracy in both single vessel (AUC: 0.89 vs 0.70; P < 0.05) and multi-vessel disease (AUC: 0.95 vs 0.82, P < 0.05)[28]. Furthermore, 3.0T has been compared to 1.5T using FFR as reference standard, corroborating it\u2019s superior diagnostic accuracy[29,30]. The higher 3.0T field strength does however pose challenges with greater field inhomogeneity, susceptibility artefacts and higher local energy deposition. Also, many implants deemed \u201cMR compatible\u201d at 1.5T cannot be scanned at 3.0T[31]. These issues are however being overcome with improved technology and the use of multi-transmit radiofrequency CMR techniques improving field homogeneity[32].', 'kwd': u'Cardiovascular magnetic resonance, Coronary heart disease, Myocardial perfusion, Viability, Prognosis', 'title': u'Assessment of stable coronary artery disease by cardiovascular magnetic resonance imaging: Current and emerging techniques'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4788549/', 'p': u'Total atherosclerotic plaque burden assessment by CT angiography (CTA) is a promising tool for diagnosis and prognosis of coronary artery disease (CAD) but its validation is restricted to small clinical studies. We tested the feasibility of semi-automatically derived coronary atheroma burden assessment for identifying patients with hemodynamically significant CAD in a large cohort of patients with heterogenous characteristics.This study focused on the CTA component of the CORE320 study population. A semi-automated contour detection algorithm quantified total coronary atheroma volume defined as the difference between vessel and lumen volume. Percent atheroma volume (PAV = [total atheroma volume/total vessel volume]\xd7100) was the primary metric for assessment (n=374). The area under the receiver operating characteristic curve (AUC) determined the diagnostic accuracy for identifying patients with hemodynamically significant CAD defined as \u226550% stenosis by quantitative coronary angiography and associated myocardial perfusion abnormality by SPECT.', 'kwd': '-', 'title': u'Total Coronary Atherosclerotic Plaque Burden Assessment by CT Angiography for Detecting Obstructive Coronary Artery Disease Associated with Myocardial Perfusion Abnormalities'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2947500/', 'p': u'Conceived and designed the experiments: WJM JW JMAvE RMB TL. Performed the experiments: SCG. Analyzed the data: SCG MEK AGK SS TL. Contributed reagents/materials/analysis tools: RJvdG. Wrote the paper: SCG MEK AGK SS MK RJvdG WJM JW JMAvE RMB TL. Data interpretation and supervision: MEK JvE TL. Statistical analysis: AK. Assistance with data acquisition in early phase of study: MK. Developed MR method: RB. Handled funding: TL.Magnetic resonance imaging (MRI) is sensitive to early atherosclerotic changes such as positive remodeling in patients with coronary artery disease (CAD). We assessed prevalence, quality, and extent of coronary atherosclerosis in a group of healthy subjects compared to patients with confirmed CAD.', 'kwd': '-', 'title': u'Visualization of Coronary Wall Atherosclerosis in Asymptomatic Subjects and Patients with Coronary Artery Disease Using Magnetic Resonance Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479873/', 'p': u"Coronary CT angiography has been increasingly used in the diagnosis of coronary artery disease owing to rapid technological developments, which are reflected in the improved spatial and temporal resolution of the images. High diagnostic accuracy has been achieved with multislice CT scanners (64 slice and higher), and in selected patients coronary CT angiography is regarded as a reliable alternative to invasive coronary angiography. With high-quality coronary CT imaging increasingly being performed, patients can benefit from an imaging modality that provides a rapid and accurate diagnosis while avoiding an invasive procedure. Despite the tremendous contributions of coronary CT angiography to cardiac imaging, study results reported in the literature should be interpreted with caution as there are some limitations existing within the study design or related to patient risk factors. In addition, some attention must be given to the potential health risks associated with the ionising radiation received during cardiac CT examinations. Radiation dose associated with coronary CT angiography has raised serious concerns in the literature, as the risk of developing malignancy is not negligible. Various dose-saving strategies have been implemented, with some of the strategies resulting in significant dose reduction. The aim of this review is to present an overview of the role of coronary CT angiography on cardiac imaging, with focus on coronary artery disease in terms of the diagnostic and prognostic value of coronary CT angiography. Various approaches for dose reduction commonly recommended in the literature are discussed. Limitations of coronary CT angiography are identified. Finally, future directions and challenges with the use of coronary CT angiography are highlighted.There is no doubt that, with increasing technological improvements, coronary CT angiography will continue to play an important role in the detection and diagnosis of CAD. Justification is a shared responsibility between requesting physicians and radiologists. For cardiac imaging exposures, the primary tasks of the medical imaging specialists are to collaborate with referring cardiologists to direct patients to the most appropriate imaging modality for the required diagnostic task and to ensure that all technical aspects of the examination are optimised so that the acquired image quality is diagnostic while keeping the doses as low as possible [97]. This is particularly important for young individuals, especially women, for whom alternative diagnostic modalities that do not involve the use of ionising radiation should be considered, such as stress electrocardiography, echocardiography or MRI. The benefit-to-risk ratio for imaging patients suspected of CAD must be driven by the benefit and appropriateness of the CCTA examination requested by the physicians. The American College of Radiology's appropriateness criteria provide evidence-based guidelines to help physicians in recommending an appropriate imaging test [98]. Similarly, the European Commission's guidelines and UK's Royal College of Radiologists' referral guidelines for imaging also provide a detailed overview of clinical indications for imaging examinations including CT [99]. Physicians need to follow guidelines like national diagnostic reference levels for reducing radiation dosages.", 'kwd': '-', 'title': u'Coronary CT angiography: current status and continuing challenges'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4613789/', 'p': u'Diagnosis and management of coronary artery disease represent a major challenge to our health care systems affecting millions of patients each year. Until recently, the diagnosis of coronary artery disease could be conclusively determined only by invasive coronary angiography. To avoid risks from cardiac catheterization, many healthcare systems relied on stress testing as gatekeeper for coronary angiography. Advancements in cardiac computed tomography angiography technology now allows to noninvasively visualize coronary artery disease, challenging the role of stress testing as the default noninvasive imaging tool for evaluating patients with chest pain. In this review, we summarize current data on the clinical utility of cardiac computed tomography and stress testing in stable patients with suspected coronary artery disease.Cardiovascular diseases, and particular, coronary artery disease (CAD), remain the leading cause of death worldwide with an enormous burden on health care systems [1]. Annually, more than 10 million stress tests and approximately one million diagnostic cardiac catheterizations are being performed in the U.S. alone [1]. Total costs of cardiovascular disease and stroke in the U.S. for 2015 are estimated to exceed 320 billion dollars [1]. Management of CAD requires an accurate diagnosis. For many decades, invasive coronary angiography (ICA) has served as the gold standard for the diagnosis of CAD despite many well recognized limitations of this seasoned technology [2;3]. To avoid risks from cardiac catheterization in low-intermediate risk patients, we have been using myocardial stress testing as gatekeeper for invasive angiography. The emergence of multi-detector computed tomography technology has allowed to noninvasively assess the presence, location, severity, and characteristics of coronary atherosclerotic disease in patients. In recent years, an abundance of clinical studies revealed data on the diagnostic and prognostic performance of cardiac computed tomography angiography (CCTA), challenging the role of stress testing as the default noninvasive test for patients presenting with non-acute chest pain. In this paper, we review current data on the clinical utility of CCTA vs. stress testing in stable patients with suspected CAD.', 'kwd': u'Coronary heart disease, cardiac computed tomography angiography, stress imaging, myocardial perfusion imaging, single-photon-emission tomography', 'title': u'Cardiac CT vs. Stress Testing in Patients with Suspected\nCoronary Artery Disease: Review and Expert Recommendations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3665274/', 'p': u'Coronary endothelial function (endoFx) is abnormal in patients with established coronary artery disease (CAD) and was recently shown by MRI to relate to the severity of luminal stenosis. Recent advances in MRI now allow the non-invasive assessment of both anatomic and functional (endoFx) changes that previously required invasive studies. We tested the hypothesis that abnormal coronary endoFx is related to measures of early atherosclerosis such as increased coronary wall thickness (CWT).Seventeen arteries in fourteen healthy adults and seventeen arteries in fourteen patients with non-obstructive CAD were studied. To measure endoFx, coronary MRI was performed before and during isometric handgrip exercise, an endothelial-dependent stressor and changes in coronary cross-sectional area (CSA) and flow were measured. Black blood imaging was performed to quantify CWT and other indices of arterial remodeling. The mean stress-induced change in CSA was significantly higher in healthy adults (13.5%\xb112.8%, mean\xb1SD, n=17) than in those with mildly diseased arteries (-2.2\xb16.8%, p<0.0001, n=17). Mean CWT was lower in healthy subjects (0.9\xb10.2mm) than in CAD patients (1.4\xb10.3mm, p<0.0001). In contrast to healthy subjects, stress-induced changes in CSA, a measure of coronary endoFx, correlated inversely with CWT in CAD patients (r= -0.73, p=0.0008).', 'kwd': u'coronary disease, endothelium, magnetic resonance imaging', 'title': u'Regional Coronary Endothelial Function is Closely Related to Local Early Coronary Atherosclerosis in Patients with Mild Coronary Artery Disease: A Pilot Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3729736/', 'p': u'The purpose of this study is to (1) investigate the image quality of phase-sensitive dual inversion recovery (PS-DIR) coronary wall imaging in healthy subjects and in subjects with known coronary artery disease (CAD) and to (2) investigate the utilization of PS-DIR at 3T in the assessment of coronary artery thickening in subjects with asymptomatic but variable degrees of CAD.A total of 37 subjects participated in this Institutional Review Board approved and HIPAA-compliant study. These included 21 subjects with known CAD as identified on Multi-Detector CT angiography (MDCT). Sixteen healthy subjects without known history of CAD were included. All subjects were scanned using free-breathing PS-DIR MRI for the assessment of coronary wall thickness at 3T. Lumen-tissue contrast-to-noise ratio (CNR), signal-to-noise (SNR), and quantitative vessel parameters including lumen area and wall thickness were measured. Statistical analyses were performed.', 'kwd': u'Atherosclerosis, coronary artery imaging, vessel wall, black blood MRI, Phase sensitive, dual inversion recovery, 3T', 'title': u'Feasibility of Coronary Artery Wall Thickening Assessment in Asymptomatic Coronary Artery Disease using Phase-Sensitive Dual Inversion Recovery MRI at 3T'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3598430/', 'p': u'To date, the therapeutic benefit of revascularization vs. medical therapy for stable individuals undergoing invasive coronary angiography (ICA) based upon coronary computed tomographic angiography (CCTA) findings has not been examined.We examined 15 223 patients without known coronary artery disease (CAD) undergoing CCTA from eight sites and six countries who were followed for median 2.1 years (interquartile range 1.4\u20133.3 years) for an endpoint of all-cause mortality. Obstructive CAD by CCTA was defined as a \u226550% luminal diameter stenosis in a major coronary artery. Patients were categorized as having high-risk CAD vs. non-high-risk CAD, with the former including patients with at least obstructive two-vessel CAD with proximal left anterior descending artery involvement, three-vessel CAD, and left main CAD. Death occurred in 185 (1.2%) patients. Patients were categorized into two treatment groups: revascularization (n = 1103; 2.2% mortality) and medical therapy (n = 14 120, 1.1% mortality). To account for non-randomized referral to revascularization, we created a propensity score developed by logistic regression to identify variables that influenced the decision to refer to revascularization. Within this model (C index 0.92, \u03c72 = 1248, P < 0.0001), obstructive CAD was the most influential factor for referral, followed by an interaction of obstructive CAD with pre-test likelihood of CAD (P = 0.0344). Within CCTA CAD groups, rates of revascularization increased from 3.8% for non-high-risk CAD to 51.2% high-risk CAD. In multivariable models, when compared with medical therapy, revascularization was associated with a survival advantage for patients with high-risk CAD [hazards ratio (HR) 0.38, 95% confidence interval 0.18\u20130.83], with no difference in survival for patients with non-high-risk CAD (HR 3.24, 95% CI 0.76\u201313.89) (P-value for interaction = 0.03).', 'kwd': u'Computed tomography, Coronary revascularization, Medical therapy, Coronary artery disease', 'title': u'All-cause mortality benefit of coronary revascularization vs. medical therapy in patients without known coronary artery disease undergoing coronary computed tomographic angiography: results from CONFIRM (COronary CT Angiography EvaluatioN For Clinical Outcomes: An InteRnational Multicenter Registry)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5594598/', 'p': u'The use of coronary MR angiography (CMRA) in patients with coronary artery disease (CAD) remains limited due to the long scan times, unpredictable and often non-diagnostic image quality secondary to respiratory motion artifacts. The purpose of this study was to evaluate CMRA with image-based respiratory navigation (iNAV CMRA) and compare it to gold standard invasive x-ray coronary angiography in patients with CAD.Consecutive patients referred for CMR assessment were included to undergo iNAV CMRA on a 1.5\xa0T scanner. Coronary vessel sharpness and a visual score were assigned to the coronary arteries. A diagnostic reading was performed on the iNAV CMRA data, where a lumen narrowing >50% was considered diseased. This was compared to invasive x-ray findings.', 'kwd': u'Coronary MR angiography, Image navigators, Respiratory motion correction, Coronary artery disease', 'title': u'Diagnostic performance of image navigated coronary CMR angiography in patients with coronary artery disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5102483/', 'p': '-', 'kwd': u'Coronary Artery Disease / diagnoses, Scintigraphy, Calcium Signaling, Tomography, Emission Computed', 'title': u'Relationship between Calcium Score and Myocardial Scintigraphy in the\nDiagnosis of Coronary Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3377563/', 'p': u'The objective of the analysis is to determine the diagnostic accuracy of stress echocardiography (ECHO) in the diagnosis of patients with suspected coronary artery disease (CAD) compared to coronary angiography (CA).The objective of the analysis is to determine the diagnostic accuracy of stress echocardiography (stress ECHO) in the diagnosis of patients with suspected coronary artery disease (CAD).', 'kwd': '-', 'title': u'Stress Echocardiography for the Diagnosis of Coronary Artery Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422837/', 'p': u'During the last decade coronary computed tomography angiography (CTA) has become the preeminent non-invasive imaging modality to detect coronary artery disease (CAD) with high accuracy. However, CTA has a limited value in assessing the hemodynamic significance of a given stenosis due to a modest specificity and positive predictive value. In recent years, different CT techniques for detecting myocardial ischemia have emerged, such as CT-derived fractional flow reserve (FFR-CT), transluminal attenuation gradient (TAG), and myocardial CT perfusion (CTP) imaging. Myocardial CTP imaging can be performed with a single static scan during first pass of the contrast agent, with monoenergetic or dual-energy acquisition, or as a dynamic, time-resolved scan during stress by using coronary vasodilator agents (adenosine, dipyridamole, or regadenoson). A number of CTP techniques are available, which can assess myocardial perfusion in both a qualitative, semi-quantitative or quantitative manner. Once used primarily as research tools, these modalities are increasingly being used in routine clinical practice. All these techniques offer the substantial advantage of combining anatomical and functional evaluation of flow-limiting coronary stenosis in the same examination that would be beneficial for clinical decision-making. This review focuses on the state-of the-art and future trends of these evolving imaging modalities in the field of cardiology for the physiologic assessments of CAD.The FFR-CT method allows the extraction of \u2018stress induced\u2019 quantitative functional information from an anatomic CTA of at least moderate quality acquired at rest without adenosine infusion. The method uses computational fluid dynamics with simulated hyperemia to calculate the FFR measurement at any point in the vascular tree (39). The concept of coronary FFR, defined as the ratio of the mean coronary pressure distal to a coronary stenosis to the mean aortic pressure during maximal coronary blood flow, has evolved into an accepted functional measure of stenosis severity since first proposed 15 years ago (22). FFR has now become the invasive gold standard for assessing lesion-specific ischemia. A FFR value less than 0.80 or less than 0.75 identifies hemodynamic significance of coronary stenosis (7,8). FFR-CT correlates well with invasive-derived FFR measurements in patients with suspected or known CAD (17,39,40). One of the temporary drawbacks is that FFR-CT needs extreme computational ability and analysis time, which hampers widespread dissemination.', 'kwd': u'Cardiac computed tomography, coronary artery disease (CAD), stress imaging, myocardial perfusion imaging, myocardial blood flow quantification', 'title': u'Myocardial blood flow quantification for evaluation of coronary artery disease by computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4499869/', 'p': u'The initial course of atherosclerotic disease is thought to begin in early adulthood. In young adults lesions in the arterial vessel wall have been observed surprisingly frequently13 but the prognostic relevance of early, adaptive or reversible changes like \u201cfatty streak\u201d or intimal thickening remains a matter of debate. Pathology studies seek to integrate autopsy findings from various stages of atherosclerosis to provide a putative sequence of events4. In brief, intimal thickening is observed early in the disease process. The early atherosclerotic lesion is composed of smooth muscle cells and is affected by increased macrophage and lipid influx. If this process continues, a necrotic core is formed and the lesion progresses to a fibrous cap atheroma. The necrotic core contains lipids and apoptotic macrophages. A stable fibrous cap may prevent rupture of the lesion. If the fibrous cap loses matrix proteins and smooth muscle cells, a thin cap atheroma can result. Intraplaque hemorrhage is also seen frequently in this entity, leading to further enlargement of the lipid core. The risk of plaque rupture is increased as the fibrous cap thins and the lipid core enlarges14. The \u201cfibrocalcific plaque\u201d is considered to be a feature of more stable plaque, although the processes involved in calcification are not fully understood.It is generally conceived that therapeutic intervention for atherosclerosisis most effective when started at an early stage of the progressive disease process 15. Imaging tools have provided a substantial database of knowledge regarding disease burden. Imaging of the larger surface vessels (carotid or femoral arteries) has been extensively used to detect early systemic vascular pathology 16. Calcium detection using non-contrast CT provides a direct approach to assessing coronary atherosclerosis burden. The coronary artery calcium score has strong predictive power for cardiovascular events in asymptomatic subjects17. However, calcium deposition is felt to be a late event in the formation of atherosclerotic plaque. The relevance of non-calcified plaque is emphasized by prospective IVUS studies that show coronary fibroatheroma without significant calcification confers an elevated risk for myocardial infarction 18. Non-calcified plaque is more common than calcified plaque in asymptomatic individuals younger than 45 years. The ability to noninvasively image non-calcified plaque or wall thickening of the coronary arteries using MRI or CT enables the detection of earlier stages of atherosclerotic disease 19, 20.', 'kwd': u'imaging, coronary disease, plaque, atherosclerosis', 'title': u'Noninvasive Imaging of Atherosclerotic Plaque Progression: Status of Coronary CT Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3314981/', 'p': u'A sexual dimorphism exists in the incidence and prevalence of coronary artery disease\u2014men are more commonly affected than are age-matched women. We explored the role of the Y chromosome in coronary artery disease in the context of this sexual inequity.We genotyped 11 markers of the male-specific region of the Y chromosome in 3233 biologically unrelated British men from three cohorts: the British Heart Foundation Family Heart Study (BHF-FHS), West of Scotland Coronary Prevention Study (WOSCOPS), and Cardiogenics Study. On the basis of this information, each Y chromosome was tracked back into one of 13 ancient lineages defined as haplogroups. We then examined associations between common Y chromosome haplogroups and the risk of coronary artery disease in cross-sectional BHF-FHS and prospective WOSCOPS. Finally, we undertook functional analysis of Y chromosome effects on monocyte and macrophage transcriptome in British men from the Cardiogenics Study.', 'kwd': '-', 'title': u'Inheritance of coronary artery disease in men: an analysis of the role of the Y chromosome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566965/', 'p': u'More than a million diagnostic cardiac catheterizations are performed annually in the US for evaluation of coronary artery anatomy and the presence of atherosclerosis. Nearly half of these patients have no significant coronary lesions or do not require mechanical or surgical revascularization. Consequently, the ability to rule out clinically significant coronary artery disease (CAD) using low cost, low risk tests of serum biomarkers in even a small percentage of patients with normal coronary arteries could be highly beneficial.Serum from 359 symptomatic subjects referred for catheterization was interrogated for proteins involved in atherogenesis, atherosclerosis, and plaque vulnerability. Coronary angiography classified 150 patients without flow-limiting CAD who did not require percutaneous intervention (PCI) while 209 required coronary revascularization (stents, angioplasty, or coronary artery bypass graft surgery). Continuous variables were compared across the two patient groups for each analyte including calculation of false discovery rate (FDR \u2264 1%) and Q value (P value for statistical significance adjusted to \u2264 0.01).', 'kwd': u'atherosclerosis, biomarkers, cardiac catheterization, coronary angiography, coronary stenosis, multiplex proteomics', 'title': u'Serum protein profiles predict coronary artery disease in symptomatic patients referred for coronary angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4395291/', 'p': u'Conceived and designed the experiments: JHZ HLK KNJ. Performed the experiments: JBS YHC WYC SHK MAK. Analyzed the data: HLK KNJ. Contributed reagents/materials/analysis tools: JBS YHC WYC SHK MAK. Wrote the paper: HLK KNJ JHZ.The aim of this study was to investigate whether brachial-ankle pulse wave velocity (baPWV) is associated with the severity of coronary artery disease (CAD) assessed by coronary computed tomography angiography (CCTA), and to evaluate baPWV as a predictor of obstructive CAD on CCTA. A total of 470 patients who underwent both baPWV and CCTA were included. We evaluated stenosis degree and plaque characteristics on CCTA. To estimate the severity of CAD, we calculated the number of segment with plaque (segment involvement score; SIS), stenosis degree-weighted plaque score (segment stenosis score; SSS), and coronary artery calcium score (CACS). The mean baPWV was 1,485 \xb1 315 cm/s (range, 935-3,175 cm/s). Non-obstructive (stenosis < 50%) and obstructive (stenosis \u2265 50%) CAD was found in 129 patients (27.4%) and 144 (30.6%), respectively. baPWV in patients with obstructive CAD was higher than that of patients with non-obstructive (1,680 \xb1 396 cm/s versus 1,477 \xb1 244 cm/s, P < 0.001) or no CAD (1,680 \xb1 396 cm/s versus \xb1 196 1,389 cm/s, P < 0.001). baPWV showed significant correlation with SSS (r = 0.429, P < 0.001), SIS (r = 0.395, P < 0.001), CACS (r 0.346, P < 0.001), and the number of segment with non-calcified plaque (r 0.092, P = 0.047), mixed plaque (r = 0.267, P < 0.001), and calcified plaque (r = 0.348, P < 0.001), respectively. The optimal baPWV cut-off value for the detection of obstructive CAD was 1,547 cm/s. baPWV \u2265 1,547 cm/s was independent predictor for the obstructive CAD. In conclusion, baPWV is well correlated with the severity of CAD evaluated by CCTA. baPWV has the potential to predict severity of coronary artery atherosclerosis.', 'kwd': '-', 'title': u'The Association of Brachial-Ankle Pulse Wave Velocity with Coronary Artery Disease Evaluated by Coronary Computed Tomography Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5553869/', 'p': u'The major issue in coronary heart disease (CHD) diagnosis and management is that symptoms onset in an advanced state of disease. Despite the availability of several clinical risk scores, the prediction of cardiovascular events is lacking, and many patients at risk are not well stratified according to the canonical risk factors alone. Therefore, adequate risk assessment remains the most challenging issue. Recently, the integration of imaging data with biochemical markers in a radiogenomic framework has been proposed in many fields of medicine as well as in cardiology. Multimodal imaging and advanced processing techniques can provide both direct (e.g., remodeling index, calcium score, total plaque volume, plaque burden) and indirect (e.g., myocardial perfusion index, coronary flow reserve) imaging features of CHD. Furthermore, the identification of novel non-invasive biochemical markers, mainly focused on plasma and/or serum samples, has increased the specificity of findings, reflecting several pathophysiological pathways of atherosclerosis, the principal actor in CHD. In this context, a multifaced approach, derived from the strengths of all these modalities, appears promising for finer risk stratification and treatment strategies, facilitating the decision-making and clinical management of patients. This review underlines the role of different imaging modalities in the quantification of coronary atherosclerosis and describes novel blood-based markers that could improve diagnosis and have a better predictive value in CHD.The need to improve diagnosis and risk prediction has prompted the search for novel markers in cardiovascular medicine. Literature data suggest that CTCA could substantially reduce the number of invasive procedures, increasing the safety of patients, and allows a more precise planning of potential treatment options. Furthermore, strong evidence has also emerged on the usefulness of coronary calcium score assessed by CTCA. In association with imaging improvements, novel high-throughput platforms investigating proteomic, metabolomic, epigenomic, and transcriptomics profiles together with genome-wide association studies may generate \u201cmultimarker CHD scores\u201d with a higher predictive power than the use of a single biomarker. Surrogate biomarkers of coronary atherosclerosis and advanced imaging techniques could represent important cornerstones to characterize sub-clinical and clinical atherosclerosis with a consequent facilitation in the decision-making and clinical management of patients.', 'kwd': u'Atherosclerosis, coronary heart disease, imaging, biomarkers', 'title': u'An integrated approach to coronary heart disease diagnosis and clinical management'}], 'Tomography AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3775479/', 'p': u'The classic imaging geometry for computed tomography is for collection of un-truncated projections and reconstruction of a global image, with the Fourier transform as the theoretical foundation that is intrinsically non-local. Recently, interior tomography research has led to theoretically exact relationships between localities in the projection and image spaces and practically promising reconstruction algorithms. Initially, interior tomography was developed for x-ray computed tomography. Then, it has been elevated as a general imaging principle. Finally, a novel framework known as \u201comni-tomography\u201d is being developed for grand fusion of multiple imaging modalities, allowing tomographic synchrony of diversified features.Acquisition of less projection data is achieved with a narrower beam, and an object larger than the beam width is not a concern anymore. In other words, interior tomography can handle objects larger than a field of view. This flexibility can certainly enhance the utility of a CT scanner. In nano-CT studies, one reduces a sample into a narrow x-ray beam for complete projection profiles. In this tedious process, morphological and functional damages may be induced. Supported by an NSF/MRI grant and in collaboration with Xradia, we are developing a next generation nano-CT system capable of focusing on an ROI and reconstructing it accurately within a large object (Figure 7). In a geo-science project, fossils from the Ediacaran Doushantuo Formation (ca. 551\u2013635 million years old) were investigated, which are too valuable to be broken, and demand interior tomography. Another example is the large patient problem, i.e., a patient is larger than the field of view of a CT scanner, which can now be solved using theoretically exact interior tomography, instead of using conventional approximate methods such as extrapolation for data completion.', 'kwd': u'Biomedical imaging, computed tomography, local tomography, interior tomography, omni-tomography', 'title': u'Meaning of Interior Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5483328/', 'p': u'This paper uses X-ray computed tomography to track the mechanical response of a vertebrate (Barnacle goose) long bone subjected to an axial compressive load, which is increased gradually until failure. A loading rig was mounted in an X-ray computed tomography system so that a time-lapse sequence of three-dimensional (3D) images of the bone\u2019s internal (cancellous or trabecular) structure could be recorded during loading. Five distinct types of deformation mechanism were observed in the cancellous part of the bone. These were (i) cracking, (ii) thinning (iii) tearing of cell walls and struts, (iv) notch formation, (v) necking and (vi) buckling. The results highlight that bone experiences brittle (notch formation and cracking), ductile (thinning, tearing and necking) and elastic (buckling) modes of deformation. Progressive deformation, leading to cracking was studied in detail using digital image correlation. The resulting strain maps were consistent with mechanisms occurring at a finer-length scale. This paper is the first to capture time-lapse 3D images of a whole long bone subject to loading until failure. The results serve as a unique reference for researchers interested in how bone responds to loading. For those using computer modelling, the study not only provides qualitative information for verification and validation of their simulations but also highlights that constitutive models for bone need to take into account a number of different deformation mechanisms.A crack appears at scan 7 (D) in Fig. 7 which opens through scan 8 (E) and scan 9 (F). However, no obvious crack initiation mechanism is visible in the preceding scans. Cracking is examined more closely using DVC later in the paper.', 'kwd': u'X-ray computed tomography, Digital image correlation, Branta leucopsis, Axial loading, Progressive damage, Stress\u2013strain, Deformation mechanisms, Computer modeling, Constitutive, Modeling and simulation', 'title': u'A study of the progression of damage in an axially loaded Branta leucopsis femur using X-ray computed tomography and digital image correlation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684619/', 'p': u'Despite increasing demand, imaging the internal structure of plant organs or tissues without the use of transgenic lines expressing fluorescent proteins remains a challenge. Techniques such as magnetic resonance imaging, optical projection tomography or X-ray absorption tomography have been used with various success, depending on the size and physical properties of the biological material.X-ray in-line phase tomography was applied for the imaging of internal structures of maize seeds at early stages of development, when the cells are metabolically fully active and water is the main cell content. This 3D imaging technique with histology-like spatial resolution is demonstrated to reveal the anatomy of seed compartments with unequalled contrast by comparison with X-ray absorption tomography. An associated image processing pipeline allowed to quantitatively segment in 3D the four compartments of the seed (embryo, endosperm, nucellus and pericarp) from 7 to 21\xa0days after pollination.', 'kwd': u'X ray in-line phase tomography, Image segmentation, Virtual histology, plant development, maize seeds', 'title': u'Fast virtual histology using X-ray in-line phase tomography: application to the 3D anatomy of maize developing seeds'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669980/', 'p': u'The promise of compressive sensing, exploitation of compressibility to achieve high quality image reconstructions with less data, has attracted a great deal of attention in the medical imaging community. At the Compressed Sensing Incubator meeting held in April 2014 at OSA Headquarters in Washington, DC, presentations were given summarizing some of the research efforts ongoing in compressive sensing for x-ray computed tomography and magnetic resonance imaging systems. This article provides an expanded version of these presentations. Sparsity-exploiting reconstruction algorithms that have gained popularity in the medical imaging community are studied, and examples of clinical applications that could benefit from compressive sensing ideas are provided. The current and potential future impact of compressive sensing on the medical imaging field is discussed.For x-ray tomographic imaging, CS is of interest due to the possibilities of x-ray dose reduction, motion artifact mitigation, and novel scan designs. The typical diagnostic CT scan subjects a patient to an x-ray dose of about a factor of one hundred greater than that of a single projection x-ray. Reduction of sampling requirements is one obvious way to reduce the dose burden of CT imaging. Acquiring fewer views than current practice also may allow for faster acquisitions particularly for C-arm imagers with flat-panel x-ray detectors. Faster acquisition times can alleviate imaging problems related to motion due to, e.g., breathing. One of the fast-growing applications of x-ray tomographic imaging is for guidance in radiation therapy or surgery. In such applications, the standard tomographic scan, where the x-ray source executes a complete data arc greater than 180 degrees, may not be possible. Limiting the angular range of the x-ray source scanning arc is thus another important form of data undersampling.', 'kwd': '-', 'title': u'Compressive sensing in medical imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3999310/', 'p': u'Repeated x-ray computed tomography (CT) scans are often required in several specific applications such as perfusion imaging, image-guided biopsy needle, image-guided intervention, and radiotherapy with noticeable benefits. However, the associated cumulative radiation dose significantly increases as comparison with that used in the conventional CT scan, which has raised major concerns in patients. In this study, to realize radiation dose reduction by reducing the x-ray tube current and exposure time (mAs) in repeated CT scans, we propose a prior-image induced nonlocal (PINL) regularization for statistical iterative reconstruction via the penalized weighted least-squares (PWLS) criteria, which we refer to as \u201cPWLS-PINL\u201d. Specifically, the PINL regularization utilizes the redundant information in the prior image and the weighted least-squares term considers a data-dependent variance estimation, aiming to improve current low-dose image quality. Subsequently, a modified iterative successive over-relaxation algorithm is adopted to optimize the associative objective function. Experimental results on both phantom and patient data show that the present PWLS-PINL method can achieve promising gains over the other existing methods in terms of the noise reduction, low-contrast object detection and edge detail preservation.An anthropomorphic torso phantom (Radiology Support Devices, Inc., Long Beach, CA) was used for the experimental data acquisition, as shown in Fig. 1(a). The phantom was scanned by a clinical CT scanner (Siemens SOMATOM Sensation 16 CT) at three exposure levels, i.e., 17, 40, 100 mAs. For each exposure level, the tube voltage was set at 120 kVp and the phantom was scanned in a cine mode at a fixed bed position. Fig. 1(b) shows the CT image reconstructed by the FBP method with an optimized Hamming filter from the sinogram data acquired at 100 mAs, 120 kVp. The deformed images were simulated by mechanically performing a cosine transform warped distortion on the images reconstructed by the FBP method from the sinogram data acquired at 100 mAs, 120 kVp. To obtain the registered prior images, the deformed images were registered to the images reconstructed by the FBP method from the sinogram data acquired at 17 and 40 mAs, respectively.', 'kwd': u'X-ray computed tomography, prior image, statistical iterative reconstruction, penalized weighted least-squares, regularization', 'title': u'Iterative Reconstruction for X-Ray Computed Tomography using Prior-Image Induced Nonlocal Regularization'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4896130/', 'p': u'We describe X-ray computed tomography (CT) datasets from three specimens recovered from Early Cretaceous lakebeds of China that illustrate the forensic interpretation of CT imagery for paleontology. Fossil vertebrates from thinly bedded sediments often shatter upon discovery and are commonly repaired as amalgamated mosaics grouted to a solid backing slab of rock or plaster. Such methods are prone to inadvertent error and willful forgery, and once required potentially destructive methods to identify mistakes in reconstruction. CT is an efficient, nondestructive alternative that can disclose many clues about how a specimen was handled and repaired. These annotated datasets illustrate the power of CT in documenting specimen integrity and are intended as a reference in applying CT more broadly to evaluating the authenticity of comparable fossils.(Figs 1,\u200b,22,\u200b,33,\u200b,44,\u200b,55,\u200b,6;6; see Table 1 for scanning parameters; Table 2 for data output; Table 3 for movies; see also Supplementary Figures; Data Citation 1; additional information is available at: http://digimorph.org/specimens/Confuciusornis_sp/skeleton/). This unnumbered specimen was provided to us for scanning in 1998 by Mr Guan Jian of the Beijing Museum of Natural History, as an early test of whether specimens from the newly discovered Liaoning basin were amenable to CT scanning59. It was reportedly collected from the lower Yixian Formation, but its precise locality within the Liaoning basin is unknown and it came to us with no other documentation. It is now housed in the Institute for Vertebrate Paleontology and Paleoanthropology in Beijing.', 'kwd': u'Research data, Palaeontology, X-ray tomography', 'title': u'X-ray computed tomography datasets for forensic analysis of vertebrate fossils'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359488/', 'p': u'Roots are vital to plants for soil exploration and uptake of water and nutrients. Root performance is critical for growth and yield of plants, in particular when resources are limited. Since roots develop in strong interaction with the soil matrix, tools are required that can visualize and quantify root growth in opaque soil at best in 3D. Two modalities that are suited for such investigations are X-ray Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Due to the different physical principles they are based on, these modalities have their specific potentials and challenges for root phenotyping. We compared the two methods by imaging the same root systems grown in 3 different pot sizes with inner diameters of 34\xa0mm, 56\xa0mm or 81\xa0mm.Both methods successfully visualized roots of two weeks old bean plants in all three pot sizes. Similar root images and almost the same root length were obtained for roots grown in the small pot, while more root details showed up in the CT images compared to MRI. For the medium sized pot, MRI showed more roots and higher root lengths whereas at some spots thin roots were only found by CT and the high water content apparently affected CT more than MRI. For the large pot, MRI detected much more roots including some laterals than CT.', 'kwd': u'X-ray Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Root system architecture, Common bean (Phaseolus vulgaris L.) 3D imaging, Roots in soil, Non-destructive', 'title': u'Direct comparison of MRI and X-ray CT technologies for 3D imaging of root systems in soil: potential and challenges for root trait quantification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478850/', 'p': u'Rhizoctonia solani is a plant pathogenic fungus that causes significant establishment and yield losses to several important food crops globally. This is the first application of high resolution X-ray micro Computed Tomography (X-ray \u03bcCT) and real-time PCR to study host\u2013pathogen interactions in situ and elucidate the mechanism of Rhizoctonia damping-off disease over a 6-day period caused by R. solani, anastomosis group (AG) 2-1 in wheat (Triticum aestivum cv. Gallant) and oil seed rape (OSR, Brassica napus cv. Marinka). Temporal, non-destructive analysis of root system architectures was performed using RooTrak and validated by the destructive method of root washing. Disease was assessed visually and related to pathogen DNA quantification in soil using real-time PCR. R. solani AG2-1 at similar initial DNA concentrations in soil was capable of causing significant damage to the developing root systems of both wheat and OSR. Disease caused reductions in primary root number, root volume, root surface area, and convex hull which were affected less in the monocotyledonous host. Wheat was more tolerant to the pathogen, exhibited fewer symptoms and developed more complex root systems. In contrast, R. solani caused earlier damage and maceration of the taproot of the dicot, OSR. Disease severity was related to pathogen DNA accumulation in soil only for OSR, however, reductions in root traits were significantly associated with both disease and pathogen DNA. The method offers the first steps in advancing current understanding of soil-borne pathogen behavior in situ at the pore scale, which may lead to the development of mitigation measures to combat disease influence in the field.The replicate subset allocated for destructive sampling at 6 dfi (12 columns), were scanned at 2, 4, and 6 days using a Phoenix Nanotom\xae (GE Measurement & Control Solutions, Wunstorf, Germany) X-ray \u03bcCT scanner. The scanner consists of a 180 kV nanofocus X-ray tube fitted with a tungsten transmission target and a 5-megapixel (2304 \xd7 2304 pixels, 50 \xd7 50 \u03bcm pixel size) flat panel detector (Hamamatsu Photonics KK, Shizuoka, Japan). A maximum X-ray energy of 110 kV, 140 \u03bcA current and a 0.15 mm thick copper filter was used to scan each sample which consisted of 1300 projection images acquired over a 360\xb0rotation. Each projection image was the average of three images acquired with a detector exposure time of 500 ms in \u2018Fast CT mode.\u2019 The resulting isotropic voxel edge length was 19 \u03bcm (i.e., spatial resolution) and total scan time was 35 min. The total X-ray dose for each sample was calculated as 25.2 Gy over the three scans, which is below the 33 Gy threshold reported by Johnson (1936) which no detrimental effects of post-germination plant growth following exposure to X-ray radiation were observed (Zappala et al., 2013a). Reconstruction of the projection images was performed using the software datos| rec (GE Measurement & Control Solutions, Wunstorf, Germany) to produce 3-D volumetric data sets with dimension 30 \xd7 30 mm (diameter \xd7 depth).', 'kwd': u'Rhizoctonia solani, X-ray Computed Tomography, qPCR, wheat, oil seed rape, fungi, soil', 'title': u'Effects of damping-off caused by Rhizoctonia solani anastomosis group 2-1 on roots of wheat and oil seed rape quantified using X-ray Computed Tomography and real-time PCR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431860/', 'p': u'Tight gas sandstone samples are imaged at high resolution industrial X-ray computed tomography (ICT) systems to provide a three-dimensional quantitative characterization of the fracture geometries. Fracture networks are quantitatively analyzed using a combination of 2-D slice analysis and 3-D visualization and counting. The core samples are firstly scanned to produce grayscale slices, and the corresponding fracture area, length, aperture and fracture porosity as well as fracture density were measured. Then the 2-D slices were stacked to create a complete 3-D image using volume-rendering software. The open fractures (vug) are colored cyan whereas the calcite-filled fractures (high density objects) are colored magenta. The surface area and volume of both open fractures and high density fractures are calculated by 3-D counting. Then the fracture porosity and fracture aperture are estimated by 3-D counting. The fracture porosity and aperture from ICT analysis performed at atmospheric pressure are higher than those calculated from image logs at reservoir conditions. At last, the fracture connectivity is determined through comparison of fracture parameters with permeability. Distribution of fracture density and fracture aperture determines the permeability and producibility of tight gas sandstones. ICT has the advantage of performing three dimensional fracture imaging in a non-destructive way.A visual inspection of each fracture density and the average aperture were performed on each core segment. The fracture types observed in these core segments consist of extensional fractures (Fig.\xa03A,B) and the coring induced petal fractures (Fig.\xa03C). In addition, there are no fractures could be detected in some samples (Fig.\xa03D). Fractures are sometimes filled with calcites (Fig.\xa03A,B). The directional fracture measurements (azimuth and dip) were conducted on the circumferential CAT scan images. The projection or the sine curve fracture picks are conducted on a mirrored circumferential CAT scan view; so that, it appears that you are looking from within the well bore versus looking at the surface of the core (Fig.\xa03E). These mirrored images should look like and be correlatable to downhole image logs like FMI log (Fig.\xa03E).\n', 'kwd': '-', 'title': u'Three-dimensional quantitative fracture analysis of tight gas sandstones using industrial computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3905628/', 'p': u'Computed tomography (CT) is an X-ray based whole body imaging technique that is widely used in medicine. Clinically approved contrast agents for CT are iodinated small molecules or barium suspensions. Over the past seven years there has been a great increase in the development of nanoparticles as CT contrast agents. Nanoparticles have several advantages over small molecule CT contrast agents, such as long blood-pool residence times, and the potential for cell tracking and targeted imaging applications. Furthermore, there is a need for novel CT contrast agents, due to the growing population of renally impaired patients and patients hypersensitive to iodinated contrast. Micelles and lipoproteins, a micelle-related class of nanoparticle, have notably been adapted as CT contrast agents. In this review we discuss the principles of CT image formation and the generation of CT contrast. We discuss the progress in developing non-targeted, targeted and cell tracking nanoparticle CT contrast agents. We feature agents based on micelles and used in conjunction with spectral CT. The large contrast agent doses needed will necessitate careful toxicology studies prior to clinical translation. However, the field has seen tremendous advances in the past decade and we expect many more advances to come in the next decade.Cell tracking is the process of imaging the delivery and movements of cells in vivo. This is frequently achieved by labeling cells ex vivo, injecting them into the subject and using an imaging technique to track the cells over time. This was first pursued using nuclear imaging in combination with indium-111 labeling.(146) Cell tracking has been extensively pursued for MRI by loading cells with iron oxides and has been used to study stem cell therapies and monocyte behavior, for example.(147-149) This topic has barely begun to be explored for CT. The Bulte group has published several reports on tracking pancreatic islet cells that are encapsulated in alginate.(150) These capsules may be made inherently radiopaque by using barium or bismuth ions to cross-link the alginate, and CT imaging has been used to track such capsules.(151) This group has also explored loading the alginate capsules with the gadolinium labeled gold nanoparticles developed by Alric et al., which were mentioned above.(51) These capsules can be detected with CT, T1-weighted MRI and ultrasound and co-encapsulation of the cells with the nanoparticles had no effect on the cell viability (CT resolution 83 \u03bcm). Furthermore, perfluorooctylbromide nanoemulsions have also been included in these capsules, allowing their detection by CT, 19F MRI and ultrasound.(152) The resolution of the in vivo imaging experiments in this study was 353 \u03bcm. Menk et al. used gold nanoparticles coated with horse serum proteins to label cancer cells.(153) These cells were injected into brains of rats and their distribution was imaged with small animal CT systems (30 \u03bcm resolution). The application of CT to cell tracking is in its infancy, and progress will likely be challenging due to the poor sensitivity of CT and toleration by the cells of very high levels of contrast media loading. Nevertheless, we anticipate more reports on this topic in the coming years.', 'kwd': u'nanoparticle, micelle, computed tomography, X-ray, spectral CT, iodine, gold nanoparticle, lipoprotein, molecular imaging, bismuth', 'title': u'Nanoparticle Contrast Agents for Computed Tomography: A Focus on Micelles'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4750279/', 'p': u'Lung diseases (resulting from air pollution) require a widely accessible method for risk estimation and early diagnosis to ensure proper and responsive treatment. Radiomics-based fractal dimension analysis of X-ray computed tomography attenuation patterns in chest voxels of mice exposed to different air polluting agents was performed to model early stages of disease and establish differential diagnosis.To model different types of air pollution, BALBc/ByJ mouse groups were exposed to cigarette smoke combined with ozone, sulphur dioxide gas and a control group was established. Two weeks after exposure, the frequency distributions of image voxel attenuation data were evaluated. Specific cut-off ranges were defined to group voxels by attenuation. Cut-off ranges were binarized and their spatial pattern was associated with calculated fractal dimension, then abstracted by the fractal dimension -- cut-off range mathematical function. Nonparametric Kruskal-Wallis (KW) and Mann\u2013Whitney post hoc (MWph) tests were used.', 'kwd': u'Fractal dimension, Radiomics, In vivo micro-CT, Air pollution, Lung disease', 'title': u'Radiomics-based differentiation of lung disease models generated by polluted air based on X-ray computed tomography data'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4424483/', 'p': u'We introduce phase-diagram analysis, a standard tool in compressed sensing (CS), to the X-ray computed tomography (CT) community as a systematic method for determining how few projections suffice for accurate sparsity-regularized reconstruction. In CS, a phase diagram is a convenient way to study and express certain theoretical relations between sparsity and sufficient sampling. We adapt phase-diagram analysis for empirical use in X-ray CT for which the same theoretical results do not hold. We demonstrate in three case studies the potential of phase-diagram analysis for providing quantitative answers to questions of undersampling. First, we demonstrate that there are cases where X-ray CT empirically performs comparably with a near-optimal CS strategy, namely taking measurements with Gaussian sensing matrices. Second, we show that, in contrast to what might have been anticipated, taking randomized CT measurements does not lead to improved performance compared with standard structured sampling patterns. Finally, we show preliminary results of how well phase-diagram analysis can predict the sufficient number of projections for accurately reconstructing a large-scale image of a given sparsity by means of total-variation regularization.Many forms of randomness can be conceived in CT sampling. In this work, we consider two straightforward ones. The first is a fan-beam geometry denoted fanbeam_rand in which the source angular positions are no longer equi-distant but sampled uniformly from [0,360\xb0]. Second, we consider a set-up we denote random_rays of independent random rays through the image. Each ray is specified by two parameters: the angle of the ray with a fixed coordinate axis and the intersection of the ray with the orthogonal diameter of the disc-shaped image. The angle and intersection are sampled from uniform distributions on [0,180]\xb0 and [\u2212Nside/2,Nside/2], respectively, where Nside is the diameter length and the image is assumed centred around the origin.', 'kwd': u'computed tomography, compressed sensing, image reconstruction, sparsity regularization, sampling', 'title': u'How little data is enough? Phase-diagram analysis of sparsity-regularized X-ray computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4083059/', 'p': u'To realize low-dose imaging in X-ray computed tomography (CT) examination, lowering milliampere-seconds (low-mAs) or reducing the required number of projection views (sparse-view) per rotation around the body has been widely studied as an easy and effective approach. In this study, we are focusing on low-dose CT image reconstruction from the sinograms acquired with a combined low-mAs and sparse-view protocol and propose a two-step image reconstruction strategy. Specifically, to suppress significant statistical noise in the noisy and insufficient sinograms, an adaptive sinogram restoration (ASR) method is first proposed with consideration of the statistical property of sinogram data, and then to further acquire a high-quality image, a total variation based projection onto convex sets (TV-POCS) method is adopted with a slight modification. For simplicity, the present reconstruction strategy was termed as \u201cASR-TV-POCS.\u201d To evaluate the present ASR-TV-POCS method, both qualitative and quantitative studies were performed on a physical phantom. Experimental results have demonstrated that the present ASR-TV-POCS method can achieve promising gains over other existing methods in terms of the noise reduction, contrast-to-noise ratio, and edge detail preservation.Sparse-view CT image reconstruction is known as an ill-posed problem. To address this issue, Sidky et al. proposed a general iterative scheme through successive and repeated applications of POCS operator with TV minimization [19]. The associative objective function can be written as follows:\n', 'kwd': u'(110.7440) X-ray imaging, (100.3190) Inverse problems, (100.3010) Image reconstruction techniques, (110.6960) Tomography', 'title': u'Low-dose X-ray computed tomography image reconstruction with a combined low-mAs and sparse-view protocol'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693932/', 'p': u'Conceived and designed the experiments: SZ JRH SRT SM CJS TP MB SJM. Performed the experiments: SZ JRH SRT SM. Analyzed the data: SZ JRH SRT SM CJS. Contributed reagents/materials/analysis tools: SZ JRH SRT SM CJS TP MB SJM. Wrote the paper: SZ JRH SRT SM CJS TP MB SJM.X-ray Computed Tomography (CT) is a non-destructive imaging technique originally designed for diagnostic medicine, which was adopted for rhizosphere and soil science applications in the early 1980s. X-ray CT enables researchers to simultaneously visualise and quantify the heterogeneous soil matrix of mineral grains, organic matter, air-filled pores and water-filled pores. Additionally, X-ray CT allows visualisation of plant roots in situ without the need for traditional invasive methods such as root washing. However, one routinely unreported aspect of X-ray CT is the potential effect of X-ray dose on the soil-borne microorganisms and plants in rhizosphere investigations. Here we aimed to i) highlight the need for more consistent reporting of X-ray CT parameters for dose to sample, ii) to provide an overview of previously reported impacts of X-rays on soil microorganisms and plant roots and iii) present new data investigating the response of plant roots and microbial communities to X-ray exposure. Fewer than 5% of the 126 publications included in the literature review contained sufficient information to calculate dose and only 2.4% of the publications explicitly state an estimate of dose received by each sample. We conducted a study involving rice roots growing in soil, observing no significant difference between the numbers of root tips, root volume and total root length in scanned versus unscanned samples. In parallel, a soil microbe experiment scanning samples over a total of 24 weeks observed no significant difference between the scanned and unscanned microbial biomass values. We conclude from the literature review and our own experiments that X-ray CT does not impact plant growth or soil microbial populations when employing a low level of dose (<30 Gy). However, the call for higher throughput X-ray CT means that doses that biological samples receive are likely to increase and thus should be closely monitored.', 'kwd': '-', 'title': u'Effects of X-Ray Dose On Rhizosphere Studies Using X-Ray Computed Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4626840/', 'p': u'We report the development of laboratory based hyperspectral X-ray computed tomography which allows the internal elemental chemistry of an object to be reconstructed and visualised in three dimensions. The method employs a spectroscopic X-ray imaging detector with sufficient energy resolution to distinguish individual elemental absorption edges. Elemental distributions can then be made by K-edge subtraction, or alternatively by voxel-wise spectral fitting to give relative atomic concentrations. We demonstrate its application to two material systems: studying the distribution of catalyst material on porous substrates for industrial scale chemical processing; and mapping of minerals and inclusion phases inside a mineralised ore sample. The method makes use of a standard laboratory X-ray source with measurement times similar to that required for conventional computed tomography.A HEXITEC spectroscopic detector was installed in a Nikon XTH 225 system. The HEXITEC detector consists of a 1\u2009mm thick CdTe single crystal detector (20\u2009\xd7\u200920\u2009mm2) bump-bonded to a large area ASIC packaged with a high performance data acquisition system. The detector has 80\u2009\xd7\u200980 pixels on a 250\u2009\u03bcm pitch with an energy resolution of 800\u2009eV at 59.5\u2009keV and 1.5\u2009keV at 141\u2009keV17. During operation each photon event has its energy, pixel position and the frame in which it occurs recorded. Events are processed and histogrammed according to measured energy into 0.25\u2009keV wide bins. We typical use between 400\u2013800\u2009bins, depending on the maximum X-ray energy. Normally, during this process, a correction is employed to deal with photons that may have shared its energy between two or more pixels which appear to be measured as multiple lower energy photon measurements on neighbouring pixels. When the flux is sufficiently low it is possible to identify these shared events and reconstruct the correct photon energy in the correct location. However due to a high flux of radiation and therefore a high percentage occupancy of events per frame (making it very difficult to identify shared events), we did not employ a charge sharing correction strategy in this case. Previous studies have shown that this does not significantly impact on the measured position of an absorption edge24,41. An inter pixel energy calibration was performed using a correlative optimised warping algorithm using data from a flat-field fluorescence image off a series of metals42.', 'kwd': '-', 'title': u'3D chemical imaging in the laboratory by hyperspectral X-ray computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4497654/', 'p': u'Conceived and designed the experiments: WY LZ. Performed the experiments: WY. Analyzed the data: WY LZ. Contributed reagents/materials/analysis tools: WY LZ. Wrote the paper: WY LZ.In medical and industrial applications of computed tomography (CT) imaging, limited by the scanning environment and the risk of excessive X-ray radiation exposure imposed to the patients, reconstructing high quality CT images from limited projection data has become a hot topic. X-ray imaging in limited scanning angular range is an effective imaging modality to reduce the radiation dose to the patients. As the projection data available in this modality are incomplete, limited-angle CT image reconstruction is actually an ill-posed inverse problem. To solve the problem, image reconstructed by conventional filtered back projection (FBP) algorithm frequently results in conspicuous streak artifacts and gradual changed artifacts nearby edges. Image reconstruction based on total variation minimization (TVM) can significantly reduce streak artifacts in few-view CT, but it suffers from the gradual changed artifacts nearby edges in limited-angle CT. To suppress this kind of artifacts, we develop an image reconstruction algorithm based on \u21130 gradient minimization for limited-angle CT in this paper. The \u21130-norm of the image gradient is taken as the regularization function in the framework of developed reconstruction model. We transformed the optimization problem into a few optimization sub-problems and then, solved these sub-problems in the manner of alternating iteration. Numerical experiments are performed to validate the efficiency and the feasibility of the developed algorithm. From the statistical analysis results of the performance evaluations peak signal-to-noise ratio (PSNR) and normalized root mean square distance (NRMSD), it shows that there are significant statistical differences between different algorithms from different scanning angular ranges (p<0.0001). From the experimental results, it also indicates that the developed algorithm outperforms classical reconstruction algorithms in suppressing the streak artifacts and the gradual changed artifacts nearby edges simultaneously.', 'kwd': '-', 'title': u'\u21130 Gradient Minimization Based Image Reconstruction for Limited-Angle Computed Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4570663/', 'p': u'Conceived and designed the experiments: SH PB CW WO. Performed the experiments: SH CW WO. Analyzed the data: SH RML. Contributed reagents/materials/analysis tools: SH CW RML WO. Wrote the paper: SH PB CW RML WO.There is currently a significant need to improve our understanding of the factors that control a number of critical soil processes by integrating physical, chemical and biological measurements on soils at microscopic scales to help produce 3D maps of the related properties. Because of technological limitations, most chemical and biological measurements can be carried out only on exposed soil surfaces or 2-dimensional cuts through soil samples. Methods need to be developed to produce 3D maps of soil properties based on spatial sequences of 2D maps. In this general context, the objective of the research described here was to develop a method to generate 3D maps of soil chemical properties at the microscale by combining 2D SEM-EDX data with 3D X-ray computed tomography images. A statistical approach using the regression tree method and ordinary kriging applied to the residuals was developed and applied to predict the 3D spatial distribution of carbon, silicon, iron, and oxygen at the microscale. The spatial correlation between the X-ray grayscale intensities and the chemical maps made it possible to use a regression-tree model as an initial step to predict the 3D chemical composition. For chemical elements, e.g., iron, that are sparsely distributed in a soil sample, the regression-tree model provides a good prediction, explaining as much as 90% of the variability in some of the data. However, for chemical elements that are more homogenously distributed, such as carbon, silicon, or oxygen, the additional kriging of the regression tree residuals improved significantly the prediction with an increase in the R2 value from 0.221 to 0.324 for carbon, 0.312 to 0.423 for silicon, and 0.218 to 0.374 for oxygen, respectively. The present research develops for the first time an integrated experimental and theoretical framework, which combines geostatistical methods with imaging techniques to unveil the 3-D chemical structure of soil at very fine scales. The methodology presented in this study can be easily adapted and applied to other types of data such as bacterial or fungal population densities for the 3D characterization of microbial distribution.', 'kwd': '-', 'title': u'Three-Dimensional Mapping of Soil Chemical Characteristics at Micrometric Scale by Combining 2D SEM-EDX Data and 3D X-Ray CT Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5544716/', 'p': u'Creep cavitation in an ex-service nuclear steam header Type 316 stainless steel sample is investigated through a multiscale tomography workflow spanning eight orders of magnitude, combining X-ray computed tomography (CT), plasma focused ion beam (FIB) scanning electron microscope (SEM) imaging and scanning transmission electron microscope (STEM) tomography. Guided by microscale X-ray CT, nanoscale X-ray CT is used to investigate the size and morphology of cavities at a triple point of grain boundaries. In order to understand the factors affecting the extent of cavitation, the orientation and crystallographic misorientation of each boundary is characterised using electron backscatter diffraction (EBSD). Additionally, in order to better understand boundary phase growth, the chemistry of a single boundary and its associated secondary phase precipitates is probed through STEM energy dispersive X-ray (EDX) tomography. The difference in cavitation of the three grain boundaries investigated suggests that the orientation of grain boundaries with respect to the direction of principal stress is important in the promotion of cavity formation.A FEI Helios Xe plasma-FIB was used to extract a pillar of approximately 25\u2009\xb5m diameter, at the location of the cavitated boundary, for subsequent nanoscale X-ray CT. Initially, the location of the cavitated grain boundary was confirmed through milling of a cross-section and ion-beam imaging. Subsequently, annular milling was used to prepare a pillar, using a milling current of 1.3\u2009\xb5A and two subsequent polishing steps of 180\u2009nA and 59\u2009nA. The pillar was attached to a liftout probe, undercut with a 59\u2009nA beam and was then subsequently lifted out. The pillar was then attached to the tip of a copper pin using Pt deposition, before the liftout probe was detached. The ion-beam remained at an accelerating voltage of 30\u2009kV throughout. The micropillar was produced so that its long-axis is perpendicular to the direction of the macroscale crack plane, meaning the long-axis of the pillar is parallel to the principal stress direction. The plasma-FIB provides a rapid method of machining volumes at the scales of tens or hundreds of micrometres, providing milling rates up to fifty times greater than conventional Ga+ ion beam milling32, 33, which was vital to creating a micropillar of this scale at a site specific location in a reasonable time. The micropillar was attached to a pin sample holder for further analysis after lift-out.', 'kwd': '-', 'title': u'Multiscale correlative tomography: an investigation of creep cavitation in 316 stainless steel'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706545/', 'p': u'To develop a core/shell nanodimer of gold (core) and silver iodine (shell) as a dual-modal contrast-enhancing agent for biomarker targeted x-ray computed tomography (CT) and photoacoustic imaging (PAI) applications.The gold and silver iodine core/shell nanodimer (Au/AgICSD) was prepared by fusing together components of gold, silver, and iodine. The physicochemical properties of Au/AgICSD were then characterized using different optical and imaging techniques (e.g., HR- transmission electron microscope, scanning transmission electron microscope, x-ray photoelectron spectroscopy, energy-dispersive x-ray spectroscopy, Z-potential, and UV-vis). The CT and PAI contrast-enhancing effects were tested and then compared with a clinically used CT contrast agent and Au nanoparticles. To confer biocompatibility and the capability for efficient biomarker targeting, the surface of the Au/AgICSD nanodimer was modified with the amphiphilic diblock polymer and then functionalized with transferrin for targeting transferrin receptor that is overexpressed in various cancer cells. Cytotoxicity of the prepared Au/AgICSD nanodimer was also tested with both normal and cancer cell lines.', 'kwd': u'contrast agent, core\u2013shell, computer tomography, gold, silver iodine, nanoparticles, photoacoustic imaging', 'title': u'A nanocomposite of Au-AgI core/shell dimer as a dual-modality contrast agent for x-ray computed tomography and photoacoustic imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5449646/', 'p': u"Laboratory x-ray micro\u2013computed tomography (micro-CT) is a fast-growing method in scientific research applications that allows for non-destructive imaging of morphological structures. This paper provides an easily operated \u201chow to\u201d guide for new potential users and describes the various steps required for successful planning of research projects that involve micro-CT. Background information on micro-CT is provided, followed by relevant setup, scanning, reconstructing, and visualization methods and considerations. Throughout the guide, a Jackson's chameleon specimen, which was scanned at different settings, is used as an interactive example. The ultimate aim of this paper is make new users familiar with the concepts and applications of micro-CT in an attempt to promote its use in future scientific studies.The voxel size of a micro-CT image is dependent on the magnification and object size as described above. This is related to the distance of the sample from the x-ray source and the detector [4]. Voxel size and spatial resolution are two concepts that are often confused since the voxel size is the size of a pixel in 3D space, i.e., the width of one volumetric pixel (isotropic in three dimensions). This value does not consider the actual spatial resolution capability of the scan system. For example, if the x-ray spot size (focused x-ray spot from the source) becomes larger than the chosen voxel size, the spatial resolution of the system becomes poorer. That means that fewer details are detectable, despite a good voxel size, due to the actual resolution being non-optimal. Since most commercial systems limit the size of the x-ray spot to the required voxel size (or provide the user an indication of this), the actual and voxel resolution are usually the same, but this is not regularly tested or reported. It is possible to use resolution standards (such as calibrated-thickness metal wires) to confirm spatial resolution, and some reference standards exist, although a generally accepted standard for industrial CT systems does not yet exist. It is therefore possible that the amount of detail that is detectable in a scan can vary considerably from system to system, or even between different scans from the same type of system. These quality differences are either due to improper settings that may result in large x-ray spot sizes or to an improper choice of other scan parameters. The sole way of testing the scan quality is to image a small feature of known dimensions and ensure the feature is visible in the CT slice image.", 'kwd': u'3D imaging, micro-computed tomography, nano-computed tomography, non-destructive analysis, x-ray tomography', 'title': u'Laboratory x-ray micro-computed tomography: a user guideline for biological samples'}], 'Coronary Artery Disease AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111838/', 'p': u'The authors are developing a computer-aided detection system to assist radiologists in analysis of coronary artery disease in coronary CT angiograms (cCTA). This study evaluated the accuracy of the authors\u2019 coronary artery segmentation and tracking method which are the essential steps to define the search space for the detection of atherosclerotic plaques.The heart region in cCTA is segmented and the vascular structures are enhanced using the authors\u2019 multiscale coronary artery response (MSCAR) method that performed 3D multiscale filtering and analysis of the eigenvalues of Hessian matrices. Starting from seed points at the origins of the left and right coronary arteries, a 3D rolling balloon region growing (RBG) method that adapts to the local vessel size segmented and tracked each of the coronary arteries and identifies the branches along the tracked vessels. The branches are queued and subsequently tracked until the queue is exhausted. With Institutional Review Board approval, 62 cCTA were collected retrospectively from the authors\u2019 patient files. Three experienced cardiothoracic radiologists manually tracked and marked center points of the coronary arteries as reference standard following the 17-segment model that includes clinically significant coronary arteries. Two radiologists visually examined the computer-segmented vessels and marked the mistakenly tracked veins and noisy structures as false positives (FPs). For the 62 cases, the radiologists marked a total of 10191 center points on 865 visible coronary artery segments.', 'kwd': u'coronary arteries, vessel segmentation, computer-aided detection, coronary artery diseases, atherosclerotic plaque, multiscale filtering', 'title': u'Computerized analysis of coronary artery disease: Performance evaluation of segmentation and tracking of coronary arteries in CT angiograms'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5456060/', 'p': u'Optical coherence tomography (OCT) is an established catheter-based imaging modality for the assessment of coronary artery disease and the guidance of stent placement during percutaneous coronary intervention. Manual analysis of large OCT datasets for vessel contours or stent struts detection is time-consuming and unsuitable for real-time applications. In this study, a fully automatic method was developed for detection of both vessel contours and stent struts. The method was applied to in vitro OCT scans of eight stented silicone bifurcation phantoms for validation purposes. The proposed algorithm comprised four main steps, namely pre-processing, lumen border detection, stent strut detection, and three-dimensional point cloud creation. The algorithm was validated against manual segmentation performed by two independent image readers. Linear regression showed good agreement between automatic and manual segmentations in terms of lumen area (r>0.99). No statistically significant differences in the number of detected struts were found between the segmentations. Mean values of similarity indexes were >95% and >85% for the lumen and stent detection, respectively. Stent point clouds of two selected cases, obtained after OCT image processing, were compared to the centerline points of the corresponding stent reconstructions from micro computed tomography, used as ground-truth. Quantitative comparison between the corresponding stent points resulted in median values of ~150 \u03bcm and ~40 \u03bcm for the total and radial distances of both cases, respectively. The repeatability of the detection method was investigated by calculating the lumen volume and the mean number of detected struts per frame for seven repeated OCT scans of one selected case. Results showed low deviation of values from the median for both analyzed quantities. In conclusion, this study presents a robust automatic method for detection of lumen contours and stent struts from OCT as supported by focused validation against both manual segmentation and micro computed tomography and by good repeatability.To test the repeatability of the lumen border detection algorithm, the lumen volume of each case was calculated as the sum of the lumen area per frame multiplied by the distance between the slices. The extremes of the stent were used as landmarks to establish the same region of interest between acquisitions. Regarding the strut detection algorithm, the mean of the number of detected struts per frame was computed for each case.', 'kwd': '-', 'title': u'Reconstruction of stented coronary arteries from optical coherence tomography images: Feasibility, validation, and repeatability of a segmentation method'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4922813/', 'p': u'After a decade of clinical use of coronary computed tomographic angiography (CCTA) to evaluate the anatomic severity of coronary artery disease, new methods of deriving functional information from CCTA have been developed. These methods utilize the anatomic information provided by CCTA in conjunction with computational fluid dynamics to calculate fractional flow reserve (FFR) values from CCTA image data sets. Computed tomography-derived FFR (CT-FFR) enables the identification of lesion-specific drop noninvasively. A three-dimensional CT-FFR modeling technique, which provides FFR values throughout the coronary tree (HeartFlow FFRCT analysis), has been validated against measured FFR and is now approved by the US Food and Drug Administration for clinical use. This technique requires off-site supercomputer analysis. More recently, a one-dimensional computational analysis technique (Siemens cFFR), which can be performed on on-site workstations, has been developed and is currently under investigation. This article reviews CT-FFR technology and clinical evidence for its use in stable patients with suspected coronary artery disease.Intermediate degrees of stenosis (30%\u201370%) present the greatest challenge in the diagnosis of CAD. Since hemodynamically significant lesions are occasionally observed in intermediate lesions with <70% stenosis,13 the use of invasive FFR is recommended to evaluate the function of intermediate coronary lesions as a class IIa indication.6 However, given the relatively lower prevalence of lesion-specific pressure drop caused by intermediate stenosis compared to that of severe stenosis in the FAME study,13 CT-derived FFR would be of great use for assessing the functional significance of intermediate lesions to avoid unnecessary ICA and help in treatment decision making. Table 2 provides a summary of the studies of FFRCT and cFFR. Similar to the overall diagnostic accuracy of CT-derived FFR, all studies demonstrated high diagnostic performance for intermediate stenosis, with the highest accuracy and specificity for FFRCT.24,31,38,39', 'kwd': u'fractional flow reserve, coronary computed tomographic angiography, FFRCT, cFFR', 'title': u'Noninvasive FFR derived from coronary CT angiography in the management of coronary artery disease: technology and clinical update'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5018003/', 'p': u'The authors are developing an automated method to identify the best-quality coronary arterial segment from multiple-phase coronary CT angiography (cCTA) acquisitions, which may be used by either interpreting physicians or computer-aided detection systems to optimally and efficiently utilize the diagnostic information available in multiple-phase cCTA for the detection of coronary artery disease.After initialization with a manually identified seed point, each coronary artery tree is automatically extracted from multiple cCTA phases using our multiscale coronary artery response enhancement and 3D rolling balloon region growing vessel segmentation and tracking method. The coronary artery trees from multiple phases are then aligned by a global registration using an affine transformation with quadratic terms and nonlinear simplex optimization, followed by a local registration using a cubic B-spline method with fast localized optimization. The corresponding coronary arteries among the available phases are identified using a recursive coronary segment matching method. Each of the identified vessel segments is transformed by the curved planar reformation (CPR) method. Four features are extracted from each corresponding segment as quality indicators in the original computed tomography volume and the straightened CPR volume, and each quality indicator is used as a voting classifier for the arterial segment. A weighted voting ensemble (WVE) classifier is designed to combine the votes of the four voting classifiers for each corresponding segment. The segment with the highest WVE vote is then selected as the best-quality segment. In this study, the training and test sets consisted of 6 and 20 cCTA cases, respectively, each with 6 phases, containing a total of 156 cCTA volumes and 312 coronary artery trees. An observer preference study was also conducted with one expert cardiothoracic radiologist and four nonradiologist readers to visually rank vessel segment quality. The performance of our automated method was evaluated by comparing the automatically identified best-quality segments identified by the computer to those selected by the observers.', 'kwd': u'coronary CT angiography, image analysis, computer-aided reading', 'title': u'Coronary artery analysis: Computer-assisted selection of best-quality segments in multiple-phase coronary CT angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4499869/', 'p': u'The initial course of atherosclerotic disease is thought to begin in early adulthood. In young adults lesions in the arterial vessel wall have been observed surprisingly frequently13 but the prognostic relevance of early, adaptive or reversible changes like \u201cfatty streak\u201d or intimal thickening remains a matter of debate. Pathology studies seek to integrate autopsy findings from various stages of atherosclerosis to provide a putative sequence of events4. In brief, intimal thickening is observed early in the disease process. The early atherosclerotic lesion is composed of smooth muscle cells and is affected by increased macrophage and lipid influx. If this process continues, a necrotic core is formed and the lesion progresses to a fibrous cap atheroma. The necrotic core contains lipids and apoptotic macrophages. A stable fibrous cap may prevent rupture of the lesion. If the fibrous cap loses matrix proteins and smooth muscle cells, a thin cap atheroma can result. Intraplaque hemorrhage is also seen frequently in this entity, leading to further enlargement of the lipid core. The risk of plaque rupture is increased as the fibrous cap thins and the lipid core enlarges14. The \u201cfibrocalcific plaque\u201d is considered to be a feature of more stable plaque, although the processes involved in calcification are not fully understood.It is generally conceived that therapeutic intervention for atherosclerosisis most effective when started at an early stage of the progressive disease process 15. Imaging tools have provided a substantial database of knowledge regarding disease burden. Imaging of the larger surface vessels (carotid or femoral arteries) has been extensively used to detect early systemic vascular pathology 16. Calcium detection using non-contrast CT provides a direct approach to assessing coronary atherosclerosis burden. The coronary artery calcium score has strong predictive power for cardiovascular events in asymptomatic subjects17. However, calcium deposition is felt to be a late event in the formation of atherosclerotic plaque. The relevance of non-calcified plaque is emphasized by prospective IVUS studies that show coronary fibroatheroma without significant calcification confers an elevated risk for myocardial infarction 18. Non-calcified plaque is more common than calcified plaque in asymptomatic individuals younger than 45 years. The ability to noninvasively image non-calcified plaque or wall thickening of the coronary arteries using MRI or CT enables the detection of earlier stages of atherosclerotic disease 19, 20.', 'kwd': u'imaging, coronary disease, plaque, atherosclerosis', 'title': u'Noninvasive Imaging of Atherosclerotic Plaque Progression: Status of Coronary CT Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3750415/', 'p': u'Author contributions: Guarantors of integrity of entire study, Y.H., J.L.N., G.S.K.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; manuscript final version approval, all authors; literature research, Y.H., T.W., D.L.B., G.S.K.; clinical studies, S.S., J.L.N.; statistical analysis, Y.H., T.W., G.S.K.; and manuscript editing, Y.H., T.W., J.S.C., S.S., S.D.T., D.L.B., G.S.K.To provide proof of concept for a diagnostic method to assess diffuse coronary artery disease (CAD) on the basis of coronary computed tomography (CT) angiography.', 'kwd': '-', 'title': u'CT-based Diagnosis of Diffuse Coronary Artery Disease on the Basis of Scaling Power Laws'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4355954/', 'p': u'Epicardial fat may play a role in the pathogenesis of coronary artery disease (CAD). We explored the relationship of epicardial fat volume (EFV) with the presence and severity of CAD or myocardial perfusion abnormalities in a diverse, symptomatic patient population.In a diverse population of symptomatic patients referred for invasive coronary angiography, we did not find associations of epicardial fat volume with the presence and severity of coronary artery disease or with myocardial perfusion abnormalities. The clinical significance of quantifying epicardial fat volume remains uncertain but may relate to the pathophysiology of acute coronary events rather than the presence of atherosclerotic disease.', 'kwd': u'epicardial fat, pericardial fat, coronary artery disease, coronary artery calcification, coronary artery stenosis, myocardial ischemia', 'title': u'Lack of Association Between Epicardial Fat Volume and Extent of Coronary Artery Calcification, Severity of Coronary Artery Disease, or Presence of Myocardial Perfusion Abnormalities in a Diverse, Symptomatic Patient Population: Results from the CORE320 Multicenter Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2947500/', 'p': u'Conceived and designed the experiments: WJM JW JMAvE RMB TL. Performed the experiments: SCG. Analyzed the data: SCG MEK AGK SS TL. Contributed reagents/materials/analysis tools: RJvdG. Wrote the paper: SCG MEK AGK SS MK RJvdG WJM JW JMAvE RMB TL. Data interpretation and supervision: MEK JvE TL. Statistical analysis: AK. Assistance with data acquisition in early phase of study: MK. Developed MR method: RB. Handled funding: TL.Magnetic resonance imaging (MRI) is sensitive to early atherosclerotic changes such as positive remodeling in patients with coronary artery disease (CAD). We assessed prevalence, quality, and extent of coronary atherosclerosis in a group of healthy subjects compared to patients with confirmed CAD.', 'kwd': '-', 'title': u'Visualization of Coronary Wall Atherosclerosis in Asymptomatic Subjects and Patients with Coronary Artery Disease Using Magnetic Resonance Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3840080/', 'p': u'A collaborative framework was initiated to establish a community resource of ground truth segmentations from cardiac MRI. Multi-site, multi-vendor cardiac MRI datasets comprising 95 patients (73 men, 22 women; mean age 62.73 \xb1 11.24 years) with coronary artery disease and prior myocardial infarction, were randomly selected from data made available by the Cardiac Atlas Project (Fonseca et al., 2011). Three semi- and two fully-automated raters segmented the left ventricular myocardium from short-axis cardiac MR images as part of a challenge introduced at the STACOM 2011 MICCAI workshop (Suinesiaputra et al., 2012). Consensus myocardium images were generated based on the Expectation-Maximization principle implemented by the STAPLE algorithm (Warfield et al., 2004). The mean sensitivity, specificity, positive predictive and negative predictive values ranged between 0.63-0.85, 0.60-0.98, 0.56-0.94 and 0.83-0.92, respectively, against the STAPLE consensus. Spatial and temporal agreement varied in different amounts for each rater. STAPLE produced high quality consensus images if the region of interest was limited to the area of discrepancy between raters. To maintain the quality of the consensus, an objective measure based on the candidate automated rater performance distribution is proposed. The consensus segmentation based on a combination of manual and automated raters were more consistent than any particular rater, even those with manual input. The consensus is expected to improve with the addition of new automated contributions. This resource is open for future contributions, and is available as a test bed for the evaluation of new segmentation algorithms, through the Cardiac Atlas Project (www.cardiacatlas.org).The Guide-Point Modeling technique (Li et al., 2010) was used to assist the fitting of a finite element cardiac model to the CMR data. This approach involves human observer input to refine the segmentation results by positioning a small number of guide points interactively on a sparse subset of slices and frames. Both long axis and short axis images were included in the analysis. The model incorporated the basal margin of the left ventricle as a plane, which was least squares fit to points placed by the user on the hinge points of the mitral valve in the long axis images. The model surfaces were influenced by the placement of user-defined guide points, and the automatic generation of edge points as well as the automated tracking of contours through all frames using non-rigid registration. The model was spatially and temporally consistent to reduce the amount of user interaction. However, inconsistency in breath-hold position can lead to mismatches between the short and long axis images. Images were manually shifted in-plane to compensate for breath-hold mis-registration, but individual slices may show errors in segmentation due to inconsistency with surrounding images in space and time. This expert-guided method has been previously validated in animals against autopsy LV mass, in patients with regional wall motion abnormalities against manually drawn contours, and in healthy volunteers against flow-derived measurements of cardiac output (Young et al., 2000). This method required expert approval of all slices and for all frames.', 'kwd': '-', 'title': u'A Collaborative Resource to Build Consensus for Automated Left Ventricular Segmentation of Cardiac MR Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3175011/', 'p': u'To evaluate our prototype method for segmentation and tracking of the coronary arterial tree, which is the foundation for a computer-aided detection (CADe) system to be developed to assist radiologists in detecting non-calcified plaques in coronary CT angiography (cCTA) scans.The heart region was first extracted by a morphological operation and an adaptive thresholding method based on expectation-maximization (EM) estimation. The vascular structures within the heart region were enhanced and segmented using a multiscale coronary response (MSCAR) method that combined 3D multiscale filtering, analysis of the eigen values of Hessian matrices and EM estimation segmentation. After the segmentation of vascular structures, the coronary arteries were tracked by a 3D dynamic balloon tracking (DBT) method. The DBT method started at two manually identified seed points located at the origins of the left and right coronary arteries (LCA and RCA) for extraction of the arterial trees. The coronary arterial trees of a data set containing 20 ECG-gated contrast-enhanced cCTA scans were extracted by our MSCAR-DBT method and a clinical GE Advantage workstation. Two experienced thoracic radiologists visually examined the coronary arteries on the original cCTA scans and the rendered volume of segmented vessels to count the untracked false-negative (FN) segments and false positives (FPs) for both methods.', 'kwd': u'Computer-aided detection, coronary artery tracking, vessel segmentation', 'title': u'Automated coronary artery tree extraction in coronary CT angiography using a multiscale enhancement and dynamic balloon tracking (MSCAR-DBT) method'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3841469/', 'p': u'Coronary artery disease (CAD) is one of the leading causes of death in the US and a substantial health-care burden in all industrialized societies. In recent years we have witnessed a constant strive towards the development and the clinical application of novel or improved detection methods as well as therapies. Particularly, noninvasive imaging is a decisive component in the cardiovascular field. Image fusion is the ability of combining into a single integrated display the anatomical as well as the physiological data retrieved by separated modalities. Clinical evidence suggests that it represents a promising strategy in CAD assessment and risk stratification by significantly improving the diagnostic power of each modality independently considered and of the traditional side-by-side interpretation. Numerous techniques and approaches taken from the image registration field have been implemented and validated in the context of CAD assessment and management. Although its diagnostic power is widely accepted, additional technical developments are still needed to become a routinely used clinical tool.Clinical evidence suggests that image fusion can be a reliable and useful tool in the hands of clinicians for a more accurate diagnosis of CAD, specifically for ambiguous and borderline cases. Additional technical developments in the extraction of anatomical information are still in order for the whole procedure to become fully automated and enter the clinical practice.', 'kwd': u'image fusion, CAD diagnosis, computed tomography angiography, nuclear imaging', 'title': u'Multimodality image fusion for diagnosing coronary artery disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3906085/', 'p': u'Conceived and designed the experiments: TL GSK. Performed the experiments: TL. Analyzed the data: TL TW YH. Contributed reagents/materials/analysis tools: TL TW YH. Wrote the paper: TL GSK. Data Collection: BKK.Accurate computed tomography (CT)-based reconstruction of coronary morphometry (diameters, length, bifurcation angles) is important for construction of patient-specific models to aid diagnosis and therapy. The objective of this study is to validate the accuracy of patient coronary artery lumen area obtained from CT images based on intravascular ultrasound (IVUS).', 'kwd': '-', 'title': u'IVUS Validation of Patient Coronary Artery Lumen Area Obtained from CT Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4263628/', 'p': u'Author contributions: Guarantors of integrity of entire study, J.A.C.L., J.B.M., D.A.B.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; manuscript final version approval, all authors; literature research, A.C.K., G.C., C.T.S., J.A.C.L., D.L.L., D.A.B.; clinical studies, C.T.S., D.L.L., J.B.M., D.A.B.; statistical analysis, H.T.M., G.C., C.T.S., D.L.L.; and manuscript editing, A.C.K., H.T.M., G.C., C.T.S., B.D.R., J.A.C.L., D.L.L., J.B.M., D.A.B.The extent of coronary plaque in asymptomatic diabetic patients is related to body mass index and duration of diabetes.', 'kwd': '-', 'title': u'Coronary Artery Plaque Volume and Obesity in Patients with Diabetes: The Factor-64 Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4213417/', 'p': u'Computed tomography (CT) angiography represents the most important technical development in CT imaging and it has challenged invasive angiography in the diagnostic evaluation of cardiovascular abnormalities. Over the last decades, technological evolution in CT imaging has enabled CT angiography to become a first-line imaging modality in the diagnosis of cardiovascular disease. This review provides an overview of the diagnostic applications of CT angiography (CTA) in cardiovascular disease, with a focus on selected clinical challenges in some common cardiovascular abnormalities, which include abdominal aortic aneurysm (AAA), aortic dissection, pulmonary embolism (PE) and coronary artery disease. An evidence-based review is conducted to demonstrate how CT angiography has changed our approach in the diagnosis and management of cardiovascular disease. Radiation dose reduction strategies are also discussed to show how CT angiography can be performed in a low-dose protocol in the current clinical practice.CT angiography represents the most important development in CT imaging, and it has evolved from the initial role of serving as a supplementary modality to an essential tool that plays an important role in the diagnosis and management of cardiovascular disease which involves arterial system in the body. Technological advancements in CT data acquisition and image processing techniques have enabled this technique to become a routine imaging modality in daily clinical practice. With emergence of novel CT scanner geometries, advanced data reconstruction and postprocessing techniques, CT angiography will continue to play a dominant role in the diagnosis of cardiovascular disease, prediction of disease extent and assistance of clinicians in effective patient management.', 'kwd': u'Cardiovascular disease, computed tomography angiography (CTA), diagnosis, visualisation', 'title': u'CT angiography in the diagnosis of cardiovascular disease: a transformation in cardiovascular CT practice'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5594598/', 'p': u'The use of coronary MR angiography (CMRA) in patients with coronary artery disease (CAD) remains limited due to the long scan times, unpredictable and often non-diagnostic image quality secondary to respiratory motion artifacts. The purpose of this study was to evaluate CMRA with image-based respiratory navigation (iNAV CMRA) and compare it to gold standard invasive x-ray coronary angiography in patients with CAD.Consecutive patients referred for CMR assessment were included to undergo iNAV CMRA on a 1.5\xa0T scanner. Coronary vessel sharpness and a visual score were assigned to the coronary arteries. A diagnostic reading was performed on the iNAV CMRA data, where a lumen narrowing >50% was considered diseased. This was compared to invasive x-ray findings.', 'kwd': u'Coronary MR angiography, Image navigators, Respiratory motion correction, Coronary artery disease', 'title': u'Diagnostic performance of image navigated coronary CMR angiography in patients with coronary artery disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422837/', 'p': u'During the last decade coronary computed tomography angiography (CTA) has become the preeminent non-invasive imaging modality to detect coronary artery disease (CAD) with high accuracy. However, CTA has a limited value in assessing the hemodynamic significance of a given stenosis due to a modest specificity and positive predictive value. In recent years, different CT techniques for detecting myocardial ischemia have emerged, such as CT-derived fractional flow reserve (FFR-CT), transluminal attenuation gradient (TAG), and myocardial CT perfusion (CTP) imaging. Myocardial CTP imaging can be performed with a single static scan during first pass of the contrast agent, with monoenergetic or dual-energy acquisition, or as a dynamic, time-resolved scan during stress by using coronary vasodilator agents (adenosine, dipyridamole, or regadenoson). A number of CTP techniques are available, which can assess myocardial perfusion in both a qualitative, semi-quantitative or quantitative manner. Once used primarily as research tools, these modalities are increasingly being used in routine clinical practice. All these techniques offer the substantial advantage of combining anatomical and functional evaluation of flow-limiting coronary stenosis in the same examination that would be beneficial for clinical decision-making. This review focuses on the state-of the-art and future trends of these evolving imaging modalities in the field of cardiology for the physiologic assessments of CAD.The FFR-CT method allows the extraction of \u2018stress induced\u2019 quantitative functional information from an anatomic CTA of at least moderate quality acquired at rest without adenosine infusion. The method uses computational fluid dynamics with simulated hyperemia to calculate the FFR measurement at any point in the vascular tree (39). The concept of coronary FFR, defined as the ratio of the mean coronary pressure distal to a coronary stenosis to the mean aortic pressure during maximal coronary blood flow, has evolved into an accepted functional measure of stenosis severity since first proposed 15 years ago (22). FFR has now become the invasive gold standard for assessing lesion-specific ischemia. A FFR value less than 0.80 or less than 0.75 identifies hemodynamic significance of coronary stenosis (7,8). FFR-CT correlates well with invasive-derived FFR measurements in patients with suspected or known CAD (17,39,40). One of the temporary drawbacks is that FFR-CT needs extreme computational ability and analysis time, which hampers widespread dissemination.', 'kwd': u'Cardiac computed tomography, coronary artery disease (CAD), stress imaging, myocardial perfusion imaging, myocardial blood flow quantification', 'title': u'Myocardial blood flow quantification for evaluation of coronary artery disease by computed tomography'}], 'Risk Stratification AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1479368/', 'p': u'In recent years a number of algorithms for cardiovascular risk assessment has been proposed to the medical community. These algorithms consider a number of variables and express their results as the percentage risk of developing a major fatal or non-fatal cardiovascular event in the following 10 to 20 yearsThe use of predictive algorithms to assess individual absolute risk of cardiovascular future events is currently hampered by methodological and mathematical flaws. The use of newer approaches, such as fuzzy logic and artificial neural networks, linked to artificial intelligence, seems to better address both the challenge of increasing complexity resulting from a correlation between predisposing factors, data on the occurrence of cardiovascular events, and the prediction of future events on an individual level.', 'kwd': '-', 'title': u'How artificial intelligence tools can be used to assess individual patient risk in cardiovascular disease: problems with the current methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3818807/', 'p': u'Recently, the greatest statistical computational challenge in genetic epidemiology is to identify and characterize the genes that interact with other genes and environment factors that bring the effect on complex multifactorial disease. These gene-gene interactions are also denoted as epitasis in which this phenomenon cannot be solved by traditional statistical method due to the high dimensionality of the data and the occurrence of multiple polymorphism. Hence, there are several machine learning methods to solve such problems by identifying such susceptibility gene which are neural networks (NNs), support vector machine (SVM), and random forests (RFs) in such common and multifactorial disease. This paper gives an overview on machine learning methods, describing the methodology of each machine learning methods and its application in detecting gene-gene and gene-environment interactions. Lastly, this paper discussed each machine learning method and presents the strengths and weaknesses of each machine learning method in detecting gene-gene interactions in complex human disease.All trees of RF are frown to their full extent without pruning because each tree of a RF is grown using random feature selection to select a training set (bootstrap sample) from the original data [31]. Based on [32], a classifiers decision tree of RF is grown as follows.', 'kwd': '-', 'title': u'A Review for Detecting Gene-Gene Interactions Using Machine Learning Methods in Genetic Epidemiology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5391398/', 'p': u'In 2016, 13 topics were selected as major research advances in gynecologic oncology. For ovarian cancer, study results supporting previous ones regarding surgical preventive strategies were reported. There were several targeted agents that showed comparable responses in phase III trials, including niraparib, cediranib, and nintedanib. On the contrary to our expectations, dose-dense weekly chemotherapy regimen failed to prove superior survival outcomes compared with conventional triweekly regimen. Single-agent non-platinum treatment to prolong platinum-free-interval in patients with recurrent, partially platinum-sensitive ovarian cancer did not improve and even worsened overall survival (OS). For cervical cancer, we reviewed robust evidences of larger-scaled population-based study and cost-effectiveness of nonavalent vaccine for expanding human papillomavirus (HPV) vaccine coverage. Standard of care treatment of locally advanced cervical cancer (LACC) was briefly reviewed. For uterine corpus cancer, new findings about appropriate surgical wait time from diagnosis to surgery were reported. Advantages of minimally invasive surgery over conventional laparotomy were reconfirmed. There were 5 new gene regions that increase the risk of developing endometrial cancer. Regarding radiation therapy, Post-Operative Radiation Therapy in Endometrial Cancer (PORTEC)-3 quality of life (QOL) data were released and higher local control rate of image-guided adaptive brachytherapy was reported in LACC. In addition, 4 general oncology topics followed: chemotherapy at the end-of-life, immunotherapy with reengineering T-cells, actualization of precision medicine, and artificial intelligence (AI) to make personalized cancer therapy real. For breast cancer, adaptively randomized trials, extending aromatase inhibitor therapy, and ribociclib and palbociclib were introduced.Five new gene regions that increase the risk of developing endometrial cancer were identified by a meta-analysis of 3 endometrial cancer genome-wide association study (GWAS) [50]: previous GWAS from 2 population studies (the UK Studies of Epidemiology and Risk factors in Cancer Heredity [SEARCH, n=681] and the Australian National Endometrial Cancer Study [ANECS, n=606]) and genotypes generated using Illumina Infinium 610K arrays, the National Study of Endometrial Cancer (NSECG), and the Collaborative Oncological Gene-environment Study (COGS) initiative. In this study, a total of 7,737 endometrial cancer cases and 37,144 controls without cancer of European ancestry were investigated. Five novel risk loci included likely regulatory regions on chromosomes 13q22.1, 6q22.31, 8q24.21, 15q15.1, and 14q32.33. Those 5 novel regions contained at least one endometrial cancer risk single nucleotide polymorphism (SNP) with Pmeta<10\u22127 and most strongly associated SNP in each region was genotyped: rs11841589 (OR=1.15; 95% CI=1.11\u20131.21; p=4.83\xd710\u221211), rs13328298 (OR=1.13; 95% CI=1.09\u20131.18; p=3.73\xd710\u221210), rs4733613 (OR=0.84; 95% CI=0.80\u20130.89; p=3.09\xd710\u22129), rs937213 (OR=0.90; 95% CI=0.86\u20130.93; p=1.77\xd710\u22128), and rs2498796 (OR=0.89; 95% CI=0.85\u20130.93; p=3.55\xd710\u22128), respectively. All the 5 SNPs were associated with endometrial cancer at genome-wide significance (p<5\xd710\u22128). Specifically, functional studies of the 13q22.1 locus showed that rs9600103 is located in a region of active chromatin that interacts with promoter region of the Kruppel-like factor 5 (KLF5) (pairwise r2=0.98 with rs11841589). KLF5, a transcription factor associated with cell cycle regulation, is thought to be active during the development of the uterus as well as tumorigenesis. Given in vitro suppression of gene expression by rs9600103-T endometrial cancer protective allele in allele-specific luciferase reporter assays using Ishikawa cells, regulation of KLF5 expression could be implicated in tumorigenesis of endometrial cancer.', 'kwd': u'Precision Medicine, Artificial Intelligence, Genital Neoplasms, Female, Ovarian Neoplasms, Breast Neoplasms', 'title': u'Major clinical research advances in gynecologic cancer in 2016: 10-year special edition'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5653644/', 'p': '-', 'kwd': '-', 'title': u'Metabolomics for the masses: The future of metabolomics in a personalized world'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4153716/', 'p': u'Conceived and designed the experiments: MD M. Rammerstorfer TP HB M. Ramharter. Performed the experiments: TP AB AM FL. Analyzed the data: FR GD MD HB M. Ramharter. Contributed reagents/materials/analysis tools: MD M. Rammerstorfer GD AB AM TP. Contributed to the writing of the manuscript: FR M. Ramharter FL GD TP HB.Bacteraemia is a frequent and severe condition with a high mortality rate. Despite profound knowledge about the pre-test probability of bacteraemia, blood culture analysis often results in low rates of pathogen detection and therefore increasing diagnostic costs. To improve the cost-effectiveness of blood culture sampling, we computed a risk prediction model based on highly standardizable variables, with the ultimate goal to identify via an automated decision support tool patients with very low risk for bacteraemia.', 'kwd': '-', 'title': u'A Risk Prediction Model for Screening Bacteremic Patients: A Cross Sectional Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4738045/', 'p': u'The aim of this study is to present an objective method based on support vector machines (SVMs) and gravitational search algorithm (GSA) which is initially utilized for recognition the pattern among risk factors and hypertension (HTN) to stratify and analysis HTN\u2019s risk factors in an Iranian urban population.This community-based and cross-sectional research has been designed based on the probabilistic sample of residents of Isfahan, Iran, aged 19 years or over from 2001 to 2007. One of the household members was randomly selected from different age groups. Selected individuals were invited to a predefined health center to be educated on how to collect 24-hour urine sample as well as learning about topographic parameters and blood pressure measurement. The data from both the estimated and measured blood pressure [for both systolic blood pressure (SBP) and diastolic blood pressure (DBP)] demonstrated that optimized SVMs have a highest estimation potential.', 'kwd': u'Support Vector Machines, Gravitational Search Algorithm, High Blood Pressure', 'title': u'Advanced method used for hypertension\u2019s risk factors strati\ufb01cation: support vector machines and gravitational search algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422742/', 'p': u'The management of thyroid nodules, one of the main clinical challenges in endocrine clinical practice, is usually straightforward. Although the most important concern is ruling out malignancy, there are grey areas where uncertainty is frequently present: the nodules labelled as indeterminate by cytology and the extent of therapy when thyroid cancer is diagnosed pathologically. There is evidence that the current available precision medicine tools (from all the \u201c-omics\u201d to molecular analysis, fine-tuning imaging or artificial intelligence) may help to fill present gaps in the future. We present here a commentary on some of the current challenges faced by endocrinologists in the field of thyroid nodules and cancer, and illustrate how precision medicine may improve their diagnostic and therapeutic capabilities in the future.Several immunocytochemical markers have been proposed to differentiate benign from malignant nodules in fine-needle aspiration samples [6]. Some of them are listed in Table \u200bTable1.1. Currently no immunomarker has demonstrated enough diagnostic accuracy to be used alone. However, the combined analysis of galectin-3 and Hector Battifora mesothelial-1 has shown acceptable sensitivity and specificity to identify malignant tumours [7]. In this regard, other promising markers are being studied, such as CD44 [8] or Ki-67 [9].', 'kwd': u'Precision medicine, Thyroid cancer, Thyroid nodule, Differentiated thyroid cancer, Medullary thyroid cancer', 'title': u'Nodular Thyroid Disease and Thyroid Cancer in the Era of Precision Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4298653/', 'p': u'Mortality prediction models for patients with perforated peptic ulcer (PPU) have not yielded consistent or highly accurate results. Given the complex nature of this disease, which has many non-linear associations with outcomes, we explored artificial neural networks (ANNs) to predict the complex interactions between the risk factors of PPU and death among patients with this condition.ANN modelling using a standard feed-forward, back-propagation neural network with three layers (i.e., an input layer, a hidden layer and an output layer) was used to predict the 30-day mortality of consecutive patients from a population-based cohort undergoing surgery for PPU. A receiver-operating characteristic (ROC) analysis was used to assess model accuracy.', 'kwd': u'Peptic ulcer perforation, Gastroduodenal ulcers, Mortality, Prediction, Prognosis, Outcome assessment, Computer simulation', 'title': u'Predicting outcomes in patients with perforated gastroduodenal ulcers: artificial neural network modelling indicates a highly complex disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520404/', 'p': '-', 'kwd': u'Electronic health records, Temporal analysis, Progression of kidney function loss, Risk stratification', 'title': u'Incorporating temporal EHR data in predictive models for risk stratification of renal function deterioration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3706990/', 'p': u"Tumor heterogeneity is a limiting factor in cancer treatment and in the discovery of biomarkers to personalize it. We describe a computational purification tool, ISOpure, which directly addresses the effects of variable contamination by normal tissue in clinical tumor specimens. ISOpure uses a set of tumor expression profiles and a panel of healthy tissue expression profiles to generate a purified cancer profile for each tumor sample, and an estimate of the proportion of RNA originating from cancerous cells. Applying ISOpure before identifying gene signatures leads to significant improvements in the prediction of prognosis and other clinical variables in lung and prostate cancer.Our regularization strategy incorporates the Dirichlet probability density function into our scoring functions. This choice allows us to use the statistical inference method described below to estimate the parameter values. The Dirichlet distribution is a continuous multivariate distribution over discrete probability distributions (that is, vectors of pre-determined size that contain non-negative elements that sum to one). We use the Dirichlet for both \u03b8n and cn because they are both discrete probability distributions. The probability density function associated with the Dirichlet has two parameters (termed hyper-parameters because they are the parameters of distributions over model parameters): a mean vector (which determines the mean of the Dirichlet distribution) and a scalar strength parameter that controls how quickly the score decreases from the mode of the Dirichlet distribution. We also estimate the following hyper-parameters from the tumor data: \xa0\u03bd, kn (for n = 1 to N), k', and \u03c9. These additional parameters are formally defined below in the statistical model provided in equations (3 to 9), but in brief \xa0\u03bd represents both the mean and strength of a Dirichlet distribution over \u03b8n; kn represents the strength parameter of the Dirichlet distribution over cn given m; k' represents the strength parameter of the Dirichlet distribution over m; \u03c9 represents the weights on the normal profiles br used to make the weighted combination that forms the mean parameter vector for the Dirichlet distribution over m.", 'kwd': '-', 'title': u'Computational purification of individual tumor gene expression profiles leads to significant improvements in prognostic prediction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4747508/', 'p': u'Conceived and designed the experiments: TKY DWK. Analyzed the data: TKY SBC EO. Contributed reagents/materials/analysis tools: EO. Wrote the paper: TKY SBC JSP DWK.Knee osteoarthritis (OA) is the most common joint disease of adults worldwide. Since the treatments for advanced radiographic knee OA are limited, clinicians face a significant challenge of identifying patients who are at high risk of OA in a timely and appropriate way. Therefore, we developed a simple self-assessment scoring system and an improved artificial neural network (ANN) model for knee OA.', 'kwd': '-', 'title': u'Simple Scoring System and Artificial Neural Network for Knee Osteoarthritis Risk Prediction: A Cross-Sectional Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4309789/', 'p': u'MicroRNAs (miRNAs) are small, noncoding RNA species with a length of 20\u201322 nucleotides that are recognized as essential regulators of relevant molecular mechanisms, including carcinogenesis. Current investigations show that miRNAs are detectable not only in different tissue types but also in a wide range of biological fluids, either free or trapped in circulating microvesicles. miRNAs were proven to be involved in cell communication, both in pathological and physiological processes. Evaluation of the global expression patterns of miRNAs provides key opportunities with important practical applications, taking into account that they modulate essential biological processes such as epithelial to mesenchymal transition, which is a mechanism relevant in bladder cancer. miRNAs collected from biological specimens can furnish valuable evidence with regard to bladder cancer oncogenesis, as they also have been linked to clinical outcomes in urothelial carcinoma. Therefore, a single miRNA or a signature of multiple miRNAs may improve risk stratification of patients and may supplement the histological diagnosis of urological tumors, particularly for bladder cancer.The challenges that clinicians face when caring for patients with bladder cancer are the difficulties of early diagnosis, disease recurrence, and progression. Current prognostic strategies, such as tumor grade, stage, size, and number of foci, have restricted utility for clinicians because they do not specifically exhibit the clinical outcomes.6 Diagnosis and monitoring strategies for bladder cancer have been based on the integration of cystoscopy and urinary cytology data.7 As a consequence, researchers have been searching for novel biomarkers, and an important research direction was focusing on the role of microRNAs (miRNAs) in the development of bladder cancer (Figure 2).', 'kwd': u'bladder cancer, miRNA, prognostic, diagnostic', 'title': u'Clinical and pathological implications of miRNA in bladder cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3046173/', 'p': u'Conceived and designed the experiments: JFR ELE. Performed the experiments: JFR ELE PCE WAR. Analyzed the data: JFR ELE. Contributed reagents/materials/analysis tools: DFR. Wrote the paper: JFR DFR. Performed the feature extraction methods: JRV. Gave important feedback on statistical procedures: ELE. Made the PCA-based feature selection and classification experiments: JRV. Gave important feedback and medical validation of the reported results: WAR. Gave important scientific background: DFR. Suggested suitable classification schemes and architectures to overcome the classification problems: DFR. Gave important feedback for the enhancement of the paper: DFR PCE.Statistical, spectral, multi-resolution and non-linear methods were applied to heart rate variability (HRV) series linked with classification schemes for the prognosis of cardiovascular risk. A total of 90 HRV records were analyzed: 45 from healthy subjects and 45 from cardiovascular risk patients. A total of 52 features from all the analysis methods were evaluated using standard two-sample Kolmogorov-Smirnov test (KS-test). The results of the statistical procedure provided input to multi-layer perceptron (MLP) neural networks, radial basis function (RBF) neural networks and support vector machines (SVM) for data classification. These schemes showed high performances with both training and test sets and many combinations of features (with a maximum accuracy of 96.67%). Additionally, there was a strong consideration for breathing frequency as a relevant feature in the HRV analysis.', 'kwd': '-', 'title': u'Heart Rate Variability Dynamics for the Prognosis of Cardiovascular Risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540429/', 'p': u'We propose a novel approach for ICU patient risk stratification by combining the learned \u201ctopic\u201d structure of clinical concepts (represented by UMLS codes) extracted from the unstructured nursing notes with physiologic data (from SAPS-I) for hospital mortality prediction. We used Hierarchical Dirichlet Processes (HDP), a non-parametric topic modeling technique, to automatically discover \u201ctopics\u201d as shared groups of co-occurring UMLS clinical concepts. We evaluated the potential utility of the inferred topic structure in predicting hospital mortality using the nursing notes of 14,739 adult ICU patients (mortality 14.6%) from the MIMIC II database. Our results indicate that learned topic structure from the first 24-hour ICU nursing notes significantly improved the performance of the SAPS-I algorithm for hospital mortality prediction. The AUC for predicting hospital mortality from the first 24 hours of physiologic data and nursing text notes was 0.82. Using the physiologic data alone with the SAPS-I algorithm, an AUC of 0.72 was achieved. Thus, the clinical topics that were extracted and used to augment the SAPS-I algorithm significantly improved the performance of the baseline algorithm.Nursing notes from the first 24 hour ICU stay of each adult patient in MIMIC II (version 2.5) were extracted. Patients whose SAPS-1 [9] score could not be determined due to missing data were excluded, as were patients whose lengths-of-stay were less than 24 hours. For patients with multiple ICU stays or multiple hospital stays, notes from the first ICU stay of the first hospital stay were used.', 'kwd': '-', 'title': u'Risk Stratification of ICU Patients Using Topic Models Inferred from Unstructured Progress Notes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4303550/', 'p': u'Multivariate pattern analysis (MVPA) methods have become an important tool in neuroimaging, revealing complex associations and yielding powerful prediction models. Despite methodological developments and novel application domains, there has been little effort to compile benchmark results that researchers can reference and compare against. This study takes a significant step in this direction. We employed three classes of state-of-the-art MVPA algorithms and common types of structural measurements from brain Magnetic Resonance Imaging (MRI) scans to predict an array of clinically relevant variables (diagnosis of Alzheimer\u2019s, schizophrenia, autism, and attention deficit and hyperactivity disorder; age, cerebrospinal fluid derived amyloid\u2013\u03b2 levels and mini-mental state exam score). We analyzed data from over 2,800 subjects, compiled from six publicly available datasets. The employed data and computational tools are freely distributed (https://www.nmr.mgh.harvard.edu/lab/mripredict), making this the largest, most comprehensive, reproducible benchmark image-based prediction experiment to date in structural neuroimaging. Finally, we make several observations regarding the factors that influence prediction performance and point to future research directions. Unsurprisingly, our results suggest that the biological footprint (effect size) has a dramatic influence on prediction performance. Though the choice of image measurement and MVPA algorithm can impact the result, there was no universally optimal selection. Intriguingly, the choice of algorithm seemed to be less critical than the choice of measurement type. Finally, our results showed that cross-validation estimates of performance, while generally optimistic, correlate well with generalization accuracy on a new dataset.We conducted a mass-univariate analysis to map regions where cortical thickness is associated with clinical variables of interest. For this analysis, we used the thickness values sampled onto the highest resolution template, fsaverage, which contains over 140k vertices on each hemisphere, and smoothed on the cortical surface with a Gaussian-like filter of a 10 mm FWHM. We then applied a general linear model at each vertex, where the outcome was thickness and the independent variables were age, gender and the clinical variable. The p-value associated with the clinical variables was then saved for each vertex (see Fig. 3). When identifying cortical areas of significant associations, we applied the false discovery rate (Benjamini and Hochberg, 1995) (FDR, q = 0.05) to correct for multiple comparisons. The total area of significant associations was then computed as the sum of the areas corresponding to the significant vertices in fsaverage.', 'kwd': u'Image-based prediction, Computer aided diagnosis, machine learning, MRI', 'title': u'Clinical prediction from structural brain MRI scans: A large-scale empirical study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4096359/', 'p': u'Using a reduced subset of SNPs in a linear mixed model can improve power for genome-wide association studies, yet this can result in insufficient correction for population stratification. We propose a hybrid approach using principal components that does not inflate statistics in the presence of population stratification and improves power over standard linear mixed models.We analyzed data from 10,204 MS cases and 5429 controls [the National Blood Service (NBS) and the 1958 Birth Cohort (1958BC)] genotyped on Illumina arrays made available to researchers via WTCCC2 (http://wtccc.org.uk/ccc2/). We follow the quality-control standards in Yang et al. (2014). Although Sawcer et al. (2011) analyzed United Kingdom (UK) and non-UK samples separately followed by meta-analysis in most of their analyses, the data made available to researchers include both UK and non-UK cases but only UK controls. We retained all samples to maximize sample size. We considered markers that were present in each of MS, NBS, and 1958BC data sets and removed markers with >0.5% missing data, P < 0.01 for allele-frequency difference between NBS and 1958BC, P < 0.05 for deviation from Hardy\u2013Weinberg equilibrium, P < 0.05 for differential missingness between cases and controls, or minor allele frequency <0.1% in any data set, leaving 360,557 markers. The 75 known associated markers were defined by including, for each MS-associated marker listed in the National Human Genome Research Institute (NHGRI) GWAS catalog (http://genome.gov/gwastudies/), a single best tag at r2 > 0.4 from the set of 360,557 markers if available.', 'kwd': u'mixed models, population stratification, GWAS', 'title': u'Improving the Power of GWAS and Avoiding Confounding from Population Stratification with PC-Select'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4994706/', 'p': u'The proliferation of large genomic databases offers the potential to perform increasingly larger-scale genome-wide association studies (GWAS). Due to privacy concerns, however, access to these data is limited, greatly reducing their usefulness for research. Here, we introduce a computational framework for performing GWAS that adapts principles of differential privacy\u2014a cryptographic theory that facilitates secure analysis of sensitive data\u2014to, for the first time, both protect private phenotype information (e.g., disease status) and correct for population stratification. This framework enables us to produce privacy-preserving GWAS results based on EIGENSTRAT and linear mixed model (LMM)-based statistics, both of which correct for population stratification. We test our differentially private statistics, PrivSTRAT and PrivLMM, on simulated and real GWAS datasets and find they are able to protect privacy while returning meaningful results. Our framework can be used to securely query private genomic datasets to discover which specific genomic alterations may be associated with a disease, thus increasing the availability of these valuable datasets.\n', 'kwd': '-', 'title': u'Enabling Privacy-Preserving GWAS in Heterogeneous Human Populations'}], 'Risk Score AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3768292/', 'p': u'Celiac disease (CD) is a difficult-to-diagnose condition because of its multiple clinical presentations and symptoms shared with other diseases. Gold-standard diagnostic confirmation of suspected CD is achieved by biopsying the small intestine.To develop a clinical decision\u2013support system (CDSS) integrated with an automated classifier to recognize CD cases, by selecting from experimental models developed using intelligence artificial techniques.', 'kwd': u'Decision support systems, clinical Celiac disease, Artificial intelligence', 'title': u'Artificial intelligence techniques applied to the development of a decision\u2013support system for diagnosing celiac disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1479368/', 'p': u'In recent years a number of algorithms for cardiovascular risk assessment has been proposed to the medical community. These algorithms consider a number of variables and express their results as the percentage risk of developing a major fatal or non-fatal cardiovascular event in the following 10 to 20 yearsThe use of predictive algorithms to assess individual absolute risk of cardiovascular future events is currently hampered by methodological and mathematical flaws. The use of newer approaches, such as fuzzy logic and artificial neural networks, linked to artificial intelligence, seems to better address both the challenge of increasing complexity resulting from a correlation between predisposing factors, data on the occurrence of cardiovascular events, and the prediction of future events on an individual level.', 'kwd': '-', 'title': u'How artificial intelligence tools can be used to assess individual patient risk in cardiovascular disease: problems with the current methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689880/', 'p': u"This paper introduces Lynx, an intelligent system for personal safety at home environments, oriented to elderly people living independently, which encompasses a decision support machine for automatic home risk prevention, tested in real-life environments to respond to real time situations. The automatic system described in this paper prevents such risks by an advanced analytic methods supported by an expert knowledge system. It is minimally intrusive, using plug-and-play sensors and machine learning algorithms to learn the elder's daily activity taking into account even his health records. If the system detects that something unusual happens (in a wide sense) or if something is wrong relative to the user's health habits or medical recommendations, it sends at real-time alarm to the family, care center, or medical agents, without human intervention. The system feeds on information from sensors deployed in the home and knowledge of subject physical activities, which can be collected by mobile applications and enriched by personalized health information from clinical reports encoded in the system. The system usability and reliability have been tested in real-life conditions, with an accuracy larger than 81%. The ultimate goal of automatic summaries is to feed the telecare platform with the most relevant clinical data obtained by an unassisted way from medical summaries and move the therapeutics procedures, treatments, or medical recommendations from medical summaries to universAAL ontology. So, the anomaly engine and the predicting intentions engine are capable of learning about personal living habits of the patients, highly correlated with their clinical conditions and prescriptions. To build this integration, thanks to semantic annotation process, we need only to join the most relevant concepts (principal diseases, diagnosis, treatment, and procedures) in the control platform (see Figure 9) with a \u201csame_as\u201d link between ontologies, by the paradigm of Linked Data recommendations [39].", 'kwd': '-', 'title': u'Lynx: Automatic Elderly Behavior Prediction in Home Telecare'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2646316/', 'p': u'The goal of personalised medicine in the intensive care unit (ICU) is to predict which diagnostic tests, monitoring interventions and treatments translate to improved outcomes given the variation between patients. Unfortunately, processes such as gene transcription and drug metabolism are dynamic in the critically ill; that is, information obtained during static non-diseased conditions may have limited applicability. We propose an alternative way of personalising medicine in the ICU on a real-time basis using information derived from the application of artificial intelligence on a high-resolution database. Calculation of maintenance fluid requirement at the height of systemic inflammatory response was selected to investigate the feasibility of this approach.The Multi-parameter Intelligent Monitoring for Intensive Care II (MIMIC II) is a database of patients admitted to the Beth Israel Deaconess Medical Center ICU in Boston. Patients who were on vasopressors for more than six hours during the first 24 hours of admission were identified from the database. Demographic and physiological variables that might affect fluid requirement or reflect the intravascular volume during the first 24 hours in the ICU were extracted from the database. The outcome to be predicted is the total amount of fluid given during the second 24 hours in the ICU, including all the fluid boluses administered.', 'kwd': '-', 'title': u'An artificial intelligence tool to predict fluid requirement in the intensive care unit: a proof-of-concept study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5694620/', 'p': u'To reduce unnecessary lab testing by predicting when a proposed future lab test is likely to contribute information gain and thereby influence clinical management in patients with gastrointestinal bleeding. Recent studies have demonstrated that frequent laboratory testing does not necessarily relate to better outcomes.Data preprocessing, feature selection, and classification were performed and an artificial intelligence tool, fuzzy modeling, was used to identify lab tests that do not contribute an information gain. There were 11 input variables in total. Ten of these were derived from bedside monitor trends heart rate, oxygen saturation, respiratory rate, temperature, blood pressure, and urine collections, as well as infusion products and transfusions. The final input variable was a previous value from one of the eight lab tests being predicted: calcium, PTT, hematocrit, fibrinogen, lactate, platelets, INR and hemoglobin. The outcome for each test was a binary framework defining whether a test result contributed information gain or not.', 'kwd': u'Phlebotomy, Harm reduction, Blood transfusions, Non-linear models, Predictive value of tests, False positive reactions', 'title': u'Reducing unnecessary lab testing in the ICU with artificial intelligence'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3896442/', 'p': u'Conceived and designed the experiments: JG LMN PPG. Performed the experiments: JG LMN. Analyzed the data: JG ML PPG. Contributed reagents/materials/analysis tools: LMN. Wrote the paper: JG ML JF PPG.Plant acclimation is a highly complex process, which cannot be fully understood by analysis at any one specific level (i.e. subcellular, cellular or whole plant scale). Various soft-computing techniques, such as neural networks or fuzzy logic, were designed to analyze complex multivariate data sets and might be used to model large such multiscale data sets in plant biology.', 'kwd': '-', 'title': u'Modeling the Effects of Light and Sucrose on In Vitro Propagated Plants: A Multiscale System Analysis Using Artificial Intelligence Technology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4267836/', 'p': u'Conceived and designed the experiments: DE AW LJ CC TM. Performed the experiments: DE LJ. Analyzed the data: DE AW LJ CC TM. Contributed to the writing of the manuscript: DE AW CC TM.Recent research with face-to-face groups found that a measure of general group effectiveness (called \u201ccollective intelligence\u201d) predicted a group\u2019s performance on a wide range of different tasks. The same research also found that collective intelligence was correlated with the individual group members\u2019 ability to reason about the mental states of others (an ability called \u201cTheory of Mind\u201d or \u201cToM\u201d). Since ToM was measured in this work by a test that requires participants to \u201cread\u201d the mental states of others from looking at their eyes (the \u201cReading the Mind in the Eyes\u201d test), it is uncertain whether the same results would emerge in online groups where these visual cues are not available. Here we find that: (1) a collective intelligence factor characterizes group performance approximately as well for online groups as for face-to-face groups; and (2) surprisingly, the ToM measure is equally predictive of collective intelligence in both face-to-face and online groups, even though the online groups communicate only via text and never see each other at all. This provides strong evidence that ToM abilities are just as important to group performance in online environments with limited nonverbal cues as they are face-to-face. It also suggests that the Reading the Mind in the Eyes test measures a deeper, domain-independent aspect of social reasoning, not merely the ability to recognize facial expressions of mental states.', 'kwd': '-', 'title': u'Reading the Mind in the Eyes or Reading between the Lines? Theory of Mind Predicts Collective Intelligence Equally Well Online and Face-To-Face'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4523666/', 'p': u'Automatic diagnosis of the Sleep Apnea-Hypopnea Syndrome (SAHS) has become an important area of research due to the growing interest in the field of sleep medicine and the costs associated with its manual diagnosis. The increment and heterogeneity of the different techniques, however, make it somewhat difficult to adequately follow the recent developments. A literature review within the area of computer-assisted diagnosis of SAHS has been performed comprising the last 15 years of research in the field. Screening approaches, methods for the detection and classification of respiratory events, comprehensive diagnostic systems, and an outline of current commercial approaches are reviewed. An overview of the different methods is presented together with validation analysis and critical discussion of the current state of the art.The authors declare that there is no conflict of interests regarding the publication of this paper.', 'kwd': '-', 'title': u'Computer-Assisted Diagnosis of the Sleep Apnea-Hypopnea Syndrome: A Review'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1964229/', 'p': '-', 'kwd': '-', 'title': u'Artificial intelligence in medicine.'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4732130/', 'p': u'Surveying threatened and invasive species to obtain accurate population estimates is an important but challenging task that requires a considerable investment in time and resources. Estimates using existing ground-based monitoring techniques, such as camera traps and surveys performed on foot, are known to be resource intensive, potentially inaccurate and imprecise, and difficult to validate. Recent developments in unmanned aerial vehicles (UAV), artificial intelligence and miniaturized thermal imaging systems represent a new opportunity for wildlife experts to inexpensively survey relatively large areas. The system presented in this paper includes thermal image acquisition as well as a video processing pipeline to perform object detection, classification and tracking of wildlife in forest or open areas. The system is tested on thermal video data from ground based and test flight footage, and is found to be able to detect all the target wildlife located in the surveyed area. The system is flexible in that the user can readily define the types of objects to classify and the object characteristics that should be considered during classification.The UAV uses a 16,000 mAh Lipo 6 cell battery. This provides a maximum hover time of approximately 20 mins with no sensor payload. The maximum motor power consumption of each motor is 500 W operating at 400 rpm/V. These are running in conjunction with 15 \xd7 5.2 inch propellers.', 'kwd': u'Unmanned Aerial Vehicle (UAV), wildlife monitoring, artificial intelligence, thermal imaging, robotics, conservation, automatic classification, koala, deer, wild pigs, dingo, conservation', 'title': u'Unmanned Aerial Vehicles (UAVs) and Artificial Intelligence Revolutionizing Wildlife Monitoring and Conservation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4856067/', 'p': u'Cognitive behavioral therapy (CBT) is one of the most effective treatments for chronic low back pain. However, only half of Department of Veterans Affairs (VA) patients have access to trained CBT therapists, and program expansion is costly. CBT typically consists of 10 weekly hour-long sessions. However, some patients improve after the first few sessions while others need more extensive contact.We are applying principles from \u201creinforcement learning\u201d (a field of artificial intelligence or AI) to develop an evidence-based, personalized CBT pain management service that automatically adapts to each patient\u2019s unique and changing needs (AI-CBT). AI-CBT uses feedback from patients about their progress in pain-related functioning measured daily via pedometer step counts to automatically personalize the intensity and type of patient support. The specific aims of the study are to (1) demonstrate that AI-CBT has pain-related outcomes equivalent to standard telephone CBT, (2) document that AI-CBT achieves these outcomes with more efficient use of clinician resources, and (3) demonstrate the intervention\u2019s impact on proximal outcomes associated with treatment response, including program engagement, pain management skill acquisition, and patients\u2019 likelihood of dropout.', 'kwd': u'Medical Informatics, mhealth, artificial intelligence, comparative effectiveness', 'title': u'Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: Protocol for a Randomized Study Funded by the US Department of Veterans Affairs Health Services Research and Development Program'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864841/', 'p': u"Conceived and designed the experiments: EAK MF. Performed the experiments: EAK SKK AS. Analyzed the data: EAK SKK SS AS HW MMK MT. Contributed reagents/materials/analysis tools: EAK SKK SS AS HW MMK MT. Wrote the paper: EAK SKK SS AS HW MMK MT MF. Contributed to the concept of the article: EAK SKK SS MF. Obtained permission for the studies from ethics committee: EAK SKK SS. Critically revised the article for important intellectual content: EAK MF.The ability of today's robots to autonomously support humans in their daily activities is still limited. To improve this, predictive human-machine interfaces (HMIs) can be applied to better support future interaction between human and machine. To infer upcoming context-based behavior relevant brain states of the human have to be detected. This is achieved by brain reading (BR), a passive approach for single trial EEG analysis that makes use of supervised machine learning (ML) methods. In this work we propose that BR is able to detect concrete states of the interacting human. To support this, we show that BR detects patterns in the electroencephalogram (EEG) that can be related to event-related activity in the EEG like the P300, which are indicators of concrete states or brain processes like target recognition processes. Further, we improve the robustness and applicability of BR in application-oriented scenarios by identifying and combining most relevant training data for single trial classification and by applying classifier transfer. We show that training and testing, i.e., application of the classifier, can be carried out on different classes, if the samples of both classes miss a relevant pattern. Classifier transfer is important for the usage of BR in application scenarios, where only small amounts of training examples are available. Finally, we demonstrate a dual BR application in an experimental setup that requires similar behavior as performed during the teleoperation of a robotic arm. Here, target recognition processes and movement preparation processes are detected simultaneously. In summary, our findings contribute to the development of robust and stable predictive HMIs that enable the simultaneous support of different interaction behaviors.", 'kwd': '-', 'title': u'On the Applicability of Brain Reading for Predictive Human-Machine Interfaces in Robotics'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4232483/', 'p': u'Many healthy women consider genetic testing for breast cancer risk, yet BRCA testing issues are complex.Determining whether an intelligent tutor, BRCA Gist, grounded in fuzzy-trace theory (FTT), increases gist comprehension and knowledge about genetic testing for breast cancer risk, improving decision-making.', 'kwd': '-', 'title': u'Efficacy of a Web-based Intelligent Tutoring System for Communicating Genetic Risk of Breast Cancer: A Fuzzy-Trace Theory Approach'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5454064/', 'p': u"All authors materially participated in the research and/or article preparation and all authors have approved the final article. SD is the article's principal author and is responsible for the implementation of the software and creation of the underlying algorithms and data analysis. OS has contributed to work on the data and the writing/editing of the article. SR, GW, and RL provided conceptual guidance on the new methodology and study, critically reviewed the drafting of the manuscript, and were consulted when needed. CM contributed to the design and development of the main features covered in this paper. MA and JG contributed to the grant application, contributed with the conception and design of the project, conduct of the study, supervision and edits on the early and final draft.The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.", 'kwd': u'youth mental health, psychosis, depression, computational health, chatbots, sentiment analysis', 'title': u'Artificial Intelligence-Assisted Online Social Therapy for Youth Mental Health'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4721742/', 'p': u'In this paper the various driving style analysis solutions are investigated. An in-depth investigation is performed to identify the relevant machine learning and artificial intelligence algorithms utilised in current driver behaviour and driving style analysis systems. This review therefore serves as a trove of information, and will inform the specialist and the student regarding the current state of the art in driver style analysis systems, the application of these systems and the underlying artificial intelligence algorithms applied to these applications. The aim of the investigation is to evaluate the possibilities for unique driver identification utilizing the approaches identified in other driver behaviour studies. It was found that Fuzzy Logic inference systems, Hidden Markov Models and Support Vector Machines consist of promising capabilities to address unique driver identification algorithms if model complexity can be reduced.Real-time acceleration and jerk variance evaluation for individual vehicles are often used to detect anomalous behaviour in risk assessment [51]. Accident detection is performed mainly based on methods that monitor traffic density as described in [52], but real-time driver behaviour and vehicle movement analysis systems can also prove relevant and immediate. Certain early-accident detection applications include immediate dispatch of emergency services and road-side assistance services directly upon accident detection [53]. This means that emergency assistance services can be aware of accidents, and the severity thereof, before or without the incident being reported. Accidents can be detected based on impact readings based on longitudinal and transversal acceleration measurements. In [26], an innovative system is presented to integrate accident detection with driver behaviour and driver style classification systems. This is done based on critical jerk evaluation, which enables safety critical braking event identification and accident occurrence recognition from real-time driving data, due to the high correlation between them. The jerk method that was performed had a success rate of 1.6 times greater than the longitudinal acceleration methods.', 'kwd': u'driving style, driver behaviour, artificial intelligence, machine learning, driver safety, road accident, driver identification', 'title': u'A Review of Intelligent Driving Style Analysis Systems and Related Artificial Intelligence Algorithms'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4938803/', 'p': u'We analyzed 3,872 common genetic variants across the\nESR1 locus (encoding estrogen receptor \u03b1) in\n118,816 subjects from three international consortia. We found evidence for at\nleast five independent causal variants, each associated with different phenotype\nsets, including estrogen receptor (ER+ or\nER\u2212) and human ERBB2 (HER2+ or\nHER2\u2212) tumor subtypes, mammographic density and tumor\ngrade. The best candidate causal variants for ER\u2212 tumors lie\nin four separate enhancer elements, and their risk alleles reduce expression of\nESR1, RMND1 and CCDC170,\nwhereas the risk alleles of the strongest candidates for the remaining\nindependent causal variant disrupt a silencer element and putatively increase\nESR1 and RMND1 expression.We successfully genotyped 902 SNPs across a 1-Mb region containing\nESR1 in 50 case-control studies from populations of\nEuropean (89,050 participants) and Asian (12,893 participants) ancestry in BCAC,\ntogether with 15,252 BRCA1 mutation carriers in CIMBA.\nMammographic density measures were available for 6,979 women from the BCAC\nstudies and an additional 1,621 women from the MODE Consortium, who had also\nbeen genotyped using the iCOGS array. Subsequently, the genotypes of additional\nvariants with minor allele frequency (MAF) >2% were imputed in all\nEuropean-ancestry participants, using data from the 1000 Genomes Project as a\nreference. In total, data from 3,872 genotyped or imputed (imputation info score\n>0.3) SNPs were analyzed. Results for all SNPs associated with overall breast\ncancer risk (P < 1 \xd7 10\u22124) are\npresented in Supplementary\nTable 1. Manhattan plots of the associations of these 3,872 SNPs with\nthe main phenotypes are shown in Figure\n1.', 'kwd': '-', 'title': u'Breast cancer risk variants at 6q25 display different phenotype\nassociations and regulate ESR1, RMND1 and\nCCDC170'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877137/', 'p': u'Quantitative analysis of SPECT and PET has become a major part of nuclear\ncardiology practice. Current software tools can automatically segment the left\nventricle, quantify function, establish myocardial perfusion maps and estimate\nglobal and local measures of stress/rest perfusion \u2013 all with minimal\nuser input. State-of-the-art automated techniques have been shown to offer high\ndiagnostic accuracy for detecting coronary artery disease, as well as predict\nprognostic outcomes. This chapter briefly reviews these techniques, highlights\nseveral challenges and discusses the latest developments.The first step in quantification of perfusion and function is\nsegmentation of the LV from both gated and static reconstructed data.\nSegmentation of the myocardium may sometimes be challenging due to possible\nlarge perfusion defects, extra-cardiac activity, and image noise. Typically, the\nmost common sources of incorrect automated contours are gut activity and\nincorrect definition of the valve plane (Figure\n1). Nonetheless, current software tools allow accurate automatic\ndefinition of LV contours in up to 90%.2 Incorrect segmentation in the minority of cases\ncan result in spurious defects mimicking perfusion abnormalities, and therefore,\nsome supervision by an experienced observer is still required during this step.\nHowever, this can be accomplished by an experienced technologist, prior to scan\ninterpretation. Furthermore, recent software developments, which are discussed\nin this review, can be used to check automated LV contours, allowing readers to\ntarget manual adjustment only to those studies flagged by the algorithm for\npotential errors.', 'kwd': u'SPECT, PET, automated quantitation, myocardial function, left ventricular ejection fraction, myocardial perfusion, total perfusion deficit, ischemia', 'title': u'Automated Quantitative Nuclear Cardiology Methods'}], 'Arterial Coronary Syndrome AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4101190/', 'p': u'This work deals with the development of an intelligent approach for clinical decision making in the diagnosis of the Sleep Apnea/Hypopnea Syndrome, SAHS, from the analysis of respiratory signals and oxygen saturation in arterial blood, SaO2. In order to accomplish the task the proposed approach makes use of different artificial intelligence techniques and reasoning processes being able to deal with imprecise data. These reasoning processes are based on fuzzy logic and on temporal analysis of the information. The developed approach also takes into account the possibility of artifacts in the monitored signals. Detection and characterization of signal artifacts allows detection of false positives. Identification of relevant diagnostic patterns and temporal correlation of events is performed through the implementation of temporal constraints.Analysis of the respiratory signals involves analysis of airflow and abdominal and thoracic excursions for the detection and quantification of respiratory pauses. These pauses are characterized by intervals with amplitude reductions with respect to the normal respiration. On the other hand analysis of arterial blood oxygen saturation signal, SaO2, is performed in order to detect and quantify de-saturation and re-saturation intervals indicative of the presence of apneic events. Main objective is localization of specific apneic evidences in the respiratory activity of the patient to be correlated in time forming diagnostic patterns.', 'kwd': u'Artificial Intelligence in Medicine, Decision Support Systems, Fuzzy Logic, Intelligent Monitoring, Signal Processing, Sleep Apneas, Temporal Reasoning.', 'title': u'Intelligent Approach for Analysis of Respiratory Signals and Oxygen Saturation in the Sleep Apnea/Hypopnea Syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4523666/', 'p': u'Automatic diagnosis of the Sleep Apnea-Hypopnea Syndrome (SAHS) has become an important area of research due to the growing interest in the field of sleep medicine and the costs associated with its manual diagnosis. The increment and heterogeneity of the different techniques, however, make it somewhat difficult to adequately follow the recent developments. A literature review within the area of computer-assisted diagnosis of SAHS has been performed comprising the last 15 years of research in the field. Screening approaches, methods for the detection and classification of respiratory events, comprehensive diagnostic systems, and an outline of current commercial approaches are reviewed. An overview of the different methods is presented together with validation analysis and critical discussion of the current state of the art.The authors declare that there is no conflict of interests regarding the publication of this paper.', 'kwd': '-', 'title': u'Computer-Assisted Diagnosis of the Sleep Apnea-Hypopnea Syndrome: A Review'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408160/', 'p': u'See Glossary (Chapter 29) for explanation of terms.The NHANES 2011 to 2014 data are used in this Update to present\nestimates of the percentage of people with high lipid values, DM, overweight,\nand obesity. The NHIS is used for the prevalence of cigarette smoking and\nphysical inactivity. Data for students in grades 9 through 12 are obtained from\nthe YRBSS.', 'kwd': u'AHA Scientific Statements, cardiovascular diseases, epidemiology, risk factors, statistics, stroke', 'title': u'Heart Disease and Stroke Statistics\u20142017\nUpdate'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2646316/', 'p': u'The goal of personalised medicine in the intensive care unit (ICU) is to predict which diagnostic tests, monitoring interventions and treatments translate to improved outcomes given the variation between patients. Unfortunately, processes such as gene transcription and drug metabolism are dynamic in the critically ill; that is, information obtained during static non-diseased conditions may have limited applicability. We propose an alternative way of personalising medicine in the ICU on a real-time basis using information derived from the application of artificial intelligence on a high-resolution database. Calculation of maintenance fluid requirement at the height of systemic inflammatory response was selected to investigate the feasibility of this approach.The Multi-parameter Intelligent Monitoring for Intensive Care II (MIMIC II) is a database of patients admitted to the Beth Israel Deaconess Medical Center ICU in Boston. Patients who were on vasopressors for more than six hours during the first 24 hours of admission were identified from the database. Demographic and physiological variables that might affect fluid requirement or reflect the intravascular volume during the first 24 hours in the ICU were extracted from the database. The outcome to be predicted is the total amount of fluid given during the second 24 hours in the ICU, including all the fluid boluses administered.', 'kwd': '-', 'title': u'An artificial intelligence tool to predict fluid requirement in the intensive care unit: a proof-of-concept study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3654146/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 36th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4456713/', 'p': u'Despite a vast literature, atherosclerosis and the associated ischemia/reperfusion injuries remain today in many ways a mystery. Why do atheromatous plaques make and store a supply of cholesterol and sulfate within the major arteries supplying the heart? Why are treatment programs aimed to suppress certain myocardial infarction risk factors, such as elevated serum homocysteine and inflammation, generally counterproductive?Our methods are based on an extensive search of the literature in atherosclerotic cardiovascular disease as well as in the area of the unique properties of water, the role of biosulfates in the vascular wall, and the role of electromagnetic fields in vascular flow. Our investigation reveals a novel pathology linked to atherosclerosis that better explains the observed facts than the currently held popular view.', 'kwd': '-', 'title': u'A novel hypothesis for atherosclerosis as a cholesterol sulfate deficiency syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4429500/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 37th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493079/', 'p': '-', 'kwd': '-', 'title': u'36th International Symposium on Intensive Care and Emergency Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3848855/', 'p': u'The classification of Acute Coronary Syndrome (ACS), using artificial intelligence (AI), has recently drawn the attention of the medical researchers. Using this approach, patients with myocardial infarction can be differentiated from those with unstable angina. The present study aims to develop an integrated model, based on the feature selection and classification, for the automatic classification of ACS.A dataset containing medical records of 809 patients suspected to suffer from ACS was used. For each subject, 266 clinical factors were collected. At first, a feature selection was performed based on interviews with 20 cardiologists; thereby 40 seminal features for classifying ACS were selected. Next, a feature selection algorithm was also applied to detect a subset of the features with the best classification accuracy. As a result, the feature numbers considerably reduced to only seven. Lastly, based on the seven selected features, eight various common pattern recognition tools for classification of ACS were used.', 'kwd': u'Acute coronary syndrome, Artificial intelligence, Clinical decision support systems, Classification, Diagnosis', 'title': u'Application of pattern recognition tools for classifying acute coronary syndrome: an integrated medical modeling'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042923/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part two'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5052591/', 'p': u'Frequency domain measures of heart rate variability (HRV) are associated with adverse events after a myocardial infarction. However, patterns in the traditional frequency domain (measured in Hz, or cycles per second) may capture different cardiac phenomena at different heart rates. An alternative is to consider frequency with respect to heartbeats, or beatquency. We compared the use of frequency and beatquency domains to predict patient risk after an acute coronary syndrome. We then determined whether machine learning could further improve the predictive performance. We first evaluated the use of pre-defined frequency and beatquency bands in a clinical trial dataset (N\u2009=\u20092302) for the HRV risk measure LF/HF (the ratio of low frequency to high frequency power). Relative to frequency, beatquency improved the ability of LF/HF to predict cardiovascular death within one year (Area Under the Curve, or AUC, of 0.730 vs. 0.704, p\u2009<\u20090.001). Next, we used machine learning to learn frequency and beatquency bands with optimal predictive power, which further improved the AUC for beatquency to 0.753 (p\u2009<\u20090.001), but not for frequency. Results in additional validation datasets (N\u2009=\u20092255 and N\u2009=\u2009765) were similar. Our results suggest that beatquency and machine learning provide valuable tools in physiological studies of HRV.Our work primarily utilized electrocardiographic (ECG) recordings obtained from a clinical trial of patients after NSTEACS19. The dataset consists of all 2,302 patients in the placebo arm, and contains 93 cardiovascular deaths within the median follow-up of one year. We focused on the placebo arm because the treatment arm was prescribed ranolazine, a drug that may have anti-arrhythmic properties20 and thus affect ECG measures. We used this dataset to compare frequency and beatquency LF/HF and to train and test machine learning models. If not otherwise indicated, all results in this work refer to this dataset. In addition, we employed two additional \u201choldout datasets\u201d, for further validation of the machine learning models that were developed, using the dataset described above. These datasets are described in the Supplementary Information. Patient characteristics are reported in Table 1. For all three datasets, up to 7 days of ambulatory ECG signals recorded at 128\u2009Hz are available for each patient. In this work, we used the first 24\u2009hours from each patient to compute the heart rate time series. The protocol was approved by the local or central Institutional Review Board at all participating centers.', 'kwd': '-', 'title': u'Beatquency domain and machine learning improve prediction of cardiovascular death after acute coronary syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4815360/', 'p': u'The endothelins comprise three structurally similar 21-amino acid peptides. Endothelin-1 and -2 activate two G-protein coupled receptors, ETA and ETB, with equal affinity, whereas endothelin-3 has a lower affinity for the ETA subtype. Genes encoding the peptides are present only among vertebrates. The ligand-receptor signaling pathway is a vertebrate innovation and may reflect the evolution of endothelin-1 as the most potent vasoconstrictor in the human cardiovascular system with remarkably long lasting action. Highly selective peptide ETA and ETB antagonists and ETB agonists together with radiolabeled analogs have accurately delineated endothelin pharmacology in humans and animal models, although surprisingly no ETA agonist has been discovered. ET antagonists (bosentan, ambrisentan) have revolutionized the treatment of pulmonary arterial hypertension, with the next generation of antagonists exhibiting improved efficacy (macitentan). Clinical trials continue to explore new applications, particularly in renal failure and for reducing proteinuria in diabetic nephropathy. Translational studies suggest a potential benefit of ETB agonists in chemotherapy and neuroprotection. However, demonstrating clinical efficacy of combined inhibitors of the endothelin converting enzyme and neutral endopeptidase has proved elusive. Over 28 genetic modifications have been made to the ET system in mice through global or cell-specific knockouts, knock ins, or alterations in gene expression of endothelin ligands or their target receptors. These studies have identified key roles for the endothelin isoforms and new therapeutic targets in development, fluid-electrolyte homeostasis, and cardiovascular and neuronal function. For the future, novel pharmacological strategies are emerging via small molecule epigenetic modulators, biologicals such as ETB monoclonal antibodies and the potential of signaling pathway biased agonists and antagonists.Despite the identification of ECE-1 as a rate limiting enzyme in the synthesis of ET-1, there has been much less development of selective small molecule inhibitors that could potentially reduce levels of ET-1 in pathophysiological conditions compared with the considerable effort to discover receptor antagonists. PD159790 was developed to be selective for ECE-1 compared with NEP (Ahn et al., 1998). The compound has been validated experimentally by altering the pH of endothelial cells in culture: at the optimum for ECE-1 activity, pH 6.9, PD159790 inhibited Big-ET-1 conversion but not at the optimum for ECE-2, pH 5.4 (Russell and Davenport, 1999a). In addition, the compound had no effect on the alternative pathway for ET-1 metabolism via chymase generation of ET-1(1-31) (Maguire et al., 2001).', 'kwd': '-', 'title': u'Endothelin'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534347/', 'p': '-', 'kwd': '-', 'title': u'B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405523/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 38th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2948592/', 'p': u'We describe semantic relation (SR) classification on medical discharge summaries. We focus on relations targeted to the creation of problem-oriented records. Thus, we define relations that involve the medical problems of patients.We represent patients\u2019 medical problems with their diseases and symptoms. We study the relations of patients\u2019 problems with each other and with concepts that are identified as tests and treatments. We present an SR classifier that studies a corpus of patient records one sentence at a time. For all pairs of concepts that appear in a sentence, this SR classifier determines the relations between them. In doing so, the SR classifier takes advantage of surface, lexical, and syntactic features and uses these features as input to a support vector machine. We apply our SR classifier to two sets of medical discharge summaries, one obtained from the Beth Israel-Deaconess Medical Center (BIDMC), Boston, MA and the other from Partners Healthcare, Boston, MA.', 'kwd': u'Lexical context, support vector machines, relation classification for the problem-oriented record, medical language processing', 'title': u'Semantic Relations for Problem-Oriented Medical Records'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4757170/', 'p': '-', 'kwd': '-', 'title': u'WFITN 2015 Abstracts Oral Abstracts'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3808723/', 'p': u'\nBackground. Coronary artery disease (CAD) is a complex, multifactorial disease in which personality seems to play a role but with no definition in combination with other risk factors. Objective. To explore the nonlinear and simultaneous pathways between traditional and personality traits risk factors and coronary stenosis by Artificial Neural Networks (ANN) data mining analysis. Method. Seventy-five subjects were examined for traditional cardiac risk factors and personality traits. Analyses were based on a new data mining method using a particular artificial adaptive system, the autocontractive map (AutoCM). Results. Several traditional Cardiovascular Risk Factors (CRF) present significant relations with coronary artery plaque (CAP) presence or severity. Moreover, anger turns out to be the main factor of personality for CAP in connection with numbers of traditional risk factors. Hidden connection map showed that anger, hostility, and the Type D personality subscale social inhibition are the core factors related to the traditional cardiovascular risk factors (CRF) specifically by hypertension. Discussion. This study shows a nonlinear and simultaneous pathway between traditional risk factors and personality traits associated with coronary stenosis in CAD patients without history of cardiovascular disease. In particular, anger seems to be the main personality factor for CAP in addition to traditional risk factors.The following traditional cardiac risk factors were examined: hypertension: arterial blood pressure \u2265 140/90\u2009mm\u2009Hg or taking antihypertensive medications [16]; diabetes: nonfasting plasma glucose concentration of at least 200\u2009mg/dL (11.1\u2009mmol/L), or fasting plasma glucose level of at least 126\u2009mg/dL (7.0\u2009mmol/L), or being treated with antidiabetic medication; overweight: body mass index (BMI) (calculated as weight divided by height squared) \u226527\u2009kg/m2 (WHO); dyslipidemia: total serum cholesterol level is higher than 240\u2009mg/dL or a serum triglyceride level is 200\u2009mg/dL or more (or both) or use of a lipid-lowering agent; smoking: at least one cigarette per day or quit smoking during the previous year; family history of CAD: a first degree or second degree relative with premature cardiovascular disease (age \u2264 55 years).', 'kwd': '-', 'title': u'Combining Personality Traits with Traditional Risk Factors for Coronary Stenosis: An Artificial Neural Networks Solution in Patients with Computed Tomography Detected Coronary Artery Disease'}], 'Tomography AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3929431/', 'p': u'3D mapping of the human body by X-ray CT provides a novel method of tracing the meridian systems, hence providing useful knowledge toward the modernization of clinical acupuncture practice.The author (J.K.) would like to thank K.S. Soh, PhD, and M.S. Chung, MD, PhD, for helpful advice. The current authors used the Digital Korean data that were produced and distributed by the Catholic Institute for Applied Anatomy, College of Medicine, Catholic University of Korea and the Korean Institute of Science and Technology Information.', 'kwd': u'Acupuncture Points, Digital Korean Data, X-Ray Computed Tomography, Surface Reconstruction, 3-Dimensional Image Process', 'title': u'Positioning Standardized Acupuncture Points on the Whole Body Based on X-Ray Computed Tomography Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405523/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 38th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/', 'p': u'The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements.Under a mild assumption on the activation function, a two-layer neural network with a finite number of hidden units can approximate any continuous function (63), and thus it is regarded as universal approximator. However, it is also possible to approximate complex functions to the same accuracy using a \u2018deep\u2019 architecture, i.e., more than two layers, with much fewer number of units in total (3). Hence, it is possible to reduce the number of trainable parameters, thus allowing to train with a relatively small dataset (64).', 'kwd': u'Medical image analysis, deep learning, unsupervised feature learning', 'title': u'Deep Learning in Medical Image Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4429500/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 37th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4114443/', 'p': u'This paper is to report the new imaging of gastric cancers without the use of imaging agents. Both gastric normal regions and gastric cancer regions can be distinguished by using the principal component analysis (PCA) based on the gray level co-occurrence matrix (GLCM).Human gastric cancer BGC823 cells were implanted into the stomachs of nude mice. Then, 3, 5, 7, 9 or 11\xa0days after cancer cells implantation, the nude mice were sacrificed and their stomachs were removed. X-ray in-line phase contrast imaging (XILPCI), an X-ray phase contrast imaging method, has greater soft tissue contrast than traditional absorption radiography and generates higher-resolution images. The gastric specimens were imaged by an XILPCIs\u2019 charge coupled device (CCD) of 9\xa0\u03bcm image resolution. The PCA of the projective images\u2019 region of interests (ROIs) based on GLCM were extracted to discriminate gastric normal regions and gastric cancer regions. Different stages of gastric cancers were classified by using support vector machines (SVMs).', 'kwd': u'X-ray in-line phase contrast imaging, X-ray absorption imaging, Gastric cancer, Principal component analysis, Support vector machine', 'title': u'Investigation of gastric cancers in nude mice using X-ray in-line phase contrast imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4511115/', 'p': u'In this study, we investigated the effectiveness of a novel Iterative Reconstruction (IR) method coupled with Dual-Dictionary Learning (DDL) for image reconstruction in a dedicated breast Computed Tomography (CT) system based on a Cadmium-Zinc-Telluride (CZT) photon-counting detector and compared it to the Filtered-Back-Projection (FBP) method with the ultimate goal of reducing the number of projections necessary for reconstruction without sacrificing image quality. Postmortem breast samples were scanned in a fan-beam CT system and were reconstructed from 100\u2013600 projections with both IR and FBP methods. The Contrast-to-Noise Ratio (CNR) between the glandular and adipose tissues of the postmortem breast samples was calculated to compare the quality of images reconstructed from IR and FBP. The spatial resolution of the two reconstruction techniques was evaluated using Aluminum (Al) wires with diameters of 643, 813, 1020, 1290 and 1630 \xb5m in a plastic epoxy resin phantom with diameter of 13 cm. Both the spatial resolution and the CNR were improved with IR compared to FBP for the images reconstructed from the same number of projections. In comparison with FBP reconstruction, the CNR was improved from 3.4 to 7.5 by using the IR method with 6-fold fewer projections while maintaining the same spatial resolution. The study demonstrated that the IR method coupled with DDL could significantly reduce the required number of projections for a CT reconstruction compared to FBP method while achieving a much better CNR and maintaining the same spatial resolution. From this, the radiation dose and scanning time can potentially be reduced by a factor of approximately 6 by using this IR method for image reconstruction in a CZT-based breast CT system.Figure 2(a) illustrates the construction of the high resolution phantom used in this study, which is motivated by a previous study (Shen et al., 2010). A cylinder with 13 cm in diameter and 2 cm in length is constructed of resin as the phantom base and an insert with 5 fine Al wires of various diameters (643, 813, 1020, 1290 and 1630 \xb5m in diameter) is placed in this base. The resin is chosen not only for its similar x-ray attenuation to breast tissue (0.2076 cm2/g for resin and 0.2186 cm2/g for breast tissue at 50 keV) (Hubbell and Seltzer, 1995), but also for its low cost and convenience in fabricating the phantom base and inserting Al wires. The Al wires in this insert are orientated vertically, and the profiles extracted from reconstructed CT images are proposed to study spatial resolution. These wires are arranged with enough space between each of them to minimize interacted artifacts in the image reconstruction. Figure 2(b) shows a photo of this high-resolution phantom and Al wires inside. Postmortem breast samples were obtained from Willed Body Program in School of Medicine at University of California Irvine, sealed in plastic bags. 6 samples were selected for this study with the mass varying from 114 to 924 gram and the breast density varying from 21% to 72%. These samples were placed in a cylindrical container approximately 10 cm in diameter made of high-density polyethylene plastic during the image acquisition.', 'kwd': u'Iterative reconstruction, Dual-dictionary learning, Contrast-to-noise ratio, Spatial resolution, Spectral breast computed tomography', 'title': u'Dual-Dictionary Learning-Based Iterative Image Reconstruction for Spectral Computed Tomography Application'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5234134/', 'p': u'Metal objects implanted in the bodies of patients usually generate severe streaking artifacts in reconstructed images of X-ray computed tomography, which degrade the image quality and affect the diagnosis of disease. Therefore, it is essential to reduce these artifacts to meet the clinical demands.In this work, we propose a Gaussian diffusion sinogram inpainting metal artifact reduction algorithm based on prior images to reduce these artifacts for fan-beam computed tomography reconstruction. In this algorithm, prior information that originated from a tissue-classified prior image is used for the inpainting of metal-corrupted projections, and it is incorporated into a Gaussian diffusion function. The prior knowledge is particularly designed to locate the diffusion position and improve the sparsity of the subtraction sinogram, which is obtained by subtracting the prior sinogram of the metal regions from the original sinogram. The sinogram inpainting algorithm is implemented through an approach of diffusing prior energy and is then solved by gradient descent. The performance of the proposed metal artifact reduction algorithm is compared with two conventional metal artifact reduction algorithms, namely the interpolation metal artifact reduction algorithm and normalized metal artifact reduction algorithm. The experimental datasets used included both simulated and clinical datasets.', 'kwd': u'Metal artifact reduction, Sinogram inpainting, Gaussian diffusion, Prior image, X-ray CT', 'title': u'Gaussian diffusion sinogram inpainting for X-ray CT metal artifact reduction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4482362/', 'p': u'This review describes the diffusion model for light transport in tissues and the medical applications of diffuse light. Diffuse optics is particularly useful for measurement of tissue hemodynamics, wherein quantitative assessment of oxy- and deoxy-hemoglobin concentrations and blood flow are desired. The theoretical basis for near-infrared or diffuse optical spectroscopy (NIRS or DOS, respectively) is developed, and the basic elements of diffuse optical tomography (DOT) are outlined. We also discuss diffuse correlation spectroscopy (DCS), a technique whereby temporal correlation functions of diffusing light are transported through tissue and are used to measure blood flow. Essential instrumentation is described, and representative brain and breast functional imaging and monitoring results illustrate the workings of these new tissue diagnostics.Most of the following theoretical discussion will be given in the frequency-domain, with the time-domain solution given for a common case. Frequency-domain sources induce fluence rate disturbances that behave in many ways like overdamped waves. To appreciate this point, we start with the diffusion equation for the fluence rate (Equation (9)) and assume the source term has DC and AC parts and can be written in the form S(r, t) = SDC(r) + SAC(r)e\u2212i\u03c9t. Then we look for the solutions that oscillate at the same angular frequency as the source. These AC solutions will have the following general form\n', 'kwd': '-', 'title': u'Diffuse Optics for Tissue Monitoring and Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5664813/', 'p': u'Wheat is one of\xa0the most widely grown crop in temperate climates for food and animal feed. In order to meet the demands of the predicted population increase in an ever-changing climate, wheat production needs to dramatically increase. Spike and grain traits are critical determinants of final yield and grain uniformity a commercially desired trait, but their analysis is laborious and often requires destructive harvest. One of the current challenges is to develop an accurate, non-destructive method for spike and grain trait analysis capable of handling large populations.In this study we describe the development of a robust method for the accurate extraction and measurement of spike and grain morphometric parameters from images acquired by X-ray micro-computed tomography (\u03bcCT). The image analysis pipeline developed automatically identifies plant material of interest in \u03bcCT images, performs image analysis, and extracts morphometric data. As a proof of principle, this integrated methodology was used to analyse the spikes from a population of wheat plants subjected to high temperatures under two different water regimes. Temperature has a negative effect on spike height and grain number with the middle of the spike being the most affected region. The data also confirmed that increased grain volume was correlated with the decrease in grain number under mild stress.', 'kwd': u'X-ray micro computed tomography, \u03bcCT, Image analysis, 3D vision, Grain traits, Wheat, Temperature', 'title': u'Non-destructive, high-content analysis of wheat grain traits using X-ray micro computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5659258/', 'p': u'Author contributions: E.L.D., C.J., K.P.K., and N.K. designed research; E.L.D., W.G.R., D.G., X.X., and N.K. performed research; E.L.D., W.G.R., J.A.P., H.F., V.d.A., K.F., J.T.V., and N.K. contributed unpublished reagents/analytic tools; E.L.D. and D.G. analyzed data; E.L.D., J.A.P., K.P.K., and N.K. wrote the paper.', 'kwd': u'Automated segmentation, cell counting, electron microscopy, neocortex, neuroanatomy, X-ray microtomography', 'title': u'Quantifying Mesoscale Neuroanatomy Using X-Ray Microtomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481066/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - A - Postergraduate Educational Programme'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5341795/', 'p': u'Rapid development in the performance of sophisticated optical components, digital image sensors, and computer abilities along with decreasing costs has enabled three-dimensional (3-D) optical measurement to replace more traditional methods in manufacturing and quality control. The advantages of 3-D optical measurement, such as noncontact, high accuracy, rapid operation, and the ability for automation, are extremely valuable for inline manufacturing. However, most of the current optical approaches are eligible for exterior instead of internal surfaces of machined parts. A 3-D optical measurement approach is proposed based on machine vision for the 3-D profile measurement of tiny complex internal surfaces, such as internally threaded holes. To capture the full topographic extent (peak to valley) of threads, a side-view commercial rigid scope is used to collect images at known camera positions and orientations. A 3-D point cloud is generated with multiview stereo vision using linear motion of the test piece, which is repeated by a rotation to form additional point clouds. Registration of these point clouds into a complete reconstruction uses a proposed automated feature-based 3-D registration algorithm. The resulting 3-D reconstruction is compared with x-ray computed tomography to validate the feasibility of our proposed method for future robotically driven industrial 3-D inspection.Yuanzheng Gong is a PhD student in mechanical engineering at the University of Washington. He received his BS degree and PhD in mechanical engineering from the University of Science and Technology of China in 2009 and University of Washington in 2016. His main research interests include three-dimensional reconstruction, optical metrology, stereo vision, and machine vision. He is a student member of SPIE and IEEE.', 'kwd': u'three-dimensional surface reconstruction, three-dimensional measurement, point cloud registration, machine vision, optical metrology', 'title': u'Three-dimensional measurement of small inner surface profiles using feature-based 3-D panoramic registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3533621/', 'p': '-', 'kwd': '-', 'title': u'ECR 2011 Book of Abstracts - A - Postgraduate Educational Programme'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3888983/', 'p': u'We report here a facile thermal decomposition approach to creating tungsten oxide nanorods (WO2.9 NRs) with a length of 13.1 \xb1 3.6\u2005nm and a diameter of 4.4 \xb1 1.5\u2005nm for tumor theranostic applications. The formed WO2.9 NRs were modified with methoxypoly(ethylene glycol) (PEG) carboxyl acid via ligand exchange to have good water dispersability and biocompatibility. With the high photothermal conversion efficiency irradiated by a 980\u2005nm laser and the better X-ray attenuation property than clinically used computed tomography (CT) contrast agent Iohexol, the formed PEGylated WO2.9 NRs are able to inhibit the growth of the model cancer cells in vitro and the corresponding tumor model in vivo, and enable effective CT imaging of the tumor model in vivo. Our \u201ckilling two birds with one stone\u201d strategy could be extended for fabricating other nanoplatforms for efficient tumor theranostic applications.The aqueous solution of PEGylated WO2.9 NRs (100\u2005\u03bcg/mL) showed a blue color with strong absorption in the NIR region (Fig. 2a), which was attributed to the strong localized surface plasmon resonances (LSPR) of the NRs59. The strong NIR absorption of PEGylated WO2.9 NRs motivated us to investigate their potential application as photothermal agents. The temperature increase of the aqueous solution in the presence of the PEGylated NRs as a function of the NR concentration (60 to 1200\u2005\u03bcg/mL) under the 980\u2005nm laser irradiation shows that the solution temperature increase can reach 41.7\xb0C at the NR concentration of 1200\u2005\u03bcg/mL, and higher concentration of NRs leads to more prominent temperature increase (Figs. 2b and S2a). In contrast, the water solution in the absence of NRs does not show any obvious temperature increase under similar experimental conditions (Fig. 2b). To assess the NIR photostability of PEGylated WO2.9 NRs, five cycles of Laser on/off were performed by irradiating the aqueous solution of PEGylated WO2.9 NRs via a 980\u2005nm laser for 10\u2005min (Laser on), followed by cooling down to room temperature without NIR laser irradiation for 30\u2005min (Laser off). As shown in Fig. 2c, the temperature increases of 20.1\xb0C and 30.3\xb0C were able to be achieved after the first laser on for the NR concentration of 100 and 750\u2005\u03bcg/mL, respectively. No significant change in the temperature increase was observed after five cycles. Furthermore, the absorbance of the NRs (180\u2005\u03bcg/mL) at 980\u2005nm remained stable even after ten cycles of laser irradiation (Fig. S2b).', 'kwd': '-', 'title': u'Tungsten Oxide Nanorods: An Efficient Nanoplatform for Tumor CT Imaging and Photothermal Therapy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3202718/', 'p': '-', 'kwd': u'three-dimensional, middle-ear, models, X-ray, computer, tomography', 'title': u'\nThree-Dimensional Modelling of the Middle-Ear Ossicular Chain Using a Commercial High-Resolution X-Ray CT Scanner\n'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5337239/', 'p': '-', 'kwd': u'Pulmonary image analysis, Computer-aided detection, Computer-aided diagnosis, Image processing, Machine learning, Deep learning', 'title': u'Fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5330543/', 'p': u'Kawasaki disease (KD) is an acute childhood disease complicated by coronary artery aneurysms, intima thickening, thrombi, stenosis, lamellar calcifications, and disappearance of the media border. Automatic classification of the coronary artery layers (intima, media, and scar features) is important for analyzing optical coherence tomography (OCT) images recorded in pediatric patients. OCT has been known as an intracoronary imaging modality using near-infrared light which has recently been used to image the inner coronary artery tissues of pediatric patients, providing high spatial resolution (ranging from 10 to 20 \u03bcm). This study aims to develop a robust and fully automated tissue classification method by using the convolutional neural networks (CNNs) as feature extractor and comparing the predictions of three state-of-the-art classifiers, CNN, random forest (RF), and support vector machine (SVM). The results show the robustness of CNN as the feature extractor and random forest as the classifier with classification rate up to 96%, especially to characterize the second layer of coronary arteries (media), which is a very thin layer and it is challenging to be recognized and specified from other tissues.Generating an ensemble of trees using random vectors, which control the growth of each tree in the ensemble significantly increases the classification accuracy. Random Forest works efficiently on large data sets, carries a very low risk of overfitting, and is a robust classifier for noisy data. The trees are grown based on the CART methodology to maximum size without pruning. Two important factors which affect the Random Forest accuracy are the strength, s, of each tree and the correlation, \u03c1, between them. Generalization error for Random Forest classifier is proportional to the ratio \u03c1/s2. Hence, the smaller this ratio, the better functioning of Random Forest will be concluded. The correlation between trees is reduced by random selection of subset of features at each node to split on [43,44]. To improve the performance of the classifier in our experiments, we started from 100 trees and increase the number of trees to 1000. The optimal number of trees is chosen by considering the Out Of Bag (OOB) error rate. By setting the number of trees to 241, the error rate is low, almost close to the minimum error rate, and fewer number of trees reduces the computational burden; so, classifier performance is faster. The number of randomly selected predictors (another tuning parameter in Random Forest) is set to 7. Random Forest training and validation is described in section 2.3.4.', 'kwd': u'(100.0100) Image processing, (100.2960) Image analysis, (100.4996) Pattern recognition, neural networks, (110.0110) Imaging systems, (110.2960) Image analysis, (110.4500) Optical coherence tomography', 'title': u'Deep feature learning for automatic tissue classification of coronary artery using optical coherence tomography'}], 'Risk Score AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684738/', 'p': u'Coronary computed tomography angiography (CTA) can be used to detect and quantitatively assess high-risk plaque features.To validate the ROMICAT score, which was derived using semi-automated quantitative measurements of high-risk plaque features, for the prediction of ACS.', 'kwd': u'coronary computed tomography angiography, acute coronary syndrome, coronary atherosclerotic plaque, acute chest pain, risk score', 'title': u'Computed Tomography-Based High-Risk Coronary Plaque Score to Predict Acute Coronary Syndrome Among Patients With Acute Chest Pain \u2013 Results from the ROMICAT II Trial'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4364461/', 'p': '-', 'kwd': '-', 'title': u'Abstracts for the 15th International Congress on Schizophrenia Research (ICOSR)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4318357/', 'p': u'The theory of task-based assessment of image quality is reviewed in the context of imaging with ionizing radiation, and objective figures of merit (FOMs) for image quality are summarized. The variation of the FOMs with the task, the observer and especially with the mean number of photons recorded in the image is discussed. Then various standard methods for specifying radiation dose are reviewed and related to the mean number of photons in the image and hence to image quality. Current knowledge of the relation between local radiation dose and the risk of various adverse effects is summarized, and some graphical depictions of the tradeoffs between image quality and risk are introduced. Then various dose-reduction strategies are discussed in terms of their effect on task-based measures of image quality.Popescu and Myers [32] recently published a paper describing a signal-search paradigm, along with a phantom design, that enables the evaluation and comparison of iterative reconstruction algorithms in terms of signal detectability vs dose performance using a few tens of images. Standard commercial phantoms for CT quality assessment have limited utility for quantitative evaluation of image quality, because of the limited area available for deriving the statistics of a model observer in the signal-absent condition. Moreover, such phantoms yield only one signal size/contrast combination per image. Phantoms that allow for multiple realizations of background-only and signal-present regions in each image, along with the addition of search as a component of the detection task, enable the investigator to customize the signal detectability level through adjustment of the search region area so that meaningful image quality comparisons can be made across algorithms or imaging systems with a fairly small number of images.', 'kwd': '-', 'title': u'Task-based measures of image quality and their relation to radiation dose and patient risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4803197/', 'p': u'Conceived and designed the experiments: MM FB KA AD. Performed the experiments: FB KA AD. Analyzed the data: FB KA AD. Contributed reagents/materials/analysis tools: MM FB KA AD. Wrote the paper: FB KA AD MM.The body image concern (BIC) continuum ranges from a healthy and positive body image, to clinical diagnoses of abnormal body image, like body dysmorphic disorder (BDD). BDD and non-clinical, yet high-BIC participants have demonstrated a local visual processing bias, characterised by reduced inversion effects. To examine whether this bias is a potential marker of BDD, the visual processing of individuals across the entire BIC continuum was examined. Dysmorphic Concern Questionnaire (DCQ; quantified BIC) scores were expected to correlate with higher discrimination accuracy and faster reaction times of inverted stimuli, indicating reduced inversion effects (occurring due to increased local visual processing). Additionally, an induced global or local processing bias via Navon stimulus presentation was expected to alter these associations. Seventy-four participants completed the DCQ and upright-inverted face and body stimulus discrimination task. Moderate positive associations were revealed between DCQ scores and accuracy rates for inverted face and body stimuli, indicating a graded local bias accompanying increases in BIC. This relationship supports a local processing bias as a marker for BDD, which has significant assessment implications. Furthermore, a moderate negative relationship was found between DCQ score and inverted face accuracy after inducing global processing, indicating the processing bias can temporarily be reversed in high BIC individuals. Navon stimuli were successfully able to alter the visual processing of individuals across the BIC continuum, which has important implications for treating BDD.', 'kwd': '-', 'title': u'Altering Visual Perception Abnormalities: A Marker for Body Image Concern'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5523839/', 'p': u'CAIDE Dementia Risk Score is the first validated tool for estimating dementia risk based on a midlife risk profile.This observational study investigated longitudinal associations of CAIDE Dementia Risk Score with brain MRI, amyloid burden evaluated with PIB-PET, and detailed cognition measures.', 'kwd': u'Aging, cognition, dementia, neuroimaging', 'title': u'Associations of CAIDE Dementia Risk Score with MRI, PIB-PET measures, and\xa0cognition'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4912513/', 'p': '-', 'kwd': '-', 'title': u'French Intensive Care Society, International congress \u2013 R\xe9animation 2016'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4244172/', 'p': '-', 'kwd': '-', 'title': u'2014 Joint ACTRIMS-ECTRIMS Meeting (MSBoston 2014) MS Journal Online: Poster Session 1'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4212306/', 'p': '-', 'kwd': '-', 'title': u'UEG Week 2014 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4519303/', 'p': u'Conceived and designed the experiments: MT KK TE LTW OAA. Performed the experiments: KK MT FB MM TK. Analyzed the data: MT KK LTW FB CLB. Contributed reagents/materials/analysis tools: SD OAA IM IA. Wrote the paper: MT KK LTW OAA FB CLB TK TE IA IM.Bipolar disorder (BD) is a highly heritable disorder with polygenic inheritance. Among the most consistent findings from functional magnetic imaging (fMRI) studies are limbic hyperactivation and dorsal hypoactivation. However, the relation between reported brain functional abnormalities and underlying genetic risk remains elusive. This is the first cross-sectional study applying a whole-brain explorative approach to investigate potential influence of BD case-control status and polygenic risk on brain activation.', 'kwd': '-', 'title': u'Altered Brain Activation during Emotional Face Processing in Relation to Both Diagnosis and Polygenic Risk of Bipolar Disorder'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4573236/', 'p': '-', 'kwd': u'Breast cancer, Computer-aided detection (CAD), Near-term breast cancer risk stratification, Mammographic density feature analysis, Full-field digital mammography (FFDM)', 'title': u'Assessment of a four-view mammographic image feature based fusion model to predict near-term breast cancer risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4896250/', 'p': u'Even though antihistamines are the mainstay in the treatment of chronic spontaneous urticaria (CSU), some CSU patients have not responded to antihistamines. Many clinicians have accepted the efficacy of steroids in the treatment of CSU. There is, however, little evidence supporting steroid use in antihistamine-resistant CSU. The purpose of this study was to demonstrate the efficacy of, and suggest a regimen for, oral steroid in the treatment of CSU patients who were refractory to a high dosage of antihistamines. We conducted a retrospective chart review of all patients diagnosed with urticaria between Feburary 1, 2012, and December 31, 2014. A total of 98 patients with CSU were included. Of these, 16 patients (16.3%) were antihistamine-resistant and prescribed a 2-week course of steroid. Thirteen patients (81.2%) were successfully controlled with antihistamines only after stopping the first course. Second course of steroid induced remission additionally in two patients (12.5%). No adverse events and complications associated with oral steroid were observed over the study period. This study demonstrated the excellence of a 2-week course of oral corticosteroid in antihistamine-resistant CSU and propose standardized corticosteroid treatment regimen.Subcutaneous immunotherapy (SCIT) is a clinically effective treatment in atopic dermatitis. However, there was no mouse model to understand the mechanism of house dust mite immunotherapy in atopic dermatitis. The aim of this study was to establish a mouse model of SCIT mouse model of house dust mite induced atopic dermatitis. Female NC/Nga mice were treated with Dermatophagoides farinae (D.farinae) body extract ointment for 4 weeks to induce atopic dermatitis like skin lesion. Then we separated the mice into 2 groups, control group and immunotherapy (IT) group. Both groups were continuously treated with D.farinae body extract ointment for 4 weeks, however, only immunotherapy (IT) group was treated with 8 injections of D.farinae (100 ug subcutaneouse) twice a week along with D.farinae body extract ointment treatment. Subsequently, clinical severity of skin lesion, histological features, total IgE, IL-4 and IL-10 were measured. We observed that AD-like skin lesions of IT group were milder than control group. In histological examination, epidermis was thinner in IT group than control group. The level of total IgE was decreased in IT group than control group at 3 weeks from the beginning of immunotherapy, while the level of IgG4 was remarkably increased in IT group than control group from the beginning of immunotherapy. Increased level of IL-10 in IT group was observed at 2 weeks from the beginning of immunotherapy than control group. However, there was no significant difference of IL-4 between IT group and control group. In the present study, we have established a house dust mite SCIT mouse model and demonstrated that house dust mite SCIT might improve atopic dermatitis-like skin lesion by decreasing total IgE and increasing the IgG4 and Il-10. Using this model, it will be possible to find novel way to potentiate the effects of allergen immunotherapy.', 'kwd': '-', 'title': u'XXIV World Allergy Congress 2015'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4943498/', 'p': u'Schizophrenia is a serious and chronic mental illness which has a profound effect on the health and well-being related with the well-known nature of psychotic symptoms. The exercise has the potential to improve the life of people with schizophrenia improving physical health and alleviating psychiatric symptoms. However, most people with schizophrenia remains sedentary and lack of access to exercise programs are barriers to achieve health benefits. The aim of this study is to evaluate the effect of exercise on I) the type of intervention in mental health, II) in salivary levels of alpha-amylase and cortisol and serum levels of S100B and BDNF, and on III) the quality of life and self-perception of the physical domain of people with schizophrenia. The sample consisted of 31 females in long-term institutions in the Casa de Sa\xfade Rainha Santa Isabel, with age between 25 and 63, and with diagnosis of schizophrenia according to the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR). Physical fitness was assessed by the six-minute walk distance test (6MWD). Biological variables were determined by ELISA (Enzyme-Linked Immunosorbent Assay). Psychological variables were assessed using SF-36, PSPP-SCV, RSES and SWLS tests. Walking exercise has a positive impact on physical fitness (6MWD \u2013 p\u2009=\u20090.001) and physical components of the psychological tests ([SF-36] physical functioning p\u2009<\u20090.05; [PSPP-SCV] functionality p\u2009<\u20090.05 and SWLS p\u2009<\u20090.05 of people with schizophrenia. The walking program enhances the quality of life and self-perception of the physical domain and physical fitness of people with schizophrenia.\u115f', 'kwd': '-', 'title': u'Proceedings of the 3rd IPLeiria\u2019s International Health Congress'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4864935/', 'p': u'Genomics and proteomics are nowadays the dominant techniques for novel biomarker discovery. However, histopathology images contain a wealth of information related to the tumor histology, morphology and tumor-host interactions that is not accessible through these techniques. Thus, integrating the histopathology images in the biomarker discovery workflow could potentially lead to the identification of new image-based biomarkers and the refinement or even replacement of the existing genomic and proteomic signatures. However, extracting meaningful and robust image features to be mined jointly with genomic (and clinical, etc.) data represents a real challenge due to the complexity of the images.We developed a framework for integrating the histopathology images in the biomarker discovery workflow based on the bag-of-features approach \u2013 a method that has the advantage of being assumption-free and data-driven. The images were reduced to a set of salient patterns and additional measurements of their spatial distribution, with the resulting features being directly used in a standard biomarker discovery application. We demonstrated this framework in a search for prognostic biomarkers in breast cancer which resulted in the identification of several prognostic image features and a promising multimodal (imaging and genomic) prognostic signature. The source code for the image analysis procedures is freely available.', 'kwd': u'Histopathology images, Image analysis, Biomarker discovery, Gene expression, Multimodal data mining', 'title': u'Joint analysis of histopathology image features and gene expression in breast cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3959006/', 'p': u"Most of the erythrocyte related diseases are detectable by hematology images analysis. At the first step of this analysis, segmentation and detection of blood cells are inevitable. In this study, a novel method using a line operator and watershed algorithm is rendered for erythrocyte detection and segmentation in blood smear images, as well as reducing over-segmentation in watershed algorithm that is useful for segmentation of different types of blood cells having partial overlap. This method uses gray scale structure of blood cell, which is obtained by exertion of Euclidian distance transform on binary images. Applying this transform, the gray intensity of cell images gradually reduces from the center of cells to their margins. For detecting this intensity variation structure, a line operator measuring gray level variations along several directional line segments is applied. Line segments have maximum and minimum gray level variations has a special pattern that is applicable for detections of the central regions of cells. Intersection of these regions with the signs which are obtained by calculating of local maxima in the watershed algorithm was applied for cells\u2019 centers detection, as well as a reduction in over-segmentation of watershed algorithm. This method creates 1300 sign in segmentation of 1274 erythrocytes available in 25 blood smear images. Accuracy and sensitivity of the proposed method are equal to 95.9% and 97.99%, respectively. The results show the proposed method's capability in detection of erythrocytes in blood smear images.In this study, an algorithm was introduced for over-segmentation reduction in the watershed algorithm in the microscopic images of blood cells and also for locating of them by the line operator. By use of this algorithm, 1300 markers are created as centers for 1274 erythrocytes in 25 blood smear images. It is a promising result to erythrocyte locating or counting, and also over-segmentation reduction using watershed algorithm to segment them.", 'kwd': u'Blood smear images, line operator, watershed algorithm', 'title': u'Detection and Segmentation of Erythrocytes in Blood Smear Images Using a Line Operator and Watershed Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5123388/', 'p': '-', 'kwd': '-', 'title': u'5th International Symposium on Focused Ultrasound'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493079/', 'p': '-', 'kwd': '-', 'title': u'36th International Symposium on Intensive Care and Emergency Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4793253/', 'p': u'Cardiac computed tomography (CCT) is a reliable and accurate tool for diagnosis of coronary artery diseases and is also frequently used in surgery guidance. Low-dose scans should be considered in order to alleviate the harm to patients caused by X-ray radiation. However, low dose CT (LDCT) images tend to be degraded by quantum noise and streak artifacts. In order to improve the cardiac LDCT image quality, a 3D sparse representation-based processing (3D SR) is proposed by exploiting the sparsity and regularity of 3D anatomical features in CCT. The proposed method was evaluated by a clinical study of 14 patients. The performance of the proposed method was compared to the 2D spares representation-based processing (2D SR) and the state-of-the-art noise reduction algorithm BM4D. The visual assessment, quantitative assessment and qualitative assessment results show that the proposed approach can lead to effective noise/artifact suppression and detail preservation. Compared to the other two tested methods, 3D SR method can obtain results with image quality most close to the reference standard dose CT (SDCT) images.Both the 2D SR method and the proposed 3D SR method include a dictionary training step and an OMP step. In our experiment, it takes 6.27\u2009seconds to train a dictionary for the general 2D SR method. Since the dictionary size (512\u2009\xd7\u20091000) and the training set are much larger than 2D SR method, the training step in 3D SR processing is rather computational intensive and takes about 217.67\u2009seconds. Fortunately, the dictionaries, once trained, can be used to process all the LDCT cases as demonstrated in the above experiments (refer also to41). Table 4 lists the average computation cost required in the operations following the dictionary training only. We can see that, with the trained dictionary available, the 2D SR method requires about 0.90\u2009seconds (in average) to process one 512\u2009\xd7\u2009512 slice; the BM4D method takes 3703.74\u2009seconds (in average) to process one 512\u2009\xd7\u2009512\u2009\xd7\u2009270 CT data set (about 13.72\u2009seconds per 2-D slice) and the 3D SR method requires 843.67\u2009seconds (in average) to process one 512\u2009\xd7\u2009512\u2009\xd7\u2009270 CT data set (about 3.12\u2009seconds per 2-D slice).', 'kwd': '-', 'title': u'Improving Low-dose Cardiac CT Images based on 3D Sparse Representation'}], 'Risk Stratification AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481086/', 'p': u'Breast cancer is the most frequently diagnosed cancer in women. However, the exact cause(s) of breast cancer still remains unknown. Early detection, precise identification of women at risk, and application of appropriate disease prevention measures are by far the most effective way to tackle breast cancer. There are more than 70 common genetic susceptibility factors included in the current non-image-based risk prediction models (e.g., the Gail and the Tyrer-Cuzick models). Image-based risk factors, such as mammographic densities and parenchymal patterns, have been established as biomarkers but have not been fully incorporated in the risk prediction models used for risk stratification in screening and/or measuring responsiveness to preventive approaches. Within computer aided mammography, automatic mammographic tissue segmentation methods have been developed for estimation of breast tissue composition to facilitate mammographic risk assessment. This paper presents a comprehensive review of automatic mammographic tissue segmentation methodologies developed over the past two decades and the evidence for risk assessment/density classification using segmentation. The aim of this review is to analyse how engineering advances have progressed and the impact automatic mammographic tissue segmentation has in a clinical environment, as well as to understand the current research gaps with respect to the incorporation of image-based risk factors in non-image-based risk prediction models.Clinical evaluation has indicated that SFM and FFDM are similar in their ability to detect cancer [107]; however, FFDM is more effective at finding cancer in certain groups of the population, such as women who are premenopausal or perimenopausal, under the age of 50, and have dense breasts. This indicates that in this subgroup some anatomical regions are better visualised by FFDM than SFM. In particular, FFDM demonstrated improved image quality with significantly better depiction of the nipple, skin, pectoral muscle, and especially contrast in parenchymal and fatty tissue [108]. Note that digital mammography imaging generates two types of images for analysis, raw (\u201cfor processing\u201d) and vendor postprocessed (\u201cfor presentation\u201d), of which postprocessed images are commonly used in clinical practice.', 'kwd': '-', 'title': u'A Review on Automatic Mammographic Density and Parenchymal Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5677362/', 'p': u'The segmentation of a high spatial resolution remote sensing image is a critical step in geographic object-based image analysis (GEOBIA). Evaluating the performance of segmentation without ground truth data, i.e., unsupervised evaluation, is important for the comparison of segmentation algorithms and the automatic selection of optimal parameters. This unsupervised strategy currently faces several challenges in practice, such as difficulties in designing effective indicators and limitations of the spectral values in the feature representation. This study proposes a novel unsupervised evaluation method to quantitatively measure the quality of segmentation results to overcome these problems. In this method, multiple spectral and spatial features of images are first extracted simultaneously and then integrated into a feature set to improve the quality of the feature representation of ground objects. The indicators designed for spatial stratified heterogeneity and spatial autocorrelation are included to estimate the properties of the segments in this integrated feature set. These two indicators are then combined into a global assessment metric as the final quality score. The trade-offs of the combined indicators are accounted for using a strategy based on the Mahalanobis distance, which can be exhibited geometrically. The method is tested on two segmentation algorithms and three testing images. The proposed method is compared with two existing unsupervised methods and a supervised method to confirm its capabilities. Through comparison and visual analysis, the results verified the effectiveness of the proposed method and demonstrated the reliability and improvements of this method with respect to other methods.A novel unsupervised method is proposed for evaluating the segmentation quality of VHR remote sensing images. This method uses a multidimensional spectral\u2013spatial feature set as the feature image, which is captured from a raw image using a bilateral filter and a Gabor wavelet filter. Based on this integrated feature set, q\xa0and MI, which respectively denote the spatial stratified heterogeneity and spatial autocorrelation, are computed to indicate the property of each segmentation result from different aspects. These two indicators are then combined into a single overall metric dM using a strategy of measuring the Mahalanobis distance of the quality points in the MI\xa0\u2212\xa0q\xa0space to reveal the segmentation quality. Evaluations of reference segmentation of two synthetic images and three remote sensing images indicate that applying the proposed method to a feature enhanced image yields superior results relative to the original image. The MRS and MSS segmentation algorithms with different parameters were applied to the three remote sensing images to produce multiple segmentation results for evaluation. The experimental results show that indicators q\xa0 and MI appropriately reflect the changes at different segmentation scales, and the combined metric dM clearly reveals the segmentation quality when applied to different algorithms and different parameters. The effectiveness of the combined metric, dM,\xa0 is further demonstrated by comparing two existing unsupervised measures and one supervised method. The results demonstrate the superior potential and robust performance of the proposed method.', 'kwd': u'high spatial resolution remote sensing, image segmentation, unsupervised segmentation evaluation, spatial stratified heterogeneity, statistical features', 'title': u'A Novel Unsupervised Segmentation Quality Evaluation Method for Remote Sensing Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4000389/', 'p': u'Due to rapid advances in radiation therapy (RT), especially image guidance and treatment adaptation, a fast and accurate segmentation of medical images is a very important part of the treatment. Manual delineation of target volumes and organs at risk is still the standard routine for most clinics, even though it is time consuming and prone to intra- and interobserver variations. Automated segmentation methods seek to reduce delineation workload and unify the organ boundary definition. In this paper, the authors review the current autosegmentation methods particularly relevant for applications in RT. The authors outline the methods\u2019 strengths and limitations and propose strategies that could lead to wider acceptance of autosegmentation in routine clinical practice. The authors conclude that currently, autosegmentation technology in RT planning is an efficient tool for the clinicians to provide them with a good starting point for review and adjustment. Modern hardware platforms including GPUs allow most of the autosegmentation tasks to be done in a range of a few minutes. In the nearest future, improvements in CT-based autosegmentation tools will be achieved through standardization of imaging and contouring protocols. In the longer term, the authors expect a wider use of multimodality approaches and better understanding of correlation of imaging with biology and pathology.Among the methods which do not use prior-knowledge for segmentation, region based methods such as adaptive thresholding1 and graph cuts2 as well as edge detection based methods, e.g., watershed segmentation3 have been used for radiotherapy planning.4, 5 Moreover, different types of deformable models6 such as geodesic active contours7 have been applied.8 In order to take advantage of the flexibility of these methods and at the same time compensate their deficiency of using prior-knowledge, especially approaches based on graph cuts and deformable models are often used in combination with atlas- or model-based segmentation methods in hybrid approaches (see Sec. 2C).', 'kwd': u'segmentation, radiation therapy, image processing', 'title': u'Vision 20/20: Perspectives on automated image segmentation for radiotherapy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537099/', 'p': u'Pretreatment risk stratification is key for personalized medicine. While many physicians rely on an \u201ceyeball test\u201d to assess whether patients will tolerate major surgery or chemotherapy, \u201ceyeballing\u201d is inherently subjective and difficult to quantify. The concept of morphometric age derived from cross-sectional imaging has been found to correlate well with outcomes such as length of stay, morbidity, and mortality. However, the determination of the morphometric age is time intensive and requires highly trained experts. In this study, we propose a fully automated deep learning system for the segmentation of skeletal muscle cross-sectional area (CSA) on an axial computed tomography image taken at the third lumbar vertebra. We utilized a fully automated deep segmentation model derived from an extended implementation of a fully convolutional network with weight initialization of an ImageNet pre-trained model, followed by post processing to eliminate intramuscular fat for a more accurate analysis. This experiment was conducted by varying window level (WL), window width (WW), and bit resolutions in order to better understand the effects of the parameters on the model performance. Our best model, fine-tuned on 250 training images and ground truth labels, achieves 0.93\xa0\xb1\xa00.02 Dice similarity coefficient (DSC) and 3.68\xa0\xb1\xa02.29% difference between predicted and ground truth muscle CSA on 150 held-out test cases. Ultimately, the fully automated segmentation system can be embedded into the clinical environment to accelerate the quantification of muscle and expanded to volume analysis of 3D datasets.We reformatted the manually tuned muscle segmentation maps created by domain experts as described previously into acceptable input for convolutional neural networks (CNN). As shown in Fig. \u200bFig.1,1, the axial images and their corresponding color-coded images served as original input data and ground truth labels, respectively. The main challenge for muscle segmentation is the accurate differentiation of muscle tissue from neighboring organs due to their overlapping HU ranges. We manually drew a boundary between organs and muscle, setting the inside region as additional segmentation class (\u201cInside\u201d) in an effort to train the neural network to learn distinguishing features of muscle for a precise segmentation from adjacent organs. The color-coded label images were assigned to pre-defined label indices, including 0 (black) for \u201cBackground\u201d, 1 (red) for \u201cMuscle\u201d, and 2 (green) for \u201cInside\u201d, before passing through CNNs for training as presented in Fig. \u200bFig.11.\n', 'kwd': u'Muscle segmentation, Convolutional neural networks, Computer-aided diagnosis (CAD), Computed tomography, Artificial intelligence, Deep learning', 'title': u'Pixel-Level Deep Segmentation: Artificial Intelligence Quantifies Muscle on Computed Tomography for Body Morphometric Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3165069/', 'p': u'Due to the non-uniform staining, and especially, the varying contrast between the nuclei and extra-cellular regions accross different sections (caused by inconsistent illumination), histopathological images usually exhibit non-consistent colors (e.g., see the five training images in the first row of Fig.1). Therefore, it is desirable to normalize the contrast across different section images. To do this, we transform the original section images by applying histogram equalization to each channel of the RGB color space. The second row corresponds to the transformed training images, which show more consistent lighting conditions across different section images than the original ones. From each of the five transformed images, we manually mark 150 locations in the nuclei and extracellular regions, respectively. An 11 \xd7 11 local neighborhood at each marked location is cropped as a training image patch. Therefore, a total of 750 positive (corresponding to nuclei regions) and 750 negative (corresponding to extra-cellular regions) training patches are created. The third column of Fig.1 shows these patches (darker corresponding to positive and lighter corresponding to negative).We assume that cells generally have elliptical shapes. Although cells have smoother boundaries, the images obtained from previous segmentation step are often characterized by irregular and undulated contours as shown in the first image of Fig.9. To reduce those irregularities, we use Fourier shape descriptors to smooth out the boundaries of the segmented regions [47]. Fourier descriptors provide a powerful mathematical framework to reduce the irregularities of shape boundaries by eliminating high order frequencies, which usually are representative of those irregularities. The fourth image of Fig.9 shows an example of Fourier shape smoothing of the touching cells (the first 12 harmonic components are retained for smoothing).', 'kwd': u'Histopathological image segmentation, touching-cell splitting, supervised learning, color-texture feature extraction, local fourier transform, discriminant analysis, radial-symmetry point, follicular lymphoma', 'title': u'Partitioning Histopathological Images: An Integrated Framework for Supervised Color-Texture Segmentation and Cell Splitting'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891256/', 'p': u'Accurate representation of myocardial infarct geometry is crucial to patient-specific computational modeling of the heart in ischemic cardiomyopathy. We have developed a methodology for segmentation of left ventricular (LV) infarct from clinically acquired, two-dimensional (2D), late-gadolinium enhanced cardiac magnetic resonance (LGE-CMR) images, for personalized modeling of ventricular electrophysiology. The infarct segmentation was expressed as a continuous min-cut optimization problem, which was solved using its dual formulation, the continuous max-flow (CMF). The optimization objective comprised of a smoothness term, and a data term that quantified the similarity between image intensity histograms of segmented regions and those of a set of training images. A manual segmentation of the LV myocardium was used to initialize and constrain the developed method. The three-dimensional geometry of infarct was reconstructed from its segmentation using an implicit, shape-based interpolation method. The proposed methodology was extensively evaluated using metrics based on geometry, and outcomes of individualized electrophysiological simulations of cardiac dys(function). Several existing LV infarct segmentation approaches were implemented, and compared with the proposed method. Our results demonstrated that the CMF method was more accurate than the existing approaches in reproducing expert manual LV infarct segmentations, and in electrophysiological simulations. The infarct segmentation method we have developed and comprehensively evaluated in this study constitutes an important step in advancing clinical applications of personalized simulations of cardiac electrophysiology.The workflow of our methodology for segmentation and 3D reconstruction of LV infarcts from clinically acquired SAX LGE-CMR images is illustrated in Fig. 1. Given an image, the epi- and endo-cardial boundaries of the LV were manually contoured in the image slices by an expert. The infarct was then segmented using the CMF method, for which the LV myocardium was used as the region of interest and the initialization region. We implemented two different versions of the CMF algorithm, namely a 2D approach, where each slice was segmented independently, and a 3D approach (CMF3D), where the entire stack of slices was segmented at once by means of an intermediate image with isotropic resolution that was created using nearest-neighbor interpolation method. Finally, the 3D geometry of the infarct was reconstructed from the infarct segmentations using an interpolation technique we developed based on LogOdds. Subsections B-D below describe in detail the components of the pipeline shown in Fig. 1. All image processing tasks were performed in the Matlab computing environment (Mathworks Inc., Natick, MA) installed on a personal computer equipped with a 2.3 GHz Intel Core i7 CPU, 12 GB of RAM, and the Windows operating system.', 'kwd': u'Image Segmentation, Late-Gadolinium Enhanced Magnetic Resonance Imaging, Convex Optimization, Simulations of Cardiac Electrophysiology', 'title': u'Myocardial Infarct Segmentation from Magnetic Resonance Images for Personalized Modeling of Cardiac Electrophysiology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3925785/', 'p': u'An automatic segmentation framework is proposed to segment the right ventricle (RV) in echocardiographic images. The method can automatically segment both epicardial and endocardial boundaries from a continuous echocardiography series by combining sparse matrix transform, a training model, and a localized region-based level set. First, the sparse matrix transform extracts main motion regions of the myocardium as eigen-images by analyzing the statistical information of the images. Second, an RV training model is registered to the eigen-images in order to locate the position of the RV. Third, the training model is adjusted and then serves as an optimized initialization for the segmentation of each image. Finally, based on the initializations, a localized, region-based level set algorithm is applied to segment both epicardial and endocardial boundaries in each echocardiograph. Three evaluation methods were used to validate the performance of the segmentation framework. The Dice coefficient measures the overall agreement between the manual and automatic segmentation. The absolute distance and the Hausdorff distance between the boundaries from manual and automatic segmentation were used to measure the accuracy of the segmentation. Ultrasound images of human subjects were used for validation. For the epicardial and endocardial boundaries, the Dice coefficients were 90.8 \xb1 1.7% and 87.3 \xb1 1.9%, the absolute distances were 2.0 \xb1 0.42 mm and 1.79 \xb1 0.45 mm, and the Hausdorff distances were 6.86 \xb1 1.71 mm and 7.02 \xb1 1.17 mm, respectively. The automatic segmentation method based on a sparse matrix transform and level set can provide a useful tool for quantitative cardiac imaging.Some implementation details are described below. The algorithm was implemented in MATLAB language. The reproduction of SMT covariance estimation was based on the previous work (Bachega et al 2010, Cao et al 2011, Qin et al 2013). On the initialization step, the criterion for choosing the training model parameter k was that the sum of the first k eigen-values was bigger than 95%, which was 4 for our training model. Its corresponding weight parameters bi were set in the range [\u22120.5, 0.5] for the GA initialization and searching. The GA crossover and mutation parameters were 0.8 and 0.1, respectively. Its stop condition was that the fitness function kept stable for more than 20 generations or when the maximum generation reaches. On the bock matching step, the block size is 50 \xd7 50 pixels and the maximum searching range is 10 pixels. Because this step only tracked one block, it only costs 30 s for 120 continuous images. The weighted parameter \u03bb in the level set, which is related to shape prior term, was set as 0.4, because RV segmentation in 2D echocardiography needed more shape prior to avoid the poorer imaging quality.', 'kwd': '-', 'title': u'Automatic segmentation of right ventricular ultrasound images using sparse matrix transform and a level set'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5469297/', 'p': u'The Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) is an ongoing, longitudinal, multicenter study designed to develop clinical, imaging, genetic, and biochemical biomarkers for the early detection and tracking of Alzheimer\u2019s disease (AD). The initial study, ADNI-1, enrolled 400 subjects with early mild cognitive impairment (MCI), 200 with early AD, and 200 cognitively normal elderly controls. ADNI-1 was extended by a 2-year Grand Opportunities grant in 2009 and by a competitive renewal, ADNI-2, which enrolled an additional 550 participants and will run until 2015. This article reviews all papers published since the inception of the initiative and summarizes the results to the end of 2013. The major accomplishments of ADNI have been as follows: (1) the development of standardized methods for clinical tests, magnetic resonance imaging (MRI), positron emission tomography (PET), and cerebrospinal fluid (CSF) biomarkers in a multicenter setting; (2) elucidation of the patterns and rates of change of imaging and CSF biomarker measurements in control subjects, MCI patients, and AD patients. CSF biomarkers are largely consistent with disease trajectories predicted by \u03b2-amyloid cascade (Hardy, J Alzheimer\u2019s Dis 2006;9(Suppl 3):151\u20133) and tau-mediated neurodegeneration hypotheses for AD, whereas brain atrophy and hypometabolism levels show predicted patterns but exhibit differing rates of change depending on region and disease severity; (3) the assessment of alternative methods of diagnostic categorization. Currently, the best classifiers select and combine optimum features from multiple modalities, including MRI, [18F]-fluorodeoxyglucose-PET, amyloid PET, CSF biomarkers, and clinical tests; (4) the development of blood biomarkers for AD as potentially noninvasive and low-cost alternatives to CSF biomarkers for AD diagnosis and the assessment of \u03b1-syn as an additional biomarker; (5) the development of methods for the early detection of AD. CSF biomarkers, \u03b2-amyloid 42 and tau, as well as amyloid PET may reflect the earliest steps in AD pathology in mildly symptomatic or even nonsymptomatic subjects and are leading candidates for the detection of AD in its preclinical stages; (6) the improvement of clinical trial efficiency through the identification of subjects most likely to undergo imminent future clinical decline and the use of more sensitive outcome measures to reduce sample sizes. Multimodal methods incorporating APOE status and longitudinal MRI proved most highly predictive of future decline. Refinements of clinical tests used as outcome measures such as clinical dementia rating-sum of boxes further reduced sample sizes; (7) the pioneering of genome-wide association studies that leverage quantitative imaging and biomarker phenotypes, including longitudinal data, to confirm recently identified loci, CR1, CLU, and PICALM and to identify novel AD risk loci; (8) worldwide impact through the establishment of ADNI-like programs in Japan, Australia, Argentina, Taiwan, China, Korea, Europe, and Italy; (9) understanding the biology and pathobiology of normal aging, MCI, and AD through integration of ADNI biomarker and clinical data to stimulate research that will resolve controversies about competing hypotheses on the etiopathogenesis of AD, thereby advancing efforts to find disease-modifying drugs for AD; and (10) the establishment of infrastructure to allow sharing of all raw and processed data without embargo to interested scientific investigators throughout the world.Alzheimer\u2019s disease (AD), the most common form of dementia, is a complex disease characterized by an accumulation of \u03b2-amyloid (A\u03b2) plaques and neurofibrillary tangles composed of tau amyloid fibrils [1] associated with synapse loss and neurodegeneration leading to memory impairment and other cognitive problems. There is currently no known treatment that slows the progression of this disorder. According to the 2014 World Alzheimer report, there are an estimated 44 million people worldwide living with dementia at a total cost of more than US$600 billion in 2010, and the incidence of AD throughout the world is expected to triple by 2050. There is a pressing need to find and validate biomarkers to both predict future clinical decline and for use as outcome measures in clinical trials of disease-modifying agents to facilitate phase II\u2013III studies and foster the development of innovative drugs [2]. To this end, Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) was conceived at the beginning of the millennium and began as a North American multicenter collaborative effort funded by public and private interests in October 2004. Although special issues focused on North American ADNI have been published in Alzheimer\u2019s and Dementia [3] and Neurobiology of Aging [4] in addition to a number of other review articles [5\u201312], the purpose of this review is to provide a detailed and comprehensive overview of the approximately 500 papers that have been published as a direct result of ADNI to the end of 2013. The original review [350] covered approximately 200 papers to the end of 2010. The first update [351] detailed an additional 150 papers published from 2011 to mid-2012, and this material is highlighted in yellow. The current iteration adds around 200 more publications from mid-2012 to the end of 2013, and these are highlighted in green. To mid-2014, an additional 70 publications indicate the continuing impact of ADNI.', 'kwd': u'Alzheimer\u2019s disease, Mild cognitive impairment, Amyloid, Tau, Biomarker', 'title': u'2014 Update of the Alzheimer\u2019s Disease Neuroimaging Initiative: A review of papers published since its inception'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2975631/', 'p': u'Conceived and designed the experiments: REL NAP PN. Performed the experiments: XY YZ SD. Analyzed the data: XY YZ REL SD NAP. Wrote the paper: XY YZ SD NAP. Critically reviewed the paper: REL PN.Most extremely preterm newborns exhibit cerebral atrophy/growth disturbances and white matter signal abnormalities on MRI at term-equivalent age. MRI brain volumes could serve as biomarkers for evaluating the effects of neonatal intensive care and predicting neurodevelopmental outcomes. This requires detailed, accurate, and reliable brain MRI segmentation methods. We describe our efforts to develop such methods in high risk newborns using a combination of manual and automated segmentation tools. After intensive efforts to accurately define structural boundaries, two trained raters independently performed manual segmentation of nine subcortical structures using axial T2-weighted MRI scans from 20 randomly selected extremely preterm infants. All scans were re-segmented by both raters to assess reliability. High intra-rater reliability was achieved, as assessed by repeatability and intra-class correlation coefficients (ICC range: 0.97 to 0.99) for all manually segmented regions. Inter-rater reliability was slightly lower (ICC range: 0.93 to 0.99). A semi-automated segmentation approach was developed that combined the parametric strengths of the Hidden Markov Random Field Expectation Maximization algorithm with non-parametric Parzen window classifier resulting in accurate white matter, gray matter, and CSF segmentation. Final manual correction of misclassification errors improved accuracy (similarity index range: 0.87 to 0.89) and facilitated objective quantification of white matter signal abnormalities. The semi-automated and manual methods were seamlessly integrated to generate full brain segmentation within two hours. This comprehensive approach can facilitate the evaluation of large cohorts to rigorously evaluate the utility of regional brain volumes as biomarkers of neonatal care and surrogate endpoints for neurodevelopmental outcomes.', 'kwd': '-', 'title': u'Comprehensive Brain MRI Segmentation in High Risk Preterm Newborns'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3621376/', 'p': u'T2-weighted cardiovascular magnetic resonance (CMR) is clinically-useful for imaging the ischemic area-at-risk and amount of salvageable myocardium in patients with acute myocardial infarction (MI). However, to date, quantification of oedema is user-defined and potentially subjective.We describe a highly automatic framework for quantifying myocardial oedema from bright blood T2-weighted CMR in patients with acute MI. Our approach retains user input (i.e. clinical judgment) to confirm the presence of oedema on an image which is then subjected to an automatic analysis. The new method was tested on 25 consecutive acute MI patients who had a CMR within 48\xa0hours of hospital admission. Left ventricular wall boundaries were delineated automatically by variational level set methods followed by automatic detection of myocardial oedema by fitting a Rayleigh-Gaussian mixture statistical model. These data were compared with results from manual segmentation of the left ventricular wall and oedema, the current standard approach.', 'kwd': u'Myocardial oedema, Bright blood T2-weighted CMR, Rayleigh-Gaussian mixture model, Level set', 'title': u'Highly automatic quantification of myocardial oedema in patients with acute myocardial infarction using bright blood T2-weighted CMR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5145140/', 'p': u'To study the interscan reproducibility of manual versus automated segmentation of carotid artery plaque components, and the agreement between both methods, in high and lower quality MRI scans.24 patients with 30\u201370% carotid artery stenosis were planned for 3T carotid MRI, followed by a rescan within 1 month. A multicontrast protocol (T1w,T2w, PDw and TOF sequences) was used. After co-registration and delineation of the lumen and outer wall, segmentation of plaque components (lipid-rich necrotic cores (LRNC) and calcifications) was performed both manually and automated. Scan quality was assessed using a visual quality scale.', 'kwd': '-', 'title': u'Manual versus Automated Carotid Artery Plaque Component Segmentation in High and Lower Quality 3.0 Tesla MRI Scans'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4981618/', 'p': u'Brain magnetic resonance imaging provides detailed information which can be used to detect and segment white matter lesions (WML). In this work we propose an approach to automatically segment WML in Lupus patients by using T1w and fluid-attenuated inversion recovery (FLAIR) images. Lupus WML appear as small focal abnormal tissue observed as hyperintensities in the FLAIR images. The quantification of these WML is a key factor for the stratification of lupus patients and therefore both lesion detection and segmentation play an important role. In our approach, the T1w image is first used to classify the three main tissues of the brain, white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF), while the FLAIR image is then used to detect focal WML as outliers of its GM intensity distribution. A set of post-processing steps based on lesion size, tissue neighborhood, and location are used to refine the lesion candidates. The proposal is evaluated on 20 patients, presenting qualitative, and quantitative results in terms of precision and sensitivity of lesion detection [True Positive Rate (62%) and Positive Prediction Value (80%), respectively] as well as segmentation accuracy [Dice Similarity Coefficient (72%)]. Obtained results illustrate the validity of the approach to automatically detect and segment lupus lesions. Besides, our approach is publicly available as a SPM8/12 toolbox extension with a simple parameter configuration.This study included 20 Lupus patients. The brain MRIs were performed between 2014 and 2015 at Hospital Cl\xednic, University of Barcelona, Spain, the main national referral institution for lupus. All scans were performed at three Tesla Siemens MAGNETOM TIM Trio scanner, using a 32-channel head coil, with the same protocol including 3D T1 and 3D FLAIR, with a voxel size = 1 \xd7 1 \xd7 1mm3. The lesions were semiautomatically annotated on FLAIR images by an expert neuroradiologist. They present a lesion volume mean and range (min-max) per patient of 0.217 [11\u22121459] mm3. This study was carried out in accordance with the ethical recommendations of the Hospital Cl\xednic committee (IDIBAPS, Barcelona), with written informed consent from all subjects.', 'kwd': u'magnetic resonance images, lupus disease, image analysis, automatic lesion detection and segmentation', 'title': u'Automated Detection of Lupus White Matter Lesions in MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3416877/', 'p': u'This study was in compliance with the Health Insurance Portability and Accountability Act (HIPAA) and received institutional review board (IRB) approval. Anonymized full-field DM images acquired as part of a separate IRB-approved multimodality breast cancer screening trial previously completed in our department (July 2007\u2013March 2008) were retrospectively analyzed. Women who participated in the trial were asymptomatic volunteers who presented for annual screening mammography and had given written informed consent before their participation. Bilateral, MLO view DM imaging was performed using a full-field digital mammography unit (Senographe DS; GE Healthcare, Chalfont St Giles, UK). The raw digital mammograms were acquired at a 100 \u03bcm isotropic resolution using a 14-bit gray-level depth. The raw mammograms were processed using PremiumView\u2122 (GE Healthcare), a vendor-specific, embedded adaptive histogram equalization algorithm41, 42 which produces 12-bit gray-level postprocessed images. Of the 83 women originally enrolled in the trial, two were excluded from this analysis: one due to a diagnosis of breast cancer, the other due to insufficient image quality. The remaining 81 women were, therefore, available for retrospective breast density analysis.Area-based breast PD% was estimated by a trained breast-imaging radiologist for these 81 women on a per-breast basis using a validated, interactive, image-thresholding tool for breast PD% estimation (Cumulus, Ver. 4.0, Univ. Toronto) (Ref. 43) in both the raw and processed images, for a total of 324 digital mammograms. As a result of the different visualization of the breast tissue between raw and postprocessed images, tailored approaches were adopted for the estimation breast PD%.39 Briefly, for the postprocessed DM images, the digital mammograms were first windowed by the radiologist for optimal display. Following this, the background air region was excluded via a manually determined intensity threshold, therefore, allowing the breast boundary to be designated. The pectoral muscle region was subsequently excluded via manual delineation of the pectoral muscle edge. The remaining portion of the image was designated as the breast tissue, and the total area of this region is computed by the software. Following identification of the breast, a second, user-defined gray-level intensity threshold is selected in order to define the gray-level cut-off between fibroglandular and adipose tissue, and those pixels within the delineated breast region above this gray-level threshold are designated as fibroglandular tissue. PD% is then computed as the percentage of the breast area occupied by fibroglandular tissue. A similar process is used to estimate PD% from raw images, except that since the raw images are not optimized for clinical visualization and interpretation, the digital mammographic image were rewindowed by the radiologist before each of the segmentation and thresholding steps described above. Two readings per image were performed by the radiologist, each six months apart, and the average of the two reading was considered as our gold-standard in order to minimize the effects of intrareader variability. The intrareader correlation and 95% confidence interval (CI) between the two radiologist PD% readings was found to be r = 0.95 (p < 0.001; CI: 0.93\u20130.97) for the raw DM images and r = 0.93 (p < 0.001; CI: 0.90\u20130.95) for the processed DM images. Table \u200bTable11 provides the distribution of BIRADs density categories assigned to the 324 DM images in this dataset.', 'kwd': u'digital mammography, breast density, breast cancer risk estimation, quantitative imaging', 'title': u'Estimation of breast percent density in raw and processed full field digital mammography images via adaptive fuzzy c-means clustering and support vector machine segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4745818/', 'p': u"This paper presents a method for the automatic 3D segmentation of the ascending aorta from coronary computed tomography angiography (CCTA). The segmentation is performed in three steps. First, the initial seed points are selected by minimizing a newly proposed energy function across the Hough circles. Second, the ascending aorta is segmented by geodesic distance transformation. Third, the seed points are effectively transferred through the next axial slice by a novel transfer function. Experiments are performed using a database composed of 10 patients' CCTA images. For the experiment, the ground truths are annotated manually on the axial image slices by a medical expert. A comparative evaluation with state-of-the-art commercial aorta segmentation algorithms shows that our approach is computationally more efficient and accurate under the DSC (Dice Similarity Coefficient) measurements.In this paper, we developed a method for the fast and fully automatic extraction of the ascending aorta from CCTA images. The method incorporates two separated algorithms, a circular Hough transformation for initialization and the Geodesic distance algorithm for segmentation. The circular Hough transformation algorithm could automatically find the most probable ascending aortic circle in the axial image. The initial seeding was found, and the VOI was defined after detecting the aortic circle. Then, the boundary of the ascending aorta was repeatedly delineated slice by slice by the geodesic distance algorithm until the aortic arch or valve was reached or until the ascending aorta was completely segmented. The proposed method was evaluated and compared with the results from other commercial workstations using 10 data sets. For all of the datasets, the proposed method segmented the ascending aorta successfully. The proposed method showed a highly accurate DSC with ground truth, and high performance regarding the computation time was measured. Future work will focus on the segmentation of the aortic valve to overcome the aforementioned limitations mentioned in Section 4.", 'kwd': '-', 'title': u'Geodesic Distance Algorithm for Extracting the Ascending Aorta from 3D CT Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5355004/', 'p': u'One in six men will develop prostate cancer in his life time. Early\ndetection and accurate diagnosis of the disease can improve cancer survival and\nreduce treatment costs. Recently, imaging of prostate cancer has greatly\nadvanced since the introduction of multi-parametric magnetic resonance imaging\n(mp-MRI). Mp-MRI consists of T2-weighted sequences combined with functional\nsequences including dynamic contrast-enhanced MRI, diffusion-weighted MRI, and\nMR spectroscopy imaging. Due to the big data and variations in imaging\nsequences, detection can be affected by multiple factors such as observer\nvariability and visibility and complexity of the lesions. In order to improve\nquantitative assessment of the disease, various computer-aided detection systems\nhave been designed to help radiologists in their clinical practice. This review\npaper presents an overview of literatures on computer-aided detection of\nprostate cancer with mp-MRI, which include the technology and its applications.\nThe aim of the survey is threefold: an introduction for those new to the field,\nan overview for those working in the field, and a reference for those searching\nfor literature on a specific application.The functional MR imaging data, like DCE-MRI and MRS, are more complex\nand larger in amounts than anatomic MR imaging. There are clinical needs to\ndevelop fast, cost-effective, supportive techniques, such as computer-aided\nanalysis tools, for easy and more reproducible diagnosis of prostate cancer.\nResearchers have focused on developing CADx methodology for automated prostate\nMRS classification and DCE-MRI analysis. Because all functional MR imaging\ntechniques have their strengths and shortcomings, single technique cannot\nadequately detect and characterize PCa. The combination of anatomic (T2W) images\nand functional techniques has been shown to increase the accuracy of MR imaging\nfor diagnosis of PCa. Table 1 compares\nthe performance of the major published prostate CADx systems [13, 14, 16\u201318, 22, 26, 27, 36, 37, 39, 51, 52, 54\u201357, 62, 122, 129, 140\u2013151]. Chan et\nal. were one of the first groups who implemented an mp-MRI CADx system for the\ndiagnosis of prostate cancer [122]. In\ntheir approach they used line-scan diffusion, T2 and T2-weighted images to\nidentify predefined areas of the peripheral zone of the prostate for the\npresence of prostate cancer. Viswanath et al. [129] present an mp-MRI CADx system for PCa detection by integrating\nfunctional and structural information obtained via DCE and T2W MRI. Liu et al.\n[141] present fuzzy MRF models for\nprostate cancer detection of multispectral MR prostate images. Tiwari et al.\n[55] investigated the use of MR\nspectroscopy in combination with T2W MRI to identify the voxels that are\naffected by prostate cancer. They also introduced the use of wavelet embedding\nto map MRS and T2-W texture features into a common space. In a study by Peng et\nal. [27], the combination of\n10th percentile ADC, average ADC, and T2-weighted skewness with\nCADx yielded an AUC value of 0.95 in differentiating prostate cancer from normal\ntissue. The combination achieved higher accuracy than any MR parameter alone. In\na more recent study by Litjens et al [62], they developed a fully automated computer-aided detection system\nwhich consists of two stages. The first (detection) stage consists of\nsegmentation of the prostate on the transversal T2W MRI, extraction of voxel\nfeatures from the image volumes, classification of the voxels and candidate\nselection. The second (diagnosis) stage consists of candidate segmentation,\ncandidate feature extraction and candidate classification. The system was\nevaluated on a large consecutive cohort of 347 patients, and yielded an AUC\nvalue of 0.889.', 'kwd': u'Prostate cancer, MR imaging, Image Quantification, Computer-aided Detection', 'title': u'Computer-Aided Detection of Prostate Cancer with MRI: Technology and\nApplications'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4263651/', 'p': u'Author contributions: Guarantors of integrity of entire study, B.M.E., R.J.H.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; approval of final version of submitted manuscript, all authors; literature research, B.M.E., D.C.W., W.B.P., T.F.C.; clinical studies, B.M.E., H.J.K., D.C.W., W.B.P., J.N.C., R.J.H., A.L., P.L.N., T.F.C.; statistical analysis, B.M.E., H.J.K., T.F.C.; and manuscript editing, B.M.E., H.J.K., W.B.P., A.L., P.L.N., T.F.C.Multivariate survival analyses, visual observations, and receiver operating characteristic comparisons helped demonstrate that, compared with conventional segmentation, contrast-enhanced T1-weighted subtraction maps of contrast-enhancing lesions produce better stratification of high- and low-risk patients treated with bevacizumab and therefore should be used in future clinical trials involving evaluation of antiangiogenic therapies in brain tumors.', 'kwd': '-', 'title': u'Recurrent Glioblastoma Treated with Bevacizumab: Contrast-enhanced T1-weighted Subtraction Maps Improve Tumor Delineation and Aid Prediction of Survival in a Multicenter Clinical Trial'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3925465/', 'p': u'The University of Washington Animal Care Committee approved the experimental protocol. Five male Sprague-Dawley rats weighing between 244 and 276 g were anesthetized by intraperitoneal injection of 80 mg/kg ketamine and 10 mg/kg xylazine, sufficient to prevent withdrawal of paw after pinch. A tracheostomy was performed, and an internal jugular vein was cannulated. The animals were mechanically ventilated at a rate of 50 breaths/min with a tidal volume of approximately 3 mL, using a piston pump ventilator. The rats were then deeply anesthetized with an intraperitoneal injection of ketamine/xylazine; their chests widely opened, and exsanguinated. The pulmonary artery was cannulated; the main aorta tied off, and blood was injected to fill the vasculature (1.3 \xb1 0.5 mL). The lungs were removed from the chest, filled via the trachea with Optimal Cutting Temperature media (OCT, Sakura Finetek Inc., Torrance, CA) until they appeared inflated to total lung capacity (10.5\xb10.7 mL) and then frozen. The frozen lung was surrounded by a mixture of 99.25% OCT and 0.75% India ink and returned to the freezer. The ink was used to create a contrast between the lung parenchyma and the surrounding OCT.The serial block-face imaging cryomicrotome (Barlow Scientific, Inc. Olympia WA) is a computer driven instrument that acquires multi-spectral digital images of en face tissue blocks that are serially sliced. It enables the three-dimensional imaging of fluorescence and fluorescent microspheres at a microscopic level. The instrument has previously been described in detail [2], but has been upgraded recently. The instrument comprises a cryostatic microtome, a Redlake MegaPlus II ES3200 camera (San Diego CA) with a resolution of 2184 \xd7 1472 pixels, a Micro-Nikkor 200 mm f/4D IFED lens (Nikon Corp, Tokyo Japan), a metal halide lamp (PE300BF Cermax, Excelitas Technologies, Fremont CA), a set of excitation/emission filters that allow isolation of specific spectral channels, and a computer controlling the device and storing images. The microtome serially sections thin slices of a frozen tissue block of up to 54\xd736 mm in size along the x- and y-direction. Following every cut, images of the tissue block are captured with different pairings of excitation/emission filters. Note that the tissue slices themselves are not utilized and are discarded. The sequence of two-dimensional images results in a three-dimensional image with potentially several spectral channels for each voxel.', 'kwd': u'airway segmentation, rat lung, serial block-face imaging cryomicrotome', 'title': u'Airway Tree Segmentation in Serial Block-Face Cryomicrotome Images of Rat Lungs'}]}