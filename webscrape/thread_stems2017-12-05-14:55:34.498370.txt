{'Tomography AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5153359/', 'p': u'The VESSEL12 (VESsel SEgmentation in the Lung) challenge objectively compares the performance of different algorithms to identify vessels in thoracic computed tomography (CT) scans. Vessel segmentation is fundamental in computer aided processing of data generated by 3D imaging modalities. As manual vessel segmentation is prohibitively time consuming, any real world application requires some form of automation. Several approaches exist for automated vessel segmentation, but judging their relative merits is difficult due to a lack of standardized evaluation. We present an annotated reference dataset containing 20 CT scans and propose nine categories to perform a comprehensive evaluation of vessel segmentation algorithms from both academia and industry. Twenty algorithms participated in the VESSEL12 challenge, held at International Symposium on Biomedical Imaging (ISBI) 2012. All results have been published at the VESSEL12 website http://vessel12.grand-challenge.org. The challenge remains ongoing and open to new participants. Our three contributions are: (1) an annotated reference dataset available online for evaluation of new algorithms; (2) a quantitative scoring system for objective comparison of algorithms; and (3) performance analysis of the strengths and weaknesses of the various vessel segmentation methods in the presence of various lung diseases.The aim of VESSEL12 Challenge, organized in conjunction with the International Symposium on Biomedical Imaging 2012 (ISBI\u201912), is to provide a public platform to compare the performance of different segmentation algorithms to identify lung vessels in thoracic computed tomography (CT) data. An additional goal is to characterize what kind of anatomical neighborhood may complicate vessel segmentation, for example the presence of nodules, dense consolidation and parenchymal or bronchial abnormalities.', 'kwd': u'Thoracic computed tomography, Lung vessels, Algorithm comparison, Segmentation, Challenge', 'title': u'Comparing algorithms for automated vessel segmentation in computed tomography scans of the lung: the VESSEL12 study'}], 'Arterial Coronary Syndrome AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}], 'Patient Assessment AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5219634/', 'p': u'Amyotrophic lateral sclerosis (ALS) is a progressive neuromuscular disease, with large variation in survival between patients. Currently, it remains rather difficult to predict survival based on clinical parameters alone. Here, we set out to use clinical characteristics in combination with MRI data to predict survival of ALS patients using deep learning, a machine learning technique highly effective in a broad range of big-data analyses. A group of 135 ALS patients was included from whom high-resolution diffusion-weighted and T1-weighted images were acquired at the first visit to the outpatient clinic. Next, each of the patients was monitored carefully and survival time to death was recorded. Patients were labeled as short, medium or long survivors, based on their recorded time to death as measured from the time of disease onset. In the deep learning procedure, the total group of 135 patients was split into a training set for deep learning (n\xa0=\xa083 patients), a validation set (n\xa0=\xa020) and an independent evaluation set (n\xa0=\xa032) to evaluate the performance of the obtained deep learning networks. Deep learning based on clinical characteristics predicted survival category correctly in 68.8% of the cases. Deep learning based on MRI predicted 62.5% correctly using structural connectivity and 62.5% using brain morphology data. Notably, when we combined the three sources of information, deep learning prediction accuracy increased to 84.4%. Taken together, our findings show the added value of MRI with respect to predicting survival in ALS, demonstrating the advantage of deep learning in disease prognostication.Each of the 135 patients was categorized according to the true survival time (i.e. time between disease onset and death): short survivors with survival up to 25\xa0months after disease onset, medium survivors with survival between 25 and 50\xa0months after disease onset, and long survivors living over 50\xa0months after disease onset (Elamin et al., 2015). The group of long survivors consisted of patients who either died after a disease duration of at least 50\xa0months or were still alive and had a disease duration of at least 50\xa0months at time of analysis.', 'kwd': u'Deep learning, Neural network, Amyotrophic lateral sclerosis, White matter connectivity, Survival, Prediction', 'title': u'Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/', 'p': u'The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements.Under a mild assumption on the activation function, a two-layer neural network with a finite number of hidden units can approximate any continuous function (63), and thus it is regarded as universal approximator. However, it is also possible to approximate complex functions to the same accuracy using a \u2018deep\u2019 architecture, i.e., more than two layers, with much fewer number of units in total (3). Hence, it is possible to reduce the number of trainable parameters, thus allowing to train with a relatively small dataset (64).', 'kwd': u'Medical image analysis, deep learning, unsupervised feature learning', 'title': u'Deep Learning in Medical Image Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869115/', 'p': u'Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name \u201cdeep patient\u201d. We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.Feature learning algorithms are usually evaluated in supervised applications to take advantage of the available manually annotated labels. Here we used the Mount Sinai data warehouse to learn the deep features and we evaluated them in predicting patient future diseases. The Mount Sinai Health System generates a high volume of structured, semi-structured and unstructured data as part of its healthcare and clinical operations, which include inpatient, outpatient and emergency room visits. Patients in the system can have as long as 12\u2009years of follow up unless they moved or changed insurance. Electronic records were completely implemented by our health system starting in 2003. The data related to patients who visited the hospital prior to 2003 was migrated to the electronic format as well but we may lack certain details of hospital visits (i.e., some diagnoses or medications may not have been recorded or transferred). The entire EHR dataset contains approximately 4.2 million de-identified patients as of March 2015, and it was made available for use under IRB approval following HIPAA guidelines. We retained all patients with at least one diagnosed disease expressed as numerical ICD-9 between 1980 and 2014, inclusive. This led to a dataset of about 1.2 million patients, with every patient having an average of 88.9 records. Then, we considered all records up to December 31, 2013 (i.e., \u201csplit-point\u201d) as training data (i.e., 34\u2009years of training information) and all the diagnoses in 2014 as testing data.', 'kwd': '-', 'title': u'Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5661078/', 'p': u'Molecular imaging enables the visualization and quantitative analysis of the alterations of biological procedures at molecular and/or cellular level, which is of great significance for early detection of cancer. In recent years, deep leaning has been widely used in medical imaging analysis, as it overcomes the limitations of visual assessment and traditional machine learning techniques by extracting hierarchical features with powerful representation capability. Research on cancer molecular images using deep learning techniques is also increasing dynamically. Hence, in this paper, we review the applications of deep learning in molecular imaging in terms of tumor lesion segmentation, tumor classification, and survival prediction. We also outline some future directions in which researchers may develop more powerful deep learning models for better performance in the applications in cancer molecular imaging.The papers on diverse applications of deep learning in different molecular imaging of cancer published from 2014 onwards were included. This review contains 25 papers and is organized according to the application of deep learning in cancer molecular imaging, including tumor lesion segmentation, cancer classification, and prediction of patient survival.  Table 1 summarizes the 13 different studies on tumor lesion segmentation, while Table 2 summarizes the 10 different studies on cancer classification. Two interesting papers on prediction of patient survival are also reviewed (Table 3). To our best knowledge, there is no previous work making such a comprehensive review on this issue. In this regard, we believe this survey can present radiologists and physicians with the application status of advanced artificial intelligent techniques in molecular images analysis and hence inspire more applications in clinical practice. Biomedical engineering researchers may also benefit from this survey by acquiring the state of the art in this field or inspiration for better models/methods in future research.', 'kwd': '-', 'title': u'Application of Deep Learning in Automated Analysis of Molecular Images in Cancer: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537095/', 'p': u'Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends.This brain tumor image segmentation challenge in conjunction with the MICCAI conference has been held annually since 2012 in order to evaluate the current state-of-the-art in automated brain tumor segmentation and compare between different methods. For this purpose, a large dataset of brain tumor MR scans and ground truth (five labels: healthy brain tissue, necrosis, edema, non-enhanced, and enhanced regions of tumors) are made publicly available. The training data has increased over the years. Currently (Brats 2015\u20132016), the training set comprises 220 subjects with high grade and 54 subjects with low-grade, and the test set comprises 53 subjects with mixed grades. All datasets have been aligned to the same anatomical template and interpolated to 1\xa0mm3 voxel resolution. Each dataset has pre-contrast T1, post contrast T1, T2, and T2 FLAIR MRI volumes. The co-registered, skull-stripped, and annotated training dataset and evaluation results of algorithms are available via the Virtual Skeleton Database (https://www.virtualskeleton.ch/).', 'kwd': u'Deep learning, Quantitative brain MRI, Convolutional neural network, Brain lesion segmentation', 'title': u'Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5651626/', 'p': u'Myelin imaging is a form of quantitative magnetic resonance imaging (MRI) that measures myelin content and can potentially allow demyelinating diseases such as multiple sclerosis (MS) to be detected earlier. Although focal lesions are the most visible signs of MS pathology on conventional MRI, it has been shown that even tissues that appear normal may exhibit decreased myelin content as revealed by myelin-specific images (i.e., myelin maps). Current methods for analyzing myelin maps typically use global or regional mean myelin measurements to detect abnormalities, but ignore finer spatial patterns that may be characteristic of MS. In this paper, we present a machine learning method to automatically learn, from multimodal MR images, latent spatial features that can potentially improve the detection of MS pathology at early stage. More specifically, 3D image patches are extracted from myelin maps and the corresponding T1-weighted (T1w) MRIs, and are used to learn a latent joint myelin-T1w feature representation via unsupervised deep learning. Using a data set of images from MS patients and healthy controls, a common set of patches are selected via a voxel-wise t-test performed between the two groups. In each MS image, any patches overlapping with focal lesions are excluded, and a feature imputation method is used to fill in the missing values. A feature selection process (LASSO) is then utilized to construct a sparse representation. The resulting normal-appearing features are used to train a random forest classifier. Using the myelin and T1w images of 55 relapse-remitting MS patients and 44 healthy controls in an 11-fold cross-validation experiment, the proposed method achieved an average classification accuracy of 87.9% (SD\xa0=\xa08.4%), which is higher and more consistent across folds than those attained by regional mean myelin (73.7%, SD\xa0=\xa013.7%) and T1w measurements (66.7%, SD\xa0=\xa010.6%), or deep-learned features in either the myelin (83.8%, SD\xa0=\xa011.0%) or T1w (70.1%, SD\xa0=\xa013.6%) images alone, suggesting that the proposed method has strong potential for identifying image features that are more sensitive and specific to MS pathology in normal-appearing brain tissues.A cohort of 55 relapsing-remitting MS (RRMS) patients and a cohort of 44 age- and gender-matched normal control (NC) subjects were included in this study. The median age and range for both groups were 45 and 30\u201360. For the RRMS patients, 63.6% (35/55) were female, and 63.5% (28/44) of the NC subjects were female. The McDonald 2010 criteria (Polman et al., 2011) were used to diagnose the patients for MS. All patients underwent a neurological assessment and were scored on the Expanded Disability Status Scale (EDSS) (Kurtzke, 1983). The median EDSS and range were 4 and 0\u20135. Informed consent from each participant and ethical approval by the local ethics committee were obtained prior to the study.', 'kwd': u'Deep learning, Multiple sclerosis, Myelin water imaging, Machine learning, Magnetic resonance imaging', 'title': u'Deep learning of joint myelin and T1w MRI features in normal-appearing brain tissue to distinguish between multiple\xa0sclerosis\xa0patients and healthy controls'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5380996/', 'p': u'Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.The dataset consisted of 74 whole-slide images of breast tumour resection samples which either retrieved from the AstraZeneca BioBank or acquired from a commercial provider (Dako Denmark A/S). Slides were obtained by cutting formalin-fixed, paraffin embedded human breast cancer samples into 4\u2009\u03bcm-thick sections, stained by IHC for HER2 demonstration (monoclonal Rabbit Anti-Human HER2 antibody, Dako Denmark A/S) and counterstained with haematoxylin using a Dako Autostainer Link48 (Dako Denmark A/S). Slides were digitized with an Aperio ScanScope whole-slide imaging microscope (Aperio, Leica Biosystems Imaging, Inc.) at a resolution of 0.49\u2009\u03bcm/pixel. The slides were reviewed to confirm the presence of invasive carcinoma and a total of 71 invasive carcinoma cases were selected for the study.', 'kwd': '-', 'title': u'Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431941/', 'p': u'Precision medicine approaches rely on obtaining precise knowledge of the true state of health of an individual patient, which results from a combination of their genetic risks and environmental exposures. This approach is currently limited by the lack of effective and efficient non-invasive medical tests to define the full range of phenotypic variation associated with individual health. Such knowledge is critical for improved early intervention, for better treatment decisions, and for ameliorating the steadily worsening epidemic of chronic disease. We present proof-of-concept experiments to demonstrate how routinely acquired cross-sectional CT imaging may be used to predict patient longevity as a proxy for overall individual health and disease status using computer image analysis techniques. Despite the limitations of a modest dataset and the use of off-the-shelf machine learning methods, our results are comparable to previous \u2018manual\u2019 clinical methods for longevity prediction. This work demonstrates that radiomics techniques can be used to extract biomarkers relevant to one of the most widely used outcomes in epidemiological and clinical research \u2013 mortality, and that deep learning with convolutional neural networks can be usefully applied to radiomics research. Computer image analysis applied to routinely collected medical images offers substantial potential to enhance precision medicine initiatives.We created a predictive model using multivariable survival analysis (Cox regression). The model was informed by the top 5 covariates selected by minimum redundancy-maximum relevance feature selection50. These covariates were standardised, and the resulting risk score was dichotomised at the mean to create high-risk and low-risk phenotypes. Table\xa02 shows the 5-year mortality rate for high and low risk phenotypes, and the related Kaplan-Meier curves are presented in Fig.\xa02. The difference between the survival curves for of the high and low risk phenotypes is highly significant (p\u2009<\u20090.00005). The distribution of the raw mortality phenotype scores among cases and controls are presented in a box and whisker plot in Supplemental Figure\xa02.\n\n', 'kwd': '-', 'title': u'Precision Radiology: Predicting longevity using feature engineering and deep learning methods in a radiomics framework'}], 'Coronary Artery Disease AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615055/', 'p': u'957 rest/stress 99mtechnetium gated MPS NC studies from 623 consecutive patients with correlating invasive coronary angiography and 334 with low likelihood of CAD (LLK < 5% ) were assessed. Patients with stenosis \u2265 50% in left main or \u2265 70% in all other vessels were considered abnormal. Total perfusion deficit (TPD) was computed automatically. In addition, ischemic changes (ISCH) and ejection fraction changes (EFC) between stress and rest were derived by quantitative software. The SVM was trained using a group of 125 pts (25 LLK, 25 0-, 25 1-, 25 2- and 25 3-vessel CAD) using above quantitative variables and second order polynomial fitting. The remaining patients (N = 832) were categorized based on probability estimates, with CAD defined as (probability estimate \u2265 0.50). The diagnostic accuracy of SVM was also compared to visual segmental scoring by two experienced readers.Sensitivity of SVM (84%) was significantly better than ISCH (75%, p < 0.05) and EFC (31%, p < 0.05). Specificity of SVM (88%) was significantly better than that of TPD (78%, p < 0.05) and EFC (77%, p < 0.05). Diagnostic accuracy of SVM (86%) was significantly better than TPD (81%), ISCH (81%), or EFC (46%) (p < 0.05 for all). The Receiver-operator-characteristic area-under-the-curve (ROC-AUC) for SVM (0.92) was significantly better than TPD (0.90), ISCH (0.87), and EFC (0.60) (p < 0.001 for all). Diagnostic accuracy of SVM was comparable to the overall accuracy of both visual readers (85% vs. 84%, p < 0.05). ROC-AUC for SVM (0.92) was significantly better than that of both visual readers (0.87 and 0.88, p < 0.03).', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit, Support Vector Machines, Machine Learning', 'title': u'Improved Accuracy of Myocardial Perfusion SPECT for the Detection of Coronary Artery Disease by Utilizing a Support Vector Machines Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563765/', 'p': u'995 rest/stress 99mTc-sestamibi MPS studies, [650 consecutive cases with coronary angiography and 345 with likelihood of CAD < 5% (LLk)] were obtained by MPS with AC. Total perfusion deficit (TPD) for AC and NC data were compared to the visual summed stress and rest scores of 2 experienced readers. Visual reads were performed in 4 consecutive steps with the following information progressively revealed: NC data, AC+NC data, computer results, all clinical information.The diagnostic accuracy of TPD for detection of CAD was similar to both readers (NC: 82% vs. 84%, AC: 86% vs. 85\u201387% p = NS) with the exception of second reader when using clinical information (89%, p < 0.05). The Receiver-Operator-Characteristics Areas-Under-Curve (ROC-AUC) for TPD were significantly better than visual reads for NC (0.91 vs. 0.87 and 0.89, p < 0.01) and AC (0.92 vs. 0.90, p < 0.01), and it was comparable to visual reads incorporating all clinical information. Per-vessel accuracy of TPD was superior to one reader for NC (81% vs. 77%, p < 0.05) and AC (83% vs. 78%, p < 0.05) and equivalent to second reader [NC (79%) and AC (81%)]. Per-vessel ROC-AUC for NC (0.83) and AC (0.84) for TPD were better than (0.78\u20130.80 p < 0.01), and comparable to second reader (0.82\u20130.84, p = NS), for all steps.', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit', 'title': u'Comparison of Fully Automated Computer Analysis and Visual Scoring for Detection of Coronary Artery Disease from Myocardial Perfusion SPECT in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547801/', 'p': u'Conceived and designed the experiments: KPL ANA VK ZX SC SNM RMP PS IK SYS EWK TC. Performed the experiments: KPL ANA VK ZX AC VSG SG PC GKS DA JYL SYS TC. Analyzed the data: KPL ANA VK ZX DA SYS TC. Wrote the paper: KPL ANA VK ZX AC VSG SG PC GKS DA SC JYL SNM RMP PS IK SYS EWK TC.Typically, algorithms to classify phenotypes using electronic medical record (EMR) data were developed to perform well in a specific patient population. There is increasing interest in analyses which can allow study of a specific outcome across different diseases. Such a study in the EMR would require an algorithm that can be applied across different patient populations. Our objectives were: (1) to develop an algorithm that would enable the study of coronary artery disease (CAD) across diverse patient populations; (2) to study the impact of adding narrative data extracted using natural language processing (NLP) in the algorithm. Additionally, we demonstrate how to implement CAD algorithm to compare risk across 3 chronic diseases in a preliminary study.', 'kwd': '-', 'title': u'Methods to Develop an Electronic Medical Record Phenotype Algorithm to Compare the Risk of Coronary Artery Disease across 3 Chronic Disease Cohorts'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680364/', 'p': u'Artificial neural networks (ANN) might help to diagnose coronary artery disease. This study aimed to determine whether the diagnostic accuracy of an ANN-based diagnostic system and conventional quantitation are comparable.The ANN was trained to classify potentially abnormal areas as true or false based on the nuclear cardiology expert interpretation of 1001 gated stress/rest 99mTc-MIBI images at 12 hospitals. The diagnostic accuracy of the ANN was compared with 364 expert interpretations that served as the gold standard of abnormality for the validation study. Conventional summed stress/rest/difference scores (SSS/SRS/SDS) were calculated and compared with receiver operating characteristics (ROC) analysis.', 'kwd': u'Artificial intelligence, Diagnostic imaging, Coronary artery disease, Nuclear cardiology, Computer-aided diagnosis', 'title': u'Diagnostic accuracy of an artificial neural network compared with statistical quantitation of myocardial perfusion images: a Japanese multicenter study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4859156/', 'p': u'We aimed to investigate if early revascularization in patients with suspected coronary artery disease (CAD) can be effectively predicted by integrating clinical data and quantitative image features derived from perfusion SPECT (MPS) by machine learning (ML) approach.713 rest 201Thallium/stress 99mTechnetium MPS studies with correlating invasive angiography (372 revascularization events (275 PCI / 97 CABG) within 90 days after MPS (91% within 30 days) were considered. Transient ischemic dilation (TID), stress combined supine/prone total perfusion deficit (TPD), quantitative rest and stress TPD, exercise ejection fraction, and end-systolic volume along with clinical parameters including patient gender, history of hypertension and diabetes mellitus, ST-depression on baseline ECG, ECG and clinical response during stress, and post-ECG probability by boosted ensemble ML algorithm (LogitBoost) to predict revascularization events. These features were selected using an automated feature selection algorithm from all available clinical and quantitative data (33 parameters). 10-fold cross-validation was utilized to train and test the prediction model. The prediction of revascularization by ML algorithm was compared to standalone measures of perfusion and visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data.', 'kwd': u'Machine Learning, Coronary Artery Disease, Myocardial Perfusion SPECT, Revascularization, Total Perfusion Deficit', 'title': u'Prediction of Revascularization after Myocardial Perfusion SPECT by Machine Learning in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3808723/', 'p': u'\nBackground. Coronary artery disease (CAD) is a complex, multifactorial disease in which personality seems to play a role but with no definition in combination with other risk factors. Objective. To explore the nonlinear and simultaneous pathways between traditional and personality traits risk factors and coronary stenosis by Artificial Neural Networks (ANN) data mining analysis. Method. Seventy-five subjects were examined for traditional cardiac risk factors and personality traits. Analyses were based on a new data mining method using a particular artificial adaptive system, the autocontractive map (AutoCM). Results. Several traditional Cardiovascular Risk Factors (CRF) present significant relations with coronary artery plaque (CAP) presence or severity. Moreover, anger turns out to be the main factor of personality for CAP in connection with numbers of traditional risk factors. Hidden connection map showed that anger, hostility, and the Type D personality subscale social inhibition are the core factors related to the traditional cardiovascular risk factors (CRF) specifically by hypertension. Discussion. This study shows a nonlinear and simultaneous pathway between traditional risk factors and personality traits associated with coronary stenosis in CAD patients without history of cardiovascular disease. In particular, anger seems to be the main personality factor for CAP in addition to traditional risk factors.The following traditional cardiac risk factors were examined: hypertension: arterial blood pressure \u2265 140/90\u2009mm\u2009Hg or taking antihypertensive medications [16]; diabetes: nonfasting plasma glucose concentration of at least 200\u2009mg/dL (11.1\u2009mmol/L), or fasting plasma glucose level of at least 126\u2009mg/dL (7.0\u2009mmol/L), or being treated with antidiabetic medication; overweight: body mass index (BMI) (calculated as weight divided by height squared) \u226527\u2009kg/m2 (WHO); dyslipidemia: total serum cholesterol level is higher than 240\u2009mg/dL or a serum triglyceride level is 200\u2009mg/dL or more (or both) or use of a lipid-lowering agent; smoking: at least one cigarette per day or quit smoking during the previous year; family history of CAD: a first degree or second degree relative with premature cardiovascular disease (age \u2264 55 years).', 'kwd': '-', 'title': u'Combining Personality Traits with Traditional Risk Factors for Coronary Stenosis: An Artificial Neural Networks Solution in Patients with Computed Tomography Detected Coronary Artery Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3732038/', 'p': u'We aimed to improve the diagnostic accuracy of myocardial perfusion SPECT (MPS) by integrating clinical data and quantitative image features with machine learning (ML) algorithms.1,181 rest 201Tl/stress 99mTc-sestamibi dual-isotope MPS studies [713 consecutive cases with correlating invasive coronary angiography (ICA) and suspected coronary artery disease (CAD) and 468 with low likelihood (LLk) of CAD <5%] were considered. Cases with stenosis <70% by ICA and LLk of CAD were considered normal. Total stress perfusion deficit (TPD) for supine/prone data, stress/rest perfusion change, and transient ischemic dilatation were derived by automated perfusion quantification software and were combined with age, sex, and post-electrocardiogram CAD probability by a boosted ensemble ML algorithm (Logit-Boost). The diagnostic accuracy of the model for prediction of obstructive CAD \u226570% was compared to standard prone/supine quantification and to visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data. Tenfold stratified cross-validation was performed.', 'kwd': u'Myocardial perfusion imaging, SPECT automated quantification, coronary artery disease, total perfusion deficit, machine learning', 'title': u'Improved accuracy of myocardial perfusion SPECT for detection of coronary artery disease by machine learning in a large population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253773/', 'p': u'Coronary artery disease (CAD) is the result of the accumulation of athermanous plaques within the walls of coronary arteries, which supply the myocardium with oxygen and nutrients. CAD leads to heart attacks or strokes and is, thus, one of the most important causes of death worldwide. Angiography, an imaging modality for blood vessels, is currently the most accurate method of diagnosing artery stenosis. However, the disadvantages of this method such as complications, costs, and possible side effects have prompted researchers to investigate alternative solutions.The current study aimed to use data analysis, a non-invasive and less costly method, and various data mining algorithms to predict the stenosis of arteries. Among many people who refer to hospitals due to chest pain, a great number of them are normal and as such do not need angiography. The objective of this study was to predict patients who are most probably normal using features with the highest correlations with CAD with a view to obviate angiography costs and complications. Not a substitute for angiography, this method would select high-risk cases that definitely need angiography.', 'kwd': u'Data Mining, Sensitivity and Specificity, Coronary Artery Disease', 'title': u'Diagnosing Coronary Artery Disease via Data Mining Algorithms by Considering Laboratory and Echocardiography Features'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643767/', 'p': u'The continuous uninterrupted feedback system is the essential part of any well-organized system. We propose aLYNX concept that is a possibility to use an artificial intelligence algorithm or a neural network model in decision-making system so as to avoid possible mistakes and to remind the doctors to review tactics once more in selected cases.aLYNX system includes: registry with significant factors, decisions and results; machine learning process based on this registry data; the use of the machine learning results as the adviser. We show a possibility to build a computer adviser with a neural network model for making a choice between coronary aortic bypass surgery (CABG) and percutaneous coronary intervention (PCI) in order to achieve a higher 5-year survival rate in patients with angina based on the experience of 5107 patients.', 'kwd': u'Coronary artery bypass grafting, Percutaneous coronary intervention, Artificial intelligence, Decision making', 'title': u'Artificial intelligence: Neural network model as the multidisciplinary team member in clinical decision support to avoid medical mistakes'}], 'Tomography AND image segmentation': [], 'Coronary Artery Disease AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5682365/', 'p': u'Cardiovascular diseases are one of the top causes of deaths worldwide. In developing nations and rural areas, difficulties with diagnosis and treatment are made worse due to the deficiency of healthcare facilities. A viable solution to this issue is telemedicine, which involves delivering health care and sharing medical knowledge at a distance. Additionally, mHealth, the utilization of mobile devices for medical care, has also proven to be a feasible choice. The integration of telemedicine, mHealth and computer-aided diagnosis systems with the fields of machine and deep learning has enabled the creation of effective services that are adaptable to a multitude of scenarios. The objective of this review is to provide an overview of heart disease diagnosis and management, especially within the context of rural healthcare, as well as discuss the benefits, issues and solutions of implementing deep learning algorithms to improve the efficacy of relevant medical applications.According to the World Health Organization (WHO), in 2015, cardiovascular diseases represented 31% of all global deaths (1), with ischemic heart disease often cited as the leading cause of death worldwide. Furthermore, public health statistics have shown an increase of patients with some form of cardiovascular disease in countries with low or middle gross national income (2). Although serious and often life threatening, cardiovascular disease in individuals can be managed clinically as a chronic condition, and treated with medications, diet, and regular monitoring of specific health indicators. Risk factors are fairly well defined and lifestyle changes can mitigate some risks. The motivation to prevent and manage heart disease has spurred development of numerous mHealth applications for consumer use, some of which have been scientifically assessed for efficacy (3). In this paper, we provide an overview of telemedicine and mHealth technologies applied in rural healthcare settings, using one form of cardiovascular disease for context. Additionally, we discuss the need for computer-aided diagnosis (CADx) as well as the implementation of machine and deep learning techniques in these systems. Finally, we explore the issues and solutions associated with using deep learning algorithms for medical applications.', 'kwd': u'Heart disease, rural healthcare, telemedicine, mHealth, computer-aided diagnosis, machine learning, deep learning', 'title': u'Deep learning for cardiac computer-aided diagnosis: benefits, issues & solutions'}], 'Patient Assessment AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689880/', 'p': u"This paper introduces Lynx, an intelligent system for personal safety at home environments, oriented to elderly people living independently, which encompasses a decision support machine for automatic home risk prevention, tested in real-life environments to respond to real time situations. The automatic system described in this paper prevents such risks by an advanced analytic methods supported by an expert knowledge system. It is minimally intrusive, using plug-and-play sensors and machine learning algorithms to learn the elder's daily activity taking into account even his health records. If the system detects that something unusual happens (in a wide sense) or if something is wrong relative to the user's health habits or medical recommendations, it sends at real-time alarm to the family, care center, or medical agents, without human intervention. The system feeds on information from sensors deployed in the home and knowledge of subject physical activities, which can be collected by mobile applications and enriched by personalized health information from clinical reports encoded in the system. The system usability and reliability have been tested in real-life conditions, with an accuracy larger than 81%. The ultimate goal of automatic summaries is to feed the telecare platform with the most relevant clinical data obtained by an unassisted way from medical summaries and move the therapeutics procedures, treatments, or medical recommendations from medical summaries to universAAL ontology. So, the anomaly engine and the predicting intentions engine are capable of learning about personal living habits of the patients, highly correlated with their clinical conditions and prescriptions. To build this integration, thanks to semantic annotation process, we need only to join the most relevant concepts (principal diseases, diagnosis, treatment, and procedures) in the control platform (see Figure 9) with a \u201csame_as\u201d link between ontologies, by the paradigm of Linked Data recommendations [39].", 'kwd': '-', 'title': u'Lynx: Automatic Elderly Behavior Prediction in Home Telecare'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5289061/', 'p': u'In efforts to develop reliable methods to detect the likelihood of impending suicidal behaviors, we have proposed the following.To gain a deeper understanding of the state of suicide risk by determining the combination of variables that distinguishes between groups with and without suicide risk.', 'kwd': u'suicide, affective disorders, artificial intelligence, risk factors, protective factors', 'title': u'Acute Mental Discomfort Associated with Suicide Behavior in a Clinical Sample of Patients with Affective Disorders: Ascertaining Critical Variables Using Artificial Intelligence Tools'}], 'Risk Stratification AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4739399/', 'p': u'Estimating patient risk of future emergency department (ED) revisits can guide the allocation of resources, e.g. local primary care and/or specialty, to better manage ED high utilization patient populations and thereby improve patient life qualities.We set to develop and validate a method to estimate patient ED revisit risk in the subsequent 6\xa0months from an ED discharge date. An ensemble decision-tree-based model with Electronic Medical Record (EMR) encounter data from HealthInfoNet (HIN), Maine\u2019s Health Information Exchange (HIE), was developed and validated, assessing patient risk for a subsequent 6\xa0month return ED visit based on the ED encounter-associated demographic and EMR clinical history data. A retrospective cohort of 293,461 ED encounters that occurred between January 1, 2012 and December 31, 2012, was assembled with the associated patients\u2019 1-year clinical histories before the ED discharge date, for model training and calibration purposes. To validate, a prospective cohort of 193,886 ED encounters that occurred between January 1, 2013 and June 30, 2013 was constructed.', 'kwd': u'ED revisit prediction, Prospective validation, Statistical modeling, EMR', 'title': u'Prospective stratification of patients at risk for emergency department revisit: resource utilization and population management strategy implications'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5382897/', 'p': u'Deep sternal wound infection following coronary artery bypass grafting is a\nserious complication associated with significant morbidity and mortality.\nDespite the substantial impact of deep sternal wound infection, there is a\nlack of specific risk stratification tools to predict this complication\nafter coronary artery bypass grafting. This study was undertaken to develop\na specific prognostic scoring system for the development of deep sternal\nwound infection that could risk-stratify patients undergoing coronary artery\nbypass grafting and be applied right after the surgical procedure.Between March 2007 and August 2016, continuous, prospective surveillance data\non deep sternal wound infection and a set of 27 variables of 1500 patients\nwere collected. Using binary logistic regression analysis, we identified\nindependent predictors of deep sternal wound infection. Initially we\ndeveloped a predictive model in a subset of 500 patients. Dataset was\nexpanded to other 1000 consecutive cases and a final model and risk score\nwere derived. Calibration of the scores was performed using the\nHosmer-Lemeshow test.', 'kwd': u'Coronary Artery Bypass, Wound Infection, Risk Assessment/Methods', 'title': u'Development and Validation of a Stratification Tool for Predicting\nRisk of Deep Sternal Wound Infection after Coronary Artery Bypass Grafting at a\nBrazilian Hospital'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5627253/', 'p': u'The accurate assessment of a patient\u2019s risk of adverse events remains a mainstay of clinical care. Commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history, presenting signs and symptoms, and lab values. More sophisticated methods, such as Artificial Neural Networks (ANN), form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data, yielding classifiers with improved performance. Using two cohorts consisting of patients admitted with a non-ST-segment elevation acute coronary syndrome, we constructed an ANN that identifies patients at high risk of cardiovascular death (CVD). The ANN was trained and tested using patient subsets derived from a cohort containing 4395 patients (Area Under the Curve (AUC) 0.743) and validated on an independent holdout set containing 861 patients (AUC 0.767). The ANN 1-year Hazard Ratio for CVD was 3.72 (95% confidence interval 1.04\u201314.3) after adjusting for the TIMI Risk Score, left ventricular ejection fraction, and B-type natriuretic peptide. A unique feature of our approach is that it captures small changes in the ST segment over time that cannot be detected by visual inspection. These findings highlight the important role that ANNs can play in risk stratification.The study populations consisted of two patient cohorts (Table\xa01) that were derived from two different clinical studies used in previous work to evaluate the performance of several computational biomarkers14,17,18. The first study19,20 included 6,560 patients with interpretable continuous ECG data, and the second study21 included 990 patients with interpretable ECG data. All patients in both cohorts were enrolled, and ECG collection began, within 48\u2009hours after presenting with symptoms consistent with a NSTE-ACS. From these datasets, we restricted our analysis to patients who had at least one day of continuous ECG signal and values for seven baseline characteristics: age, gender, current smoker, history of hypertension, history of diabetes, previous myocardial infarction (MI), and a history of previous angiography. These features correspond to the subset of features that were in common to both cohorts. Using these criteria, 4,395 patients from Cohort-1 and 861 patients from Cohort-2 were used in the analysis.\n', 'kwd': '-', 'title': u'Machine Learning Improves Risk Stratification After Acute Coronary Syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5596298/', 'p': u'To improve health outcomes and cut health care costs, we often need to conduct prediction/classification using large clinical datasets (aka, clinical big data), for example, to identify high-risk patients for preventive interventions. Machine learning has been proposed as a key technology for doing this. Machine learning has won most data science competitions and could support many clinical activities, yet only 15% of hospitals use it for even limited purposes. Despite familiarity with data, health care researchers often lack machine learning expertise to directly use clinical big data, creating a hurdle in realizing value from their data. Health care researchers can work with data scientists with deep machine learning knowledge, but it takes time and effort for both parties to communicate effectively. Facing a shortage in the United States of data scientists and hiring competition from companies with deep pockets, health care systems have difficulty recruiting data scientists. Building and generalizing a machine learning model often requires hundreds to thousands of manual iterations by data scientists to select the following: (1) hyper-parameter values and complex algorithms that greatly affect model accuracy and (2) operators and periods for temporally aggregating clinical attributes (eg, whether a patient\u2019s weight kept rising in the past year). This process becomes infeasible with limited budgets.This study\u2019s goal is to enable health care researchers to directly use clinical big data, make machine learning feasible with limited budgets and data scientist resources, and realize value from data.', 'kwd': u'machine learning, automated temporal aggregation, automatic model selection, care management, clinical big data', 'title': u'Automating Construction of Machine Learning Models With Clinical Big Data: Proposal Rationale and Methods'}], 'Patient Assessment AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2816803/', 'p': u'Cardiac magnetic resonance (CMR) imaging enables accurate and reproducible quantification of measurements of global and regional ventricular function, blood flow, perfusion at rest and stress as well as myocardial injury. Recent advances in MR hardware and software have resulted in significant improvements in image quality and a reduction in imaging time. Methods for automated and robust assessment of the parameters of cardiac function, blood flow and morphology are being developed. This article reviews the recent advances in image acquisition and quantitative image analysis in CMR.The diagnosis and management of cardiac disease requires a precise assessment of the parameters of cardiac morphology and function. Cardiac magnetic resonance (CMR) imaging has shown to be a versatile non-invasive imaging modality providing accurate and reproducible assessment of global and ventricular regional function, blood flow, myocardial perfusion and myocardial scar. In addition to enhancing clinical decision making, the accuracy and reproducibility of the CMR quantitative measures of cardiac function and morphology allow research studies to be carried out with fewer subjects enhancing cost effectiveness. Significant recent advances have been made in the generation of new CMR acquisition protocols as well as MR hardware enabling more rapid image acquisition. Despite these advances, the quantitative analysis of the images often still relies on manual tracing of the contours in many images, a time-consuming process. Reliable automated or semi-automated image segmentation and analysis software allowing for reproducible and rapid quantification are under development. In this paper an overview is provided on some of the recent work that has been carried out on image acquisition, computerized quantitative image analysis methods and semi-automated contour detection software for CMR imaging. The emerging clinical applications of quantitative CMR parameters are highlighted.', 'kwd': u'Cardiac MRI, Quantification', 'title': u'Quantification in cardiac MRI: advances in image acquisition and processing'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3893567/', 'p': u"Nonrigid image registration is an important, but time-consuming task in medical image analysis. In typical neuroimaging studies, multiple image registrations are performed, i.e., for atlas-based segmentation or template construction. Faster image registration routines would therefore be beneficial. In this paper we explore acceleration of the image registration package elastix by a combination of several techniques: (i) parallelization on the CPU, to speed up the cost function derivative calculation; (ii) parallelization on the GPU building on and extending the OpenCL framework from ITKv4, to speed up the Gaussian pyramid computation and the image resampling step; (iii) exploitation of certain properties of the B-spline transformation model; (iv) further software optimizations. The accelerated registration tool is employed in a study on diagnostic classification of Alzheimer's disease and cognitively normal controls based on T1-weighted MRI. We selected 299 participants from the publicly available Alzheimer's Disease Neuroimaging Initiative database. Classification is performed with a support vector machine based on gray matter volumes as a marker for atrophy. We evaluated two types of strategies (voxel-wise and region-wise) that heavily rely on nonrigid image registration. Parallelization and optimization resulted in an acceleration factor of 4\u20135x on an 8-core machine. Using OpenCL a speedup factor of 2 was realized for computation of the Gaussian pyramids, and 15\u201360 for the resampling step, for larger images. The voxel-wise and the region-wise classification methods had an area under the receiver operator characteristic curve of 88 and 90%, respectively, both for standard and accelerated registration. We conclude that the image registration package elastix was substantially accelerated, with nearly identical results to the non-optimized version. The new functionality will become available in the next release of elastix as open source under the BSD license.Data from the ADNI1 database was used. The ADNI cohort used for our experiments is adopted from the study of Cuingnet et al. (2011), from which we selected the AD patient group and the normal elderly control group. The inclusion criteria for participants were defined in the ADNI GO protocol (www.adni-info.org/Scientists/AboutADNI.aspx\\#). The patient group consisted of 137 patients (67 males, age = 76.0 \xb1 7.3 years, Mini Mental State Examination (MMSE) score = 23.2 \xb1 2.0), and the control group of 162 participants (76 males, age = 76.3 \xb1 5.4 years, MMSE = 29.2 \xb1 1.0). The participants were randomly split into two groups of the same size, a training set and a test set, while preserving the age and sex distribution (Cuingnet et al., 2011). Structural MRI (T1w) data were acquired at 1.5T according to the ADNI acquisition protocol (Jack et al., 2008).", 'kwd': u"image registration, parallelization, acceleration, OpenCL, elastix, Alzheimer's disease", 'title': u"Fast parallel image registration on CPU and GPU for diagnostic classification of Alzheimer's disease"}], 'Patient Assessment AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049832/', 'p': u'The dataset contains 18 T1-weighted MR brain images and their manual segmentations. The images provided by IBSR have been normalized into the Talairach space (rotation only) and preprocessed by intensity inhomogeneity correction routines. The images have slice thickness of 1.5 mm with in-plane resolution varying between 1.0 mm \xd7 1.0 mm and 0.84 mm \xd7 0.84 mm. The manual segmentations contain labels for gray matter, white matter and the ventricles. Notably, cerebrospinal fluid (CSF) outside of the ventricles is assigned the gray matter label in the IBSR segmentations.BET (Smith, 2002) uses a deformable model to separate the brain from other tissues in MR images. In our experiments, BET was applied with the default parameters to segment each of the 18 brain images from IBSR. The EC method was used to improve the accuracy of brain extraction relative to the brain masks in IBSR. 10 cross-validation experiments were performed. For each cross-validation evaluation, 9 subjects were randomly selected for training the EC method, and the remaining 9 for testing. The brain volumes have millions of voxels, posing a challenge for the AdaBoost learning, which requires loading all data in memory for efficient learning. For efficiency, we randomly selected 1% voxels uniformly from the working ROIs for training.', 'kwd': u'medical image segmentation, error correction, AdaBoost, hippocampal segmentation, brain extraction, brain tissue segmentation', 'title': u'A Learning-Based Wrapper Method to Correct Systematic Errors in Automatic Image Segmentation: Consistently Improved Performance in Hippocampus, Cortex and Brain Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5683197/', 'p': u'The MRBrainS131 (Mendrik et al., 2015) framework consists of MR images and manual segmentations from 20 patients (age, mean \xb1 standard deviation: 71 \xb1 4 years; 10 male, 10 female). The MR images were manually segmented in eight classes: white matter (WM), cortical grey matter (cGM), basal ganglia and thalami (BGT), cerebellum (CB), brain stem (BS), lateral ventricular cerebrospinal fluid (lvCSF), peripheral cerebrospinal fluid (pCSF), and WMH. Note that the MRBrainS13 challenge only includes evaluation of three combined tissue classes: white matter (including WMH), grey matter (including BGT) and CSF (pCSF and lvCSF) instead of all eight classes.Patients with type 2 diabetes mellitus and healthy controls were included from the Utrecht Diabetic Encephalopathy Study part 2 (UDES2) (Reijmer et al., 2013). The images used in MRBrainS13 were selected from the UDES2 cohort. From the UDES2 cohort we analysed images from 96 additional patients (age, mean \xb1 standard deviation: 71 \xb1 5 years; 58 male, 38 female; 51 with type 2 diabetes mellitus and 45 healthy controls). Reference segmentations of WMH were performed by manual outlining on the FLAIR images using relatively strict criteria (Brundel et al., 2014).', 'kwd': u'Brain MRI, Segmentation, White matter hyperintensities, Deep learning, Convolutional neural networks, Motion artefacts, Brain atrophy', 'title': u'Evaluation of a deep learning approach for the segmentation of brain tissues and white matter hyperintensities of presumed vascular origin in\xa0MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706546/', 'p': u'Segmentation of the hippocampus from magnetic resonance (MR) images is a key task in the evaluation of mesial temporal lobe epilepsy (mTLE) patients. Several automated algorithms have been proposed although manual segmentation remains the benchmark. Choosing a reliable algorithm is problematic since structural definition pertaining to multiple edges, missing and fuzzy boundaries, and shape changes varies among mTLE subjects. Lack of statistical references and guidance for quantifying the reliability and reproducibility of automated techniques has further detracted from automated approaches. The purpose of this study was to develop a systematic and statistical approach using a large dataset for the evaluation of automated methods and establish a method that would achieve results better approximating those attained by manual tracing in the epileptogenic hippocampus.A template database of 195 (81 males, 114 females; age range 32\u201367 yr, mean 49.16 yr) MR images of mTLE patients was used in this study. Hippocampal segmentation was accomplished manually and by two well-known tools (FreeSurfer and hammer) and two previously published methods developed at their institution [Automatic brain structure segmentation (ABSS) and LocalInfo]. To establish which method was better performing for mTLE cases, several voxel-based, distance-based, and volume-based performance metrics were considered. Statistical validations of the results using automated techniques were compared with the results of benchmark manual segmentation. Extracted metrics were analyzed to find the method that provided a more similar result relative to the benchmark.', 'kwd': u'medical imaging, image processing, segmentation, hippocampus, temporal lobe epilepsy, magnetic resonance imaging (MRI)', 'title': u'Comparative performance evaluation of automated segmentation methods of hippocampus from magnetic resonance images of temporal lobe epilepsy patients'}], 'Angiography AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5375621/', 'p': u'Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works.The assigning of a class or label to a group of pixels, such as those labeled as tumor with use of a segmentation algorithm. For instance, if segmentation has been used to mark some part of an image as \u201cabnormal brain,\u201d the classifier might then try to determine whether the marked part represents benign or malignant tissue.', 'kwd': '-', 'title': u'Machine Learning for Medical Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890616/', 'p': u'CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32 \xd7 32 images of 10 object classes. The objects are normally centered in the images. Some example images and class categories from the Cifar10 dataset are shown in Figure 7. CifarNet has three convolution layers, three pooling layers, and one fully-connected layer. This CNN architecture, also used in [22] has about 0.15 million free parameters. We adopt it as a baseline model for the LN detection.The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. This success has revived the interest in CNNs [3] in computer vision. ImageNet consists of 1.2 million 256 \xd7 256 images belonging to 1000 categories. At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model. More details about the ImageNet dataset will be discussed in Sec. III-B. AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters. AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.', 'kwd': '-', 'title': u'Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning'}], 'Arterial Coronary Syndrome AND Image processing': [], 'Angiography AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756595/', 'p': u'We propose new methods for automatic segmentation of images based on an atlas of manually labeled scans and contours in the image. First, we introduce a Bayesian framework for creating initial label maps from manually annotated training images. Within this framework, we model various registration- and patch-based segmentation techniques by changing the deformation field prior. Second, we perform contour-driven regression on the created label maps to refine the segmentation. Image contours and image parcellations give rise to non-stationary kernel functions that model the relationship between image locations. Setting the kernel to the covariance function in a Gaussian process establishes a distribution over label maps supported by image structures. Maximum a posteriori estimation of the distribution over label maps conditioned on the outcome of the atlas-based segmentation yields the refined segmentation. We evaluate the segmentation in two clinical applications: the segmentation of parotid glands in head and neck CT scans and the segmentation of the left atrium in cardiac MR angiography images.Given a novel image I, we aim to infer its segmentation S based on an atlas that contains training images \u2110 =\xa0{I1,\xa0\u2026,\xa0In} with segmentations \U0001d4ae =\xa0{S1,\xa0\u2026,\xa0Sn}. A probabilistic label map \u2112 =\xa0{L1,\xa0\u2026,\xa0L\u03b7} specifies the likelihood for each label l \u2208 {1, ..., \u03b7} ', 'kwd': u'Image segmentation, atlas, patch, spectral clustering, Gaussian process, left atrium, parotid glands', 'title': u'Contour-Driven Atlas-Based Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3138936/', 'p': '-', 'kwd': u'Magnetic resonance angiography (MRA), time-of-flight (TOF), cerebrovascular segmentation, statistical model analysis, fast curve evolution', 'title': u'A Fast and Fully Automatic Method for Cerebrovascular Segmentation on Time-of-Flight (TOF) MRA Image'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5568763/', 'p': u'Anatomical-based partial volume correction (PVC) has been shown to improve image quality and quantitative accuracy in cardiac SPECT/CT. However, this method requires manual segmentation of various organs from contrast-enhanced computed tomography angiography (CTA) data. In order to achieve fully automatic CTA segmentation for clinical translation, we investigated the most common multi-atlas segmentation methods. We also modified the multi-atlas segmentation method by introducing a novel label fusion algorithm for multiple organ segmentation to eliminate overlap and gap voxels. To evaluate our proposed automatic segmentation, eight canine 99mTc-labeled red blood cell SPECT/CT datasets that incorporated PVC were analyzed, using the leave-one-out approach. The Dice similarity coefficient of each organ was computed. Compared to the conventional label fusion method, our proposed label fusion method effectively eliminated gaps and overlaps and improved the CTA segmentation accuracy. The anatomical-based PVC of cardiac SPECT images with automatic multi-atlas segmentation provided consistent image quality and quantitative estimation of intramyocardial blood volume, as compared to those derived using manual segmentation. In conclusion, our proposed automatic multi-atlas segmentation method of CTAs is feasible, practical, and facilitates anatomical-based PVC of cardiac SPECT/CT images.After registering each particular atlas image to the target image, the transformation matrices were obtained. Then, the candidate segmentation images were obtained by applying these transformations to the corresponding label images of the five critical organ ROIs, respectively. In our method, we performed this label propagation process for each atlas image respectively to generate all candidate segmentation image sets of five ROIs.', 'kwd': u'multi-atlas based segmentation, partial volume correction, cardiac SPECT/CT', 'title': u'Fully automatic multi-atlas segmentation of CTA for partial volume correction in cardiac SPECT/CT'}], 'Arterial Coronary Syndrome AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}], 'Angiography AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480986/', 'p': u'Disease staging involves the assessment of disease severity or progression and is used for treatment selection. In diabetic retinopathy, disease staging using a wide area is more desirable than that using a limited area. We investigated if deep learning artificial intelligence (AI) could be used to grade diabetic retinopathy and determine treatment and prognosis.The retrospective study analyzed 9,939 posterior pole photographs of 2,740 patients with diabetes. Nonmydriatic 45\xb0 field color fundus photographs were taken of four fields in each eye annually at Jichi Medical University between May 2011 and June 2015. A modified fully randomly initialized GoogLeNet deep learning neural network was trained on 95% of the photographs using manual modified Davis grading of three additional adjacent photographs. We graded 4,709 of the 9,939 posterior pole fundus photographs using real prognoses. In addition, 95% of the photographs were learned by the modified GoogLeNet. Main outcome measures were prevalence and bias-adjusted Fleiss\u2019 kappa (PABAK) of AI staging of the remaining 5% of the photographs.', 'kwd': '-', 'title': u'Applying artificial intelligence to disease staging: Deep learning for improved staging of diabetic retinopathy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643767/', 'p': u'The continuous uninterrupted feedback system is the essential part of any well-organized system. We propose aLYNX concept that is a possibility to use an artificial intelligence algorithm or a neural network model in decision-making system so as to avoid possible mistakes and to remind the doctors to review tactics once more in selected cases.aLYNX system includes: registry with significant factors, decisions and results; machine learning process based on this registry data; the use of the machine learning results as the adviser. We show a possibility to build a computer adviser with a neural network model for making a choice between coronary aortic bypass surgery (CABG) and percutaneous coronary intervention (PCI) in order to achieve a higher 5-year survival rate in patients with angina based on the experience of 5107 patients.', 'kwd': u'Coronary artery bypass grafting, Percutaneous coronary intervention, Artificial intelligence, Decision making', 'title': u'Artificial intelligence: Neural network model as the multidisciplinary team member in clinical decision support to avoid medical mistakes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4859156/', 'p': u'We aimed to investigate if early revascularization in patients with suspected coronary artery disease (CAD) can be effectively predicted by integrating clinical data and quantitative image features derived from perfusion SPECT (MPS) by machine learning (ML) approach.713 rest 201Thallium/stress 99mTechnetium MPS studies with correlating invasive angiography (372 revascularization events (275 PCI / 97 CABG) within 90 days after MPS (91% within 30 days) were considered. Transient ischemic dilation (TID), stress combined supine/prone total perfusion deficit (TPD), quantitative rest and stress TPD, exercise ejection fraction, and end-systolic volume along with clinical parameters including patient gender, history of hypertension and diabetes mellitus, ST-depression on baseline ECG, ECG and clinical response during stress, and post-ECG probability by boosted ensemble ML algorithm (LogitBoost) to predict revascularization events. These features were selected using an automated feature selection algorithm from all available clinical and quantitative data (33 parameters). 10-fold cross-validation was utilized to train and test the prediction model. The prediction of revascularization by ML algorithm was compared to standalone measures of perfusion and visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data.', 'kwd': u'Machine Learning, Coronary Artery Disease, Myocardial Perfusion SPECT, Revascularization, Total Perfusion Deficit', 'title': u'Prediction of Revascularization after Myocardial Perfusion SPECT by Machine Learning in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615055/', 'p': u'957 rest/stress 99mtechnetium gated MPS NC studies from 623 consecutive patients with correlating invasive coronary angiography and 334 with low likelihood of CAD (LLK < 5% ) were assessed. Patients with stenosis \u2265 50% in left main or \u2265 70% in all other vessels were considered abnormal. Total perfusion deficit (TPD) was computed automatically. In addition, ischemic changes (ISCH) and ejection fraction changes (EFC) between stress and rest were derived by quantitative software. The SVM was trained using a group of 125 pts (25 LLK, 25 0-, 25 1-, 25 2- and 25 3-vessel CAD) using above quantitative variables and second order polynomial fitting. The remaining patients (N = 832) were categorized based on probability estimates, with CAD defined as (probability estimate \u2265 0.50). The diagnostic accuracy of SVM was also compared to visual segmental scoring by two experienced readers.Sensitivity of SVM (84%) was significantly better than ISCH (75%, p < 0.05) and EFC (31%, p < 0.05). Specificity of SVM (88%) was significantly better than that of TPD (78%, p < 0.05) and EFC (77%, p < 0.05). Diagnostic accuracy of SVM (86%) was significantly better than TPD (81%), ISCH (81%), or EFC (46%) (p < 0.05 for all). The Receiver-operator-characteristic area-under-the-curve (ROC-AUC) for SVM (0.92) was significantly better than TPD (0.90), ISCH (0.87), and EFC (0.60) (p < 0.001 for all). Diagnostic accuracy of SVM was comparable to the overall accuracy of both visual readers (85% vs. 84%, p < 0.05). ROC-AUC for SVM (0.92) was significantly better than that of both visual readers (0.87 and 0.88, p < 0.03).', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit, Support Vector Machines, Machine Learning', 'title': u'Improved Accuracy of Myocardial Perfusion SPECT for the Detection of Coronary Artery Disease by Utilizing a Support Vector Machines Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563765/', 'p': u'995 rest/stress 99mTc-sestamibi MPS studies, [650 consecutive cases with coronary angiography and 345 with likelihood of CAD < 5% (LLk)] were obtained by MPS with AC. Total perfusion deficit (TPD) for AC and NC data were compared to the visual summed stress and rest scores of 2 experienced readers. Visual reads were performed in 4 consecutive steps with the following information progressively revealed: NC data, AC+NC data, computer results, all clinical information.The diagnostic accuracy of TPD for detection of CAD was similar to both readers (NC: 82% vs. 84%, AC: 86% vs. 85\u201387% p = NS) with the exception of second reader when using clinical information (89%, p < 0.05). The Receiver-Operator-Characteristics Areas-Under-Curve (ROC-AUC) for TPD were significantly better than visual reads for NC (0.91 vs. 0.87 and 0.89, p < 0.01) and AC (0.92 vs. 0.90, p < 0.01), and it was comparable to visual reads incorporating all clinical information. Per-vessel accuracy of TPD was superior to one reader for NC (81% vs. 77%, p < 0.05) and AC (83% vs. 78%, p < 0.05) and equivalent to second reader [NC (79%) and AC (81%)]. Per-vessel ROC-AUC for NC (0.83) and AC (0.84) for TPD were better than (0.78\u20130.80 p < 0.01), and comparable to second reader (0.82\u20130.84, p = NS), for all steps.', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit', 'title': u'Comparison of Fully Automated Computer Analysis and Visual Scoring for Detection of Coronary Artery Disease from Myocardial Perfusion SPECT in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2759696/', 'p': u'To determine the performance of an artificial neural network in transcranial color-coded duplex sonography (TCCS) diagnosis of middle cerebral artery (MCA) spasm. TCCS was prospectively acquired within 2 h prior to routine cerebral angiography in 100 consecutive patients (54M:46F, median age 50 years). Angiographic MCA vasospasm was classified as mild (<25% of vessel caliber reduction), moderate (25\u201350%), or severe (>50%). A Learning Vector Quantization neural network classified MCA spasm based on TCCS peak-systolic, mean, and end-diastolic velocity data. During a four-class discrimination task, accurate classification by the network ranged from 64.9% to 72.3%, depending on the number of neurons in the Kohonen layer. Accurate classification of vasospasm ranged from 79.6% to 87.6%, with an accuracy of 84.7% to 92.1% for the detection of moderate-to-severe vasospasm. An artificial neural network may increase the accuracy of TCCS in diagnosis of MCA spasm.Following a 15-min rest period in supine position, TCCS was performed on a sonographic scanner (Toshiba Applio, Toshiba Medical System, Tokyo, Japan) equipped with a 2.5 MHz 90\xb0 phased-array probe with B-mode and Doppler imaging. Proximal segments of the basal cerebral arteries were insonated via a transtemporal and were identified on grayscale and color imaging in relation to intracranial structures (Krejza et al. 2000). A 3-mm wide sample volume was placed on the color image of the proximal MCA (M1) about 10 mm distal to the terminal carotid or at the site of the highest flow velocity acceleration indicated by aliasing phenomenon. A linear marker was placed under visual guidance on the color image of the insonated vascular segment along the long axis of the vessel to determine the angle of insonation. The angle between this linear marker and the ultrasound beam, displayed automatically on the screen of the scanner, was considered a two-dimensional approximation of the angle of insonation. A typical TCCS image of MCA spasm is shown on Fig. 1. Angle-corrected peak systolic (VPS), mean (VMEAN), and end diastolic (VED) blood flow velocities were subsequently obtained. Automated blood flow velocity determinations were used, although manual tracing of the maximum frequency envelope of the Doppler waveform was used to obtain these values when a weak Doppler signal was noted. An expert radiologist reviewed the TCCS data for quality purposes and rejected 42 waveforms due to inferior quality. Further analyses were based on the remaining 158 data sets, including waveforms of the left and right MCA.', 'kwd': u'Ultrasound, Cerebral blood vessels, Vasospasm, Artificial neural networks, Transcranial Doppler, Diagnosis, Blood velocity, Velocimetry, Brain arteries, Ischemia, Stroke', 'title': u'Learning Vector Quantization Neural Networks Improve Accuracy of Transcranial Color-coded Duplex Sonography in Detection of Middle Cerebral Artery Spasm\u2014Preliminary Report'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4659889/', 'p': u'Monitoring heart failure patients through continues assessment of sign and symptoms by information technology tools lead to large reduction in re-hospitalization. Agent technology is one of the strongest artificial intelligence areas; therefore, it can be expected to facilitate, accelerate, and improve health services especially in home care and telemedicine. The aim of this article is to provide an agent-based model for chronic heart failure (CHF) follow-up management.This research was performed in 2013-2014 to determine appropriate scenarios and the data required to monitor and follow-up CHF patients, and then an agent-based model was designed.', 'kwd': u'Health Information Systems, Heart Failure, Artificial Intelligence, Multi-agent Systems', 'title': u'Chronic Heart Failure Follow-up Management Based on Agent Technology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680364/', 'p': u'Artificial neural networks (ANN) might help to diagnose coronary artery disease. This study aimed to determine whether the diagnostic accuracy of an ANN-based diagnostic system and conventional quantitation are comparable.The ANN was trained to classify potentially abnormal areas as true or false based on the nuclear cardiology expert interpretation of 1001 gated stress/rest 99mTc-MIBI images at 12 hospitals. The diagnostic accuracy of the ANN was compared with 364 expert interpretations that served as the gold standard of abnormality for the validation study. Conventional summed stress/rest/difference scores (SSS/SRS/SDS) were calculated and compared with receiver operating characteristics (ROC) analysis.', 'kwd': u'Artificial intelligence, Diagnostic imaging, Coronary artery disease, Nuclear cardiology, Computer-aided diagnosis', 'title': u'Diagnostic accuracy of an artificial neural network compared with statistical quantitation of myocardial perfusion images: a Japanese multicenter study'}], 'Angiography AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3736499/', 'p': u'We address the problem of motion artifact reduction in digital subtraction angiography (DSA) using image registration techniques. Most of registration algorithms proposed for application in DSA, have been designed for peripheral and cerebral angiography images in which we mainly deal with global rigid motions. These algorithms did not yield good results when applied to coronary angiography images because of complex nonrigid motions that exist in this type of angiography images. Multiresolution and iterative algorithms are proposed to cope with this problem, but these algorithms are associated with high computational cost which makes them not acceptable for real-time clinical applications. In this paper we propose a nonrigid image registration algorithm for coronary angiography images that is significantly faster than multiresolution and iterative blocking methods and outperforms competing algorithms evaluated on the same data sets. This algorithm is based on a sparse set of matched feature point pairs and the elastic registration is performed by means of multilevel B-spline image warping. Experimental results with several clinical data sets demonstrate the effectiveness of our approach.The algorithm here is a summary of the operations involved in the registration of two images of a digital angiographic image sequence, presented and discussed in the previous section. Given a mask image and a live image from an angiographic image sequence, the registration is accomplished by carrying out the following steps.', 'kwd': '-', 'title': u'Nonrigid Image Registration in Digital Subtraction Angiography Using Multilevel B-Spline'}], 'Risk Stratification AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4433206/', 'p': u'Conceived and designed the experiments: ZS XC. Performed the experiments: YL JL. Analyzed the data: YL JL Xudong Liu WK. Contributed reagents/materials/analysis tools: YL Xiaoyong Liu WK XZ FW. Wrote the paper: YL JL XC.Exfoliative cytology has been widely used for early diagnosis of oral squamous cell carcinoma (OSCC). Test outcome is reported as \u201cnegative\u201d, \u201catypical\u201d (defined as abnormal epithelial changes of uncertain diagnostic significance), and \u201cpositive\u201d (defined as definitive cellular evidence of epithelial dysplasia or carcinoma). The major challenge is how to properly manage the \u201catypical\u201d patients in order to diagnose OSCC early and prevent OSCC. In this study, we collected exfoliative cytology data, histopathology data, and clinical data of normal subjects (n=102), oral leukoplakia (OLK) patients (n=82), and OSCC patients (n=93), and developed a data analysis procedure for quantitative risk stratification of OLK patients. This procedure involving a step called expert-guided data transformation and reconstruction (EdTAR) which allows automatic data processing and reconstruction and reveals informative signals for subsequent risk stratification. Modern machine learning techniques were utilized to build statistical prediction models on the reconstructed data. Among the several models tested using resampling methods for parameter pruning and performance evaluation, Support Vector Machine (SVM) was found to be optimal with a high sensitivity (median>0.98) and specificity (median>0.99). With the SVM model, we constructed an oral cancer risk index (OCRI) which may potentially guide clinical follow-up of OLK patients. One OLK patient with an initial OCRI of 0.88 developed OSCC after 40 months of follow-up. In conclusion, we have developed a statistical method for qualitative risk stratification of OLK patients. This method may potentially improve cost-effectiveness of clinical follow-up of OLK patients, and help design clinical chemoprevention trial for high-risk populations.', 'kwd': '-', 'title': u'Quantitative Risk Stratification of Oral Leukoplakia with Exfoliative Cytology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4780431/', 'p': u'Great effort has been devoted in recent years to the development of sudden cardiac risk predictors as a function of electric cardiac signals, mainly obtained from the electrocardiogram (ECG) analysis. But these prediction techniques are still seldom used in clinical practice, partly due to its limited diagnostic accuracy and to the lack of consensus about the appropriate computational signal processing implementation. This paper addresses a three-fold approach, based on ECG indices, to structure this review on sudden cardiac risk stratification. First, throughout the computational techniques that had been widely proposed for obtaining these indices in technical literature. Second, over the scientific evidence, that although is supported by observational clinical studies, they are not always representative enough. And third, via the limited technology transfer of academy-accepted algorithms, requiring further meditation for future systems. We focus on three families of ECG derived indices which are tackled from the aforementioned viewpoints, namely, heart rate turbulence (HRT), heart rate variability (HRV), and T-wave alternans. In terms of computational algorithms, we still need clearer scientific evidence, standardizing, and benchmarking, siting on advanced algorithms applied over large and representative datasets. New scenarios like electronic health recordings, big data, long-term monitoring, and cloud databases, will eventually open new frameworks to foresee suitable new paradigms in the near future.This set of indices aims to improve the robustness of the HRV measurements in RR tachograms, and for this purpose, they distribute the series of observed RR intervals by following a specific geometric pattern, based on the probability density function of normal RR intervals or their first difference, or on the sampling distribution density of normal RR interval durations. Emerging patterns are then measured and classified in different categories and measuring the range or the geometric figure scatter. For instance, when trying to match a given RR histogram with a triangle pattern shape, the parameters better approximating the histogram provide with a measurement of the scatter by means of the triangle basis. The most usual geometrical methods are the the triangular index, the differential index, and the logarithmic index (see Malik et al., 1996 for further details).', 'kwd': u'sudden cardiac death, risk stratification, computational algorithms, scientific evidence, technology transfer, heart rate variability, heart rate turbulence, T\u2013wave alternans', 'title': u'Sudden Cardiac Risk Stratification with Electrocardiographic Indices - A Review on Computational Processing, Technology Transfer, and Scientific Evidence'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3621376/', 'p': u'T2-weighted cardiovascular magnetic resonance (CMR) is clinically-useful for imaging the ischemic area-at-risk and amount of salvageable myocardium in patients with acute myocardial infarction (MI). However, to date, quantification of oedema is user-defined and potentially subjective.We describe a highly automatic framework for quantifying myocardial oedema from bright blood T2-weighted CMR in patients with acute MI. Our approach retains user input (i.e. clinical judgment) to confirm the presence of oedema on an image which is then subjected to an automatic analysis. The new method was tested on 25 consecutive acute MI patients who had a CMR within 48\xa0hours of hospital admission. Left ventricular wall boundaries were delineated automatically by variational level set methods followed by automatic detection of myocardial oedema by fitting a Rayleigh-Gaussian mixture statistical model. These data were compared with results from manual segmentation of the left ventricular wall and oedema, the current standard approach.', 'kwd': u'Myocardial oedema, Bright blood T2-weighted CMR, Rayleigh-Gaussian mixture model, Level set', 'title': u'Highly automatic quantification of myocardial oedema in patients with acute myocardial infarction using bright blood T2-weighted CMR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537099/', 'p': u'Pretreatment risk stratification is key for personalized medicine. While many physicians rely on an \u201ceyeball test\u201d to assess whether patients will tolerate major surgery or chemotherapy, \u201ceyeballing\u201d is inherently subjective and difficult to quantify. The concept of morphometric age derived from cross-sectional imaging has been found to correlate well with outcomes such as length of stay, morbidity, and mortality. However, the determination of the morphometric age is time intensive and requires highly trained experts. In this study, we propose a fully automated deep learning system for the segmentation of skeletal muscle cross-sectional area (CSA) on an axial computed tomography image taken at the third lumbar vertebra. We utilized a fully automated deep segmentation model derived from an extended implementation of a fully convolutional network with weight initialization of an ImageNet pre-trained model, followed by post processing to eliminate intramuscular fat for a more accurate analysis. This experiment was conducted by varying window level (WL), window width (WW), and bit resolutions in order to better understand the effects of the parameters on the model performance. Our best model, fine-tuned on 250 training images and ground truth labels, achieves 0.93\xa0\xb1\xa00.02 Dice similarity coefficient (DSC) and 3.68\xa0\xb1\xa02.29% difference between predicted and ground truth muscle CSA on 150 held-out test cases. Ultimately, the fully automated segmentation system can be embedded into the clinical environment to accelerate the quantification of muscle and expanded to volume analysis of 3D datasets.We reformatted the manually tuned muscle segmentation maps created by domain experts as described previously into acceptable input for convolutional neural networks (CNN). As shown in Fig. \u200bFig.1,1, the axial images and their corresponding color-coded images served as original input data and ground truth labels, respectively. The main challenge for muscle segmentation is the accurate differentiation of muscle tissue from neighboring organs due to their overlapping HU ranges. We manually drew a boundary between organs and muscle, setting the inside region as additional segmentation class (\u201cInside\u201d) in an effort to train the neural network to learn distinguishing features of muscle for a precise segmentation from adjacent organs. The color-coded label images were assigned to pre-defined label indices, including 0 (black) for \u201cBackground\u201d, 1 (red) for \u201cMuscle\u201d, and 2 (green) for \u201cInside\u201d, before passing through CNNs for training as presented in Fig. \u200bFig.11.\n', 'kwd': u'Muscle segmentation, Convolutional neural networks, Computer-aided diagnosis (CAD), Computed tomography, Artificial intelligence, Deep learning', 'title': u'Pixel-Level Deep Segmentation: Artificial Intelligence Quantifies Muscle on Computed Tomography for Body Morphometric Analysis'}], 'Risk Score AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756595/', 'p': u'We propose new methods for automatic segmentation of images based on an atlas of manually labeled scans and contours in the image. First, we introduce a Bayesian framework for creating initial label maps from manually annotated training images. Within this framework, we model various registration- and patch-based segmentation techniques by changing the deformation field prior. Second, we perform contour-driven regression on the created label maps to refine the segmentation. Image contours and image parcellations give rise to non-stationary kernel functions that model the relationship between image locations. Setting the kernel to the covariance function in a Gaussian process establishes a distribution over label maps supported by image structures. Maximum a posteriori estimation of the distribution over label maps conditioned on the outcome of the atlas-based segmentation yields the refined segmentation. We evaluate the segmentation in two clinical applications: the segmentation of parotid glands in head and neck CT scans and the segmentation of the left atrium in cardiac MR angiography images.Given a novel image I, we aim to infer its segmentation S based on an atlas that contains training images \u2110 =\xa0{I1,\xa0\u2026,\xa0In} with segmentations \U0001d4ae =\xa0{S1,\xa0\u2026,\xa0Sn}. A probabilistic label map \u2112 =\xa0{L1,\xa0\u2026,\xa0L\u03b7} specifies the likelihood for each label l \u2208 {1, ..., \u03b7} ', 'kwd': u'Image segmentation, atlas, patch, spectral clustering, Gaussian process, left atrium, parotid glands', 'title': u'Contour-Driven Atlas-Based Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3570946/', 'p': u"This paper overviews one of the most important, interesting, and challenging problems in oncology, the problem of lung cancer diagnosis. Developing an effective computer-aided diagnosis (CAD) system for lung cancer is of great clinical importance and can increase the patient's chance of survival. For this reason, CAD systems for lung cancer have been investigated in a huge number of research studies. A typical CAD system for lung cancer diagnosis is composed of four main processing steps: segmentation of the lung fields, detection of nodules inside the lung fields, segmentation of the detected nodules, and diagnosis of the nodules as benign or malignant. This paper overviews the current state-of-the-art techniques that have been developed to implement each of these CAD processing steps. For each technique, various aspects of technical issues, implemented methodologies, training and testing databases, and validation methods, as well as achieved performances, are described. In addition, the paper addresses several challenges that researchers face in each implementation step and outlines the strengths and drawbacks of the existing approaches for lung cancer CAD systems. Several challenges and aspects have been facing CAD systems for lung cancer. These challenges can be summarized as follows.", 'kwd': '-', 'title': u'Computer-Aided Diagnosis Systems for Lung Cancer: Challenges and Methodologies'}], 'Risk Score AND Deep Learning': [], 'Coronary Artery Disease AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123729/', 'p': u'Despite advances in the pharmacologic and interventional treatment of coronary artery disease (CAD), atherosclerosis remains the leading cause of death in Western societies. X-ray coronary angiography has been the modality of choice for diagnosing the presence and extent of CAD. However, this technique is invasive and provides limited information on the composition of atherosclerotic plaque. Coronary computed tomography angiography (CCTA) and cardiac magnetic resonance (CMR) have emerged as promising non-invasive techniques for the clinical imaging of CAD. Hereby, CCTA allows for visualization of coronary calcification, lumen narrowing and atherosclerotic plaque composition. In this regard, data from the CONFIRM Registry recently demonstrated that both atherosclerotic plaque burden and lumen narrowing exhibit incremental value for the prediction of future cardiac events. However, due to technical limitations with CCTA, resulting in false positive or negative results in the presence of severe calcification or motion artifacts, this technique cannot entirely replace invasive angiography at the present time. CMR on the other hand, provides accurate assessment of the myocardial function due to its high spatial and temporal resolution and intrinsic blood-to-tissue contrast. Hereby, regional wall motion and perfusion abnormalities, during dobutamine or vasodilator stress, precede the development of ST-segment depression and anginal symptoms enabling the detection of functionally significant CAD. While CT generally offers better spatial resolution, the versatility of CMR can provide information on myocardial function, perfusion, and viability, all without ionizing radiation for the patients. Technical developments with these 2 non-invasive imaging tools and their current implementation in the clinical imaging of CAD will be presented and discussed herein.The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.', 'kwd': u'coronary artery disease, atherosclerotic plaque, coronary computed tomography, cardiac magnetic resonance, risk stratification', 'title': u'Cardiac magnetic resonance and computed tomography angiography for clinical imaging of stable coronary artery disease. Diagnostic classification and risk stratification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4922813/', 'p': u'After a decade of clinical use of coronary computed tomographic angiography (CCTA) to evaluate the anatomic severity of coronary artery disease, new methods of deriving functional information from CCTA have been developed. These methods utilize the anatomic information provided by CCTA in conjunction with computational fluid dynamics to calculate fractional flow reserve (FFR) values from CCTA image data sets. Computed tomography-derived FFR (CT-FFR) enables the identification of lesion-specific drop noninvasively. A three-dimensional CT-FFR modeling technique, which provides FFR values throughout the coronary tree (HeartFlow FFRCT analysis), has been validated against measured FFR and is now approved by the US Food and Drug Administration for clinical use. This technique requires off-site supercomputer analysis. More recently, a one-dimensional computational analysis technique (Siemens cFFR), which can be performed on on-site workstations, has been developed and is currently under investigation. This article reviews CT-FFR technology and clinical evidence for its use in stable patients with suspected coronary artery disease.Intermediate degrees of stenosis (30%\u201370%) present the greatest challenge in the diagnosis of CAD. Since hemodynamically significant lesions are occasionally observed in intermediate lesions with <70% stenosis,13 the use of invasive FFR is recommended to evaluate the function of intermediate coronary lesions as a class IIa indication.6 However, given the relatively lower prevalence of lesion-specific pressure drop caused by intermediate stenosis compared to that of severe stenosis in the FAME study,13 CT-derived FFR would be of great use for assessing the functional significance of intermediate lesions to avoid unnecessary ICA and help in treatment decision making. Table 2 provides a summary of the studies of FFRCT and cFFR. Similar to the overall diagnostic accuracy of CT-derived FFR, all studies demonstrated high diagnostic performance for intermediate stenosis, with the highest accuracy and specificity for FFRCT.24,31,38,39', 'kwd': u'fractional flow reserve, coronary computed tomographic angiography, FFRCT, cFFR', 'title': u'Noninvasive FFR derived from coronary CT angiography in the management of coronary artery disease: technology and clinical update'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5329750/', 'p': u'Coronary artery disease (CAD) is a leading cause of death and disability worldwide. Cardiovascular magnetic resonance (CMR) is established in clinical practice guidelines with a growing evidence base supporting its use to aid the diagnosis and management of patients with suspected or established CAD. CMR is a multi-parametric imaging modality that yields high spatial resolution images that can be acquired in any plane for the assessment of global and regional cardiac function, myocardial perfusion and viability, tissue characterisation and coronary artery anatomy, all within a single study protocol and without exposure to ionising radiation. Advances in technology and acquisition techniques continue to progress the utility of CMR across a wide spectrum of cardiovascular disease, and the publication of large scale clinical trials continues to strengthen the role of CMR in daily cardiology practice. This article aims to review current practice and explore the future directions of multi-parametric CMR imaging in the investigation of stable CAD.Although 1.5T is remains the standard field strength used in clinical CMR, imaging at a higher field strength of 3.0T offers increased signal to noise and contrast to noise ratios thereby giving improved spatial and temporal enhancement[27]. Consequently the diagnostic accuracy of perfusion imaging at 3.0T may be improved, and in a small direct comparison of CMR perfusion at 1.5T, 3.0T (n = 61) showed greater diagnostic accuracy in both single vessel (AUC: 0.89 vs 0.70; P < 0.05) and multi-vessel disease (AUC: 0.95 vs 0.82, P < 0.05)[28]. Furthermore, 3.0T has been compared to 1.5T using FFR as reference standard, corroborating it\u2019s superior diagnostic accuracy[29,30]. The higher 3.0T field strength does however pose challenges with greater field inhomogeneity, susceptibility artefacts and higher local energy deposition. Also, many implants deemed \u201cMR compatible\u201d at 1.5T cannot be scanned at 3.0T[31]. These issues are however being overcome with improved technology and the use of multi-transmit radiofrequency CMR techniques improving field homogeneity[32].', 'kwd': u'Cardiovascular magnetic resonance, Coronary heart disease, Myocardial perfusion, Viability, Prognosis', 'title': u'Assessment of stable coronary artery disease by cardiovascular magnetic resonance imaging: Current and emerging techniques'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4788549/', 'p': u'Total atherosclerotic plaque burden assessment by CT angiography (CTA) is a promising tool for diagnosis and prognosis of coronary artery disease (CAD) but its validation is restricted to small clinical studies. We tested the feasibility of semi-automatically derived coronary atheroma burden assessment for identifying patients with hemodynamically significant CAD in a large cohort of patients with heterogenous characteristics.This study focused on the CTA component of the CORE320 study population. A semi-automated contour detection algorithm quantified total coronary atheroma volume defined as the difference between vessel and lumen volume. Percent atheroma volume (PAV = [total atheroma volume/total vessel volume]\xd7100) was the primary metric for assessment (n=374). The area under the receiver operating characteristic curve (AUC) determined the diagnostic accuracy for identifying patients with hemodynamically significant CAD defined as \u226550% stenosis by quantitative coronary angiography and associated myocardial perfusion abnormality by SPECT.', 'kwd': '-', 'title': u'Total Coronary Atherosclerotic Plaque Burden Assessment by CT Angiography for Detecting Obstructive Coronary Artery Disease Associated with Myocardial Perfusion Abnormalities'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2947500/', 'p': u'Conceived and designed the experiments: WJM JW JMAvE RMB TL. Performed the experiments: SCG. Analyzed the data: SCG MEK AGK SS TL. Contributed reagents/materials/analysis tools: RJvdG. Wrote the paper: SCG MEK AGK SS MK RJvdG WJM JW JMAvE RMB TL. Data interpretation and supervision: MEK JvE TL. Statistical analysis: AK. Assistance with data acquisition in early phase of study: MK. Developed MR method: RB. Handled funding: TL.Magnetic resonance imaging (MRI) is sensitive to early atherosclerotic changes such as positive remodeling in patients with coronary artery disease (CAD). We assessed prevalence, quality, and extent of coronary atherosclerosis in a group of healthy subjects compared to patients with confirmed CAD.', 'kwd': '-', 'title': u'Visualization of Coronary Wall Atherosclerosis in Asymptomatic Subjects and Patients with Coronary Artery Disease Using Magnetic Resonance Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479873/', 'p': u"Coronary CT angiography has been increasingly used in the diagnosis of coronary artery disease owing to rapid technological developments, which are reflected in the improved spatial and temporal resolution of the images. High diagnostic accuracy has been achieved with multislice CT scanners (64 slice and higher), and in selected patients coronary CT angiography is regarded as a reliable alternative to invasive coronary angiography. With high-quality coronary CT imaging increasingly being performed, patients can benefit from an imaging modality that provides a rapid and accurate diagnosis while avoiding an invasive procedure. Despite the tremendous contributions of coronary CT angiography to cardiac imaging, study results reported in the literature should be interpreted with caution as there are some limitations existing within the study design or related to patient risk factors. In addition, some attention must be given to the potential health risks associated with the ionising radiation received during cardiac CT examinations. Radiation dose associated with coronary CT angiography has raised serious concerns in the literature, as the risk of developing malignancy is not negligible. Various dose-saving strategies have been implemented, with some of the strategies resulting in significant dose reduction. The aim of this review is to present an overview of the role of coronary CT angiography on cardiac imaging, with focus on coronary artery disease in terms of the diagnostic and prognostic value of coronary CT angiography. Various approaches for dose reduction commonly recommended in the literature are discussed. Limitations of coronary CT angiography are identified. Finally, future directions and challenges with the use of coronary CT angiography are highlighted.There is no doubt that, with increasing technological improvements, coronary CT angiography will continue to play an important role in the detection and diagnosis of CAD. Justification is a shared responsibility between requesting physicians and radiologists. For cardiac imaging exposures, the primary tasks of the medical imaging specialists are to collaborate with referring cardiologists to direct patients to the most appropriate imaging modality for the required diagnostic task and to ensure that all technical aspects of the examination are optimised so that the acquired image quality is diagnostic while keeping the doses as low as possible [97]. This is particularly important for young individuals, especially women, for whom alternative diagnostic modalities that do not involve the use of ionising radiation should be considered, such as stress electrocardiography, echocardiography or MRI. The benefit-to-risk ratio for imaging patients suspected of CAD must be driven by the benefit and appropriateness of the CCTA examination requested by the physicians. The American College of Radiology's appropriateness criteria provide evidence-based guidelines to help physicians in recommending an appropriate imaging test [98]. Similarly, the European Commission's guidelines and UK's Royal College of Radiologists' referral guidelines for imaging also provide a detailed overview of clinical indications for imaging examinations including CT [99]. Physicians need to follow guidelines like national diagnostic reference levels for reducing radiation dosages.", 'kwd': '-', 'title': u'Coronary CT angiography: current status and continuing challenges'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4613789/', 'p': u'Diagnosis and management of coronary artery disease represent a major challenge to our health care systems affecting millions of patients each year. Until recently, the diagnosis of coronary artery disease could be conclusively determined only by invasive coronary angiography. To avoid risks from cardiac catheterization, many healthcare systems relied on stress testing as gatekeeper for coronary angiography. Advancements in cardiac computed tomography angiography technology now allows to noninvasively visualize coronary artery disease, challenging the role of stress testing as the default noninvasive imaging tool for evaluating patients with chest pain. In this review, we summarize current data on the clinical utility of cardiac computed tomography and stress testing in stable patients with suspected coronary artery disease.Cardiovascular diseases, and particular, coronary artery disease (CAD), remain the leading cause of death worldwide with an enormous burden on health care systems [1]. Annually, more than 10 million stress tests and approximately one million diagnostic cardiac catheterizations are being performed in the U.S. alone [1]. Total costs of cardiovascular disease and stroke in the U.S. for 2015 are estimated to exceed 320 billion dollars [1]. Management of CAD requires an accurate diagnosis. For many decades, invasive coronary angiography (ICA) has served as the gold standard for the diagnosis of CAD despite many well recognized limitations of this seasoned technology [2;3]. To avoid risks from cardiac catheterization in low-intermediate risk patients, we have been using myocardial stress testing as gatekeeper for invasive angiography. The emergence of multi-detector computed tomography technology has allowed to noninvasively assess the presence, location, severity, and characteristics of coronary atherosclerotic disease in patients. In recent years, an abundance of clinical studies revealed data on the diagnostic and prognostic performance of cardiac computed tomography angiography (CCTA), challenging the role of stress testing as the default noninvasive test for patients presenting with non-acute chest pain. In this paper, we review current data on the clinical utility of CCTA vs. stress testing in stable patients with suspected CAD.', 'kwd': u'Coronary heart disease, cardiac computed tomography angiography, stress imaging, myocardial perfusion imaging, single-photon-emission tomography', 'title': u'Cardiac CT vs. Stress Testing in Patients with Suspected\nCoronary Artery Disease: Review and Expert Recommendations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3665274/', 'p': u'Coronary endothelial function (endoFx) is abnormal in patients with established coronary artery disease (CAD) and was recently shown by MRI to relate to the severity of luminal stenosis. Recent advances in MRI now allow the non-invasive assessment of both anatomic and functional (endoFx) changes that previously required invasive studies. We tested the hypothesis that abnormal coronary endoFx is related to measures of early atherosclerosis such as increased coronary wall thickness (CWT).Seventeen arteries in fourteen healthy adults and seventeen arteries in fourteen patients with non-obstructive CAD were studied. To measure endoFx, coronary MRI was performed before and during isometric handgrip exercise, an endothelial-dependent stressor and changes in coronary cross-sectional area (CSA) and flow were measured. Black blood imaging was performed to quantify CWT and other indices of arterial remodeling. The mean stress-induced change in CSA was significantly higher in healthy adults (13.5%\xb112.8%, mean\xb1SD, n=17) than in those with mildly diseased arteries (-2.2\xb16.8%, p<0.0001, n=17). Mean CWT was lower in healthy subjects (0.9\xb10.2mm) than in CAD patients (1.4\xb10.3mm, p<0.0001). In contrast to healthy subjects, stress-induced changes in CSA, a measure of coronary endoFx, correlated inversely with CWT in CAD patients (r= -0.73, p=0.0008).', 'kwd': u'coronary disease, endothelium, magnetic resonance imaging', 'title': u'Regional Coronary Endothelial Function is Closely Related to Local Early Coronary Atherosclerosis in Patients with Mild Coronary Artery Disease: A Pilot Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3729736/', 'p': u'The purpose of this study is to (1) investigate the image quality of phase-sensitive dual inversion recovery (PS-DIR) coronary wall imaging in healthy subjects and in subjects with known coronary artery disease (CAD) and to (2) investigate the utilization of PS-DIR at 3T in the assessment of coronary artery thickening in subjects with asymptomatic but variable degrees of CAD.A total of 37 subjects participated in this Institutional Review Board approved and HIPAA-compliant study. These included 21 subjects with known CAD as identified on Multi-Detector CT angiography (MDCT). Sixteen healthy subjects without known history of CAD were included. All subjects were scanned using free-breathing PS-DIR MRI for the assessment of coronary wall thickness at 3T. Lumen-tissue contrast-to-noise ratio (CNR), signal-to-noise (SNR), and quantitative vessel parameters including lumen area and wall thickness were measured. Statistical analyses were performed.', 'kwd': u'Atherosclerosis, coronary artery imaging, vessel wall, black blood MRI, Phase sensitive, dual inversion recovery, 3T', 'title': u'Feasibility of Coronary Artery Wall Thickening Assessment in Asymptomatic Coronary Artery Disease using Phase-Sensitive Dual Inversion Recovery MRI at 3T'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3598430/', 'p': u'To date, the therapeutic benefit of revascularization vs. medical therapy for stable individuals undergoing invasive coronary angiography (ICA) based upon coronary computed tomographic angiography (CCTA) findings has not been examined.We examined 15 223 patients without known coronary artery disease (CAD) undergoing CCTA from eight sites and six countries who were followed for median 2.1 years (interquartile range 1.4\u20133.3 years) for an endpoint of all-cause mortality. Obstructive CAD by CCTA was defined as a \u226550% luminal diameter stenosis in a major coronary artery. Patients were categorized as having high-risk CAD vs. non-high-risk CAD, with the former including patients with at least obstructive two-vessel CAD with proximal left anterior descending artery involvement, three-vessel CAD, and left main CAD. Death occurred in 185 (1.2%) patients. Patients were categorized into two treatment groups: revascularization (n = 1103; 2.2% mortality) and medical therapy (n = 14 120, 1.1% mortality). To account for non-randomized referral to revascularization, we created a propensity score developed by logistic regression to identify variables that influenced the decision to refer to revascularization. Within this model (C index 0.92, \u03c72 = 1248, P < 0.0001), obstructive CAD was the most influential factor for referral, followed by an interaction of obstructive CAD with pre-test likelihood of CAD (P = 0.0344). Within CCTA CAD groups, rates of revascularization increased from 3.8% for non-high-risk CAD to 51.2% high-risk CAD. In multivariable models, when compared with medical therapy, revascularization was associated with a survival advantage for patients with high-risk CAD [hazards ratio (HR) 0.38, 95% confidence interval 0.18\u20130.83], with no difference in survival for patients with non-high-risk CAD (HR 3.24, 95% CI 0.76\u201313.89) (P-value for interaction = 0.03).', 'kwd': u'Computed tomography, Coronary revascularization, Medical therapy, Coronary artery disease', 'title': u'All-cause mortality benefit of coronary revascularization vs. medical therapy in patients without known coronary artery disease undergoing coronary computed tomographic angiography: results from CONFIRM (COronary CT Angiography EvaluatioN For Clinical Outcomes: An InteRnational Multicenter Registry)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5594598/', 'p': u'The use of coronary MR angiography (CMRA) in patients with coronary artery disease (CAD) remains limited due to the long scan times, unpredictable and often non-diagnostic image quality secondary to respiratory motion artifacts. The purpose of this study was to evaluate CMRA with image-based respiratory navigation (iNAV CMRA) and compare it to gold standard invasive x-ray coronary angiography in patients with CAD.Consecutive patients referred for CMR assessment were included to undergo iNAV CMRA on a 1.5\xa0T scanner. Coronary vessel sharpness and a visual score were assigned to the coronary arteries. A diagnostic reading was performed on the iNAV CMRA data, where a lumen narrowing >50% was considered diseased. This was compared to invasive x-ray findings.', 'kwd': u'Coronary MR angiography, Image navigators, Respiratory motion correction, Coronary artery disease', 'title': u'Diagnostic performance of image navigated coronary CMR angiography in patients with coronary artery disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5102483/', 'p': '-', 'kwd': u'Coronary Artery Disease / diagnoses, Scintigraphy, Calcium Signaling, Tomography, Emission Computed', 'title': u'Relationship between Calcium Score and Myocardial Scintigraphy in the\nDiagnosis of Coronary Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3377563/', 'p': u'The objective of the analysis is to determine the diagnostic accuracy of stress echocardiography (ECHO) in the diagnosis of patients with suspected coronary artery disease (CAD) compared to coronary angiography (CA).The objective of the analysis is to determine the diagnostic accuracy of stress echocardiography (stress ECHO) in the diagnosis of patients with suspected coronary artery disease (CAD).', 'kwd': '-', 'title': u'Stress Echocardiography for the Diagnosis of Coronary Artery Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422837/', 'p': u'During the last decade coronary computed tomography angiography (CTA) has become the preeminent non-invasive imaging modality to detect coronary artery disease (CAD) with high accuracy. However, CTA has a limited value in assessing the hemodynamic significance of a given stenosis due to a modest specificity and positive predictive value. In recent years, different CT techniques for detecting myocardial ischemia have emerged, such as CT-derived fractional flow reserve (FFR-CT), transluminal attenuation gradient (TAG), and myocardial CT perfusion (CTP) imaging. Myocardial CTP imaging can be performed with a single static scan during first pass of the contrast agent, with monoenergetic or dual-energy acquisition, or as a dynamic, time-resolved scan during stress by using coronary vasodilator agents (adenosine, dipyridamole, or regadenoson). A number of CTP techniques are available, which can assess myocardial perfusion in both a qualitative, semi-quantitative or quantitative manner. Once used primarily as research tools, these modalities are increasingly being used in routine clinical practice. All these techniques offer the substantial advantage of combining anatomical and functional evaluation of flow-limiting coronary stenosis in the same examination that would be beneficial for clinical decision-making. This review focuses on the state-of the-art and future trends of these evolving imaging modalities in the field of cardiology for the physiologic assessments of CAD.The FFR-CT method allows the extraction of \u2018stress induced\u2019 quantitative functional information from an anatomic CTA of at least moderate quality acquired at rest without adenosine infusion. The method uses computational fluid dynamics with simulated hyperemia to calculate the FFR measurement at any point in the vascular tree (39). The concept of coronary FFR, defined as the ratio of the mean coronary pressure distal to a coronary stenosis to the mean aortic pressure during maximal coronary blood flow, has evolved into an accepted functional measure of stenosis severity since first proposed 15 years ago (22). FFR has now become the invasive gold standard for assessing lesion-specific ischemia. A FFR value less than 0.80 or less than 0.75 identifies hemodynamic significance of coronary stenosis (7,8). FFR-CT correlates well with invasive-derived FFR measurements in patients with suspected or known CAD (17,39,40). One of the temporary drawbacks is that FFR-CT needs extreme computational ability and analysis time, which hampers widespread dissemination.', 'kwd': u'Cardiac computed tomography, coronary artery disease (CAD), stress imaging, myocardial perfusion imaging, myocardial blood flow quantification', 'title': u'Myocardial blood flow quantification for evaluation of coronary artery disease by computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4499869/', 'p': u'The initial course of atherosclerotic disease is thought to begin in early adulthood. In young adults lesions in the arterial vessel wall have been observed surprisingly frequently13 but the prognostic relevance of early, adaptive or reversible changes like \u201cfatty streak\u201d or intimal thickening remains a matter of debate. Pathology studies seek to integrate autopsy findings from various stages of atherosclerosis to provide a putative sequence of events4. In brief, intimal thickening is observed early in the disease process. The early atherosclerotic lesion is composed of smooth muscle cells and is affected by increased macrophage and lipid influx. If this process continues, a necrotic core is formed and the lesion progresses to a fibrous cap atheroma. The necrotic core contains lipids and apoptotic macrophages. A stable fibrous cap may prevent rupture of the lesion. If the fibrous cap loses matrix proteins and smooth muscle cells, a thin cap atheroma can result. Intraplaque hemorrhage is also seen frequently in this entity, leading to further enlargement of the lipid core. The risk of plaque rupture is increased as the fibrous cap thins and the lipid core enlarges14. The \u201cfibrocalcific plaque\u201d is considered to be a feature of more stable plaque, although the processes involved in calcification are not fully understood.It is generally conceived that therapeutic intervention for atherosclerosisis most effective when started at an early stage of the progressive disease process 15. Imaging tools have provided a substantial database of knowledge regarding disease burden. Imaging of the larger surface vessels (carotid or femoral arteries) has been extensively used to detect early systemic vascular pathology 16. Calcium detection using non-contrast CT provides a direct approach to assessing coronary atherosclerosis burden. The coronary artery calcium score has strong predictive power for cardiovascular events in asymptomatic subjects17. However, calcium deposition is felt to be a late event in the formation of atherosclerotic plaque. The relevance of non-calcified plaque is emphasized by prospective IVUS studies that show coronary fibroatheroma without significant calcification confers an elevated risk for myocardial infarction 18. Non-calcified plaque is more common than calcified plaque in asymptomatic individuals younger than 45 years. The ability to noninvasively image non-calcified plaque or wall thickening of the coronary arteries using MRI or CT enables the detection of earlier stages of atherosclerotic disease 19, 20.', 'kwd': u'imaging, coronary disease, plaque, atherosclerosis', 'title': u'Noninvasive Imaging of Atherosclerotic Plaque Progression: Status of Coronary CT Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3314981/', 'p': u'A sexual dimorphism exists in the incidence and prevalence of coronary artery disease\u2014men are more commonly affected than are age-matched women. We explored the role of the Y chromosome in coronary artery disease in the context of this sexual inequity.We genotyped 11 markers of the male-specific region of the Y chromosome in 3233 biologically unrelated British men from three cohorts: the British Heart Foundation Family Heart Study (BHF-FHS), West of Scotland Coronary Prevention Study (WOSCOPS), and Cardiogenics Study. On the basis of this information, each Y chromosome was tracked back into one of 13 ancient lineages defined as haplogroups. We then examined associations between common Y chromosome haplogroups and the risk of coronary artery disease in cross-sectional BHF-FHS and prospective WOSCOPS. Finally, we undertook functional analysis of Y chromosome effects on monocyte and macrophage transcriptome in British men from the Cardiogenics Study.', 'kwd': '-', 'title': u'Inheritance of coronary artery disease in men: an analysis of the role of the Y chromosome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566965/', 'p': u'More than a million diagnostic cardiac catheterizations are performed annually in the US for evaluation of coronary artery anatomy and the presence of atherosclerosis. Nearly half of these patients have no significant coronary lesions or do not require mechanical or surgical revascularization. Consequently, the ability to rule out clinically significant coronary artery disease (CAD) using low cost, low risk tests of serum biomarkers in even a small percentage of patients with normal coronary arteries could be highly beneficial.Serum from 359 symptomatic subjects referred for catheterization was interrogated for proteins involved in atherogenesis, atherosclerosis, and plaque vulnerability. Coronary angiography classified 150 patients without flow-limiting CAD who did not require percutaneous intervention (PCI) while 209 required coronary revascularization (stents, angioplasty, or coronary artery bypass graft surgery). Continuous variables were compared across the two patient groups for each analyte including calculation of false discovery rate (FDR \u2264 1%) and Q value (P value for statistical significance adjusted to \u2264 0.01).', 'kwd': u'atherosclerosis, biomarkers, cardiac catheterization, coronary angiography, coronary stenosis, multiplex proteomics', 'title': u'Serum protein profiles predict coronary artery disease in symptomatic patients referred for coronary angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4395291/', 'p': u'Conceived and designed the experiments: JHZ HLK KNJ. Performed the experiments: JBS YHC WYC SHK MAK. Analyzed the data: HLK KNJ. Contributed reagents/materials/analysis tools: JBS YHC WYC SHK MAK. Wrote the paper: HLK KNJ JHZ.The aim of this study was to investigate whether brachial-ankle pulse wave velocity (baPWV) is associated with the severity of coronary artery disease (CAD) assessed by coronary computed tomography angiography (CCTA), and to evaluate baPWV as a predictor of obstructive CAD on CCTA. A total of 470 patients who underwent both baPWV and CCTA were included. We evaluated stenosis degree and plaque characteristics on CCTA. To estimate the severity of CAD, we calculated the number of segment with plaque (segment involvement score; SIS), stenosis degree-weighted plaque score (segment stenosis score; SSS), and coronary artery calcium score (CACS). The mean baPWV was 1,485 \xb1 315 cm/s (range, 935-3,175 cm/s). Non-obstructive (stenosis < 50%) and obstructive (stenosis \u2265 50%) CAD was found in 129 patients (27.4%) and 144 (30.6%), respectively. baPWV in patients with obstructive CAD was higher than that of patients with non-obstructive (1,680 \xb1 396 cm/s versus 1,477 \xb1 244 cm/s, P < 0.001) or no CAD (1,680 \xb1 396 cm/s versus \xb1 196 1,389 cm/s, P < 0.001). baPWV showed significant correlation with SSS (r = 0.429, P < 0.001), SIS (r = 0.395, P < 0.001), CACS (r 0.346, P < 0.001), and the number of segment with non-calcified plaque (r 0.092, P = 0.047), mixed plaque (r = 0.267, P < 0.001), and calcified plaque (r = 0.348, P < 0.001), respectively. The optimal baPWV cut-off value for the detection of obstructive CAD was 1,547 cm/s. baPWV \u2265 1,547 cm/s was independent predictor for the obstructive CAD. In conclusion, baPWV is well correlated with the severity of CAD evaluated by CCTA. baPWV has the potential to predict severity of coronary artery atherosclerosis.', 'kwd': '-', 'title': u'The Association of Brachial-Ankle Pulse Wave Velocity with Coronary Artery Disease Evaluated by Coronary Computed Tomography Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5553869/', 'p': u'The major issue in coronary heart disease (CHD) diagnosis and management is that symptoms onset in an advanced state of disease. Despite the availability of several clinical risk scores, the prediction of cardiovascular events is lacking, and many patients at risk are not well stratified according to the canonical risk factors alone. Therefore, adequate risk assessment remains the most challenging issue. Recently, the integration of imaging data with biochemical markers in a radiogenomic framework has been proposed in many fields of medicine as well as in cardiology. Multimodal imaging and advanced processing techniques can provide both direct (e.g., remodeling index, calcium score, total plaque volume, plaque burden) and indirect (e.g., myocardial perfusion index, coronary flow reserve) imaging features of CHD. Furthermore, the identification of novel non-invasive biochemical markers, mainly focused on plasma and/or serum samples, has increased the specificity of findings, reflecting several pathophysiological pathways of atherosclerosis, the principal actor in CHD. In this context, a multifaced approach, derived from the strengths of all these modalities, appears promising for finer risk stratification and treatment strategies, facilitating the decision-making and clinical management of patients. This review underlines the role of different imaging modalities in the quantification of coronary atherosclerosis and describes novel blood-based markers that could improve diagnosis and have a better predictive value in CHD.The need to improve diagnosis and risk prediction has prompted the search for novel markers in cardiovascular medicine. Literature data suggest that CTCA could substantially reduce the number of invasive procedures, increasing the safety of patients, and allows a more precise planning of potential treatment options. Furthermore, strong evidence has also emerged on the usefulness of coronary calcium score assessed by CTCA. In association with imaging improvements, novel high-throughput platforms investigating proteomic, metabolomic, epigenomic, and transcriptomics profiles together with genome-wide association studies may generate \u201cmultimarker CHD scores\u201d with a higher predictive power than the use of a single biomarker. Surrogate biomarkers of coronary atherosclerosis and advanced imaging techniques could represent important cornerstones to characterize sub-clinical and clinical atherosclerosis with a consequent facilitation in the decision-making and clinical management of patients.', 'kwd': u'Atherosclerosis, coronary heart disease, imaging, biomarkers', 'title': u'An integrated approach to coronary heart disease diagnosis and clinical management'}], 'Tomography AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3775479/', 'p': u'The classic imaging geometry for computed tomography is for collection of un-truncated projections and reconstruction of a global image, with the Fourier transform as the theoretical foundation that is intrinsically non-local. Recently, interior tomography research has led to theoretically exact relationships between localities in the projection and image spaces and practically promising reconstruction algorithms. Initially, interior tomography was developed for x-ray computed tomography. Then, it has been elevated as a general imaging principle. Finally, a novel framework known as \u201comni-tomography\u201d is being developed for grand fusion of multiple imaging modalities, allowing tomographic synchrony of diversified features.Acquisition of less projection data is achieved with a narrower beam, and an object larger than the beam width is not a concern anymore. In other words, interior tomography can handle objects larger than a field of view. This flexibility can certainly enhance the utility of a CT scanner. In nano-CT studies, one reduces a sample into a narrow x-ray beam for complete projection profiles. In this tedious process, morphological and functional damages may be induced. Supported by an NSF/MRI grant and in collaboration with Xradia, we are developing a next generation nano-CT system capable of focusing on an ROI and reconstructing it accurately within a large object (Figure 7). In a geo-science project, fossils from the Ediacaran Doushantuo Formation (ca. 551\u2013635 million years old) were investigated, which are too valuable to be broken, and demand interior tomography. Another example is the large patient problem, i.e., a patient is larger than the field of view of a CT scanner, which can now be solved using theoretically exact interior tomography, instead of using conventional approximate methods such as extrapolation for data completion.', 'kwd': u'Biomedical imaging, computed tomography, local tomography, interior tomography, omni-tomography', 'title': u'Meaning of Interior Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5483328/', 'p': u'This paper uses X-ray computed tomography to track the mechanical response of a vertebrate (Barnacle goose) long bone subjected to an axial compressive load, which is increased gradually until failure. A loading rig was mounted in an X-ray computed tomography system so that a time-lapse sequence of three-dimensional (3D) images of the bone\u2019s internal (cancellous or trabecular) structure could be recorded during loading. Five distinct types of deformation mechanism were observed in the cancellous part of the bone. These were (i) cracking, (ii) thinning (iii) tearing of cell walls and struts, (iv) notch formation, (v) necking and (vi) buckling. The results highlight that bone experiences brittle (notch formation and cracking), ductile (thinning, tearing and necking) and elastic (buckling) modes of deformation. Progressive deformation, leading to cracking was studied in detail using digital image correlation. The resulting strain maps were consistent with mechanisms occurring at a finer-length scale. This paper is the first to capture time-lapse 3D images of a whole long bone subject to loading until failure. The results serve as a unique reference for researchers interested in how bone responds to loading. For those using computer modelling, the study not only provides qualitative information for verification and validation of their simulations but also highlights that constitutive models for bone need to take into account a number of different deformation mechanisms.A crack appears at scan 7 (D) in Fig. 7 which opens through scan 8 (E) and scan 9 (F). However, no obvious crack initiation mechanism is visible in the preceding scans. Cracking is examined more closely using DVC later in the paper.', 'kwd': u'X-ray computed tomography, Digital image correlation, Branta leucopsis, Axial loading, Progressive damage, Stress\u2013strain, Deformation mechanisms, Computer modeling, Constitutive, Modeling and simulation', 'title': u'A study of the progression of damage in an axially loaded Branta leucopsis femur using X-ray computed tomography and digital image correlation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684619/', 'p': u'Despite increasing demand, imaging the internal structure of plant organs or tissues without the use of transgenic lines expressing fluorescent proteins remains a challenge. Techniques such as magnetic resonance imaging, optical projection tomography or X-ray absorption tomography have been used with various success, depending on the size and physical properties of the biological material.X-ray in-line phase tomography was applied for the imaging of internal structures of maize seeds at early stages of development, when the cells are metabolically fully active and water is the main cell content. This 3D imaging technique with histology-like spatial resolution is demonstrated to reveal the anatomy of seed compartments with unequalled contrast by comparison with X-ray absorption tomography. An associated image processing pipeline allowed to quantitatively segment in 3D the four compartments of the seed (embryo, endosperm, nucellus and pericarp) from 7 to 21\xa0days after pollination.', 'kwd': u'X ray in-line phase tomography, Image segmentation, Virtual histology, plant development, maize seeds', 'title': u'Fast virtual histology using X-ray in-line phase tomography: application to the 3D anatomy of maize developing seeds'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669980/', 'p': u'The promise of compressive sensing, exploitation of compressibility to achieve high quality image reconstructions with less data, has attracted a great deal of attention in the medical imaging community. At the Compressed Sensing Incubator meeting held in April 2014 at OSA Headquarters in Washington, DC, presentations were given summarizing some of the research efforts ongoing in compressive sensing for x-ray computed tomography and magnetic resonance imaging systems. This article provides an expanded version of these presentations. Sparsity-exploiting reconstruction algorithms that have gained popularity in the medical imaging community are studied, and examples of clinical applications that could benefit from compressive sensing ideas are provided. The current and potential future impact of compressive sensing on the medical imaging field is discussed.For x-ray tomographic imaging, CS is of interest due to the possibilities of x-ray dose reduction, motion artifact mitigation, and novel scan designs. The typical diagnostic CT scan subjects a patient to an x-ray dose of about a factor of one hundred greater than that of a single projection x-ray. Reduction of sampling requirements is one obvious way to reduce the dose burden of CT imaging. Acquiring fewer views than current practice also may allow for faster acquisitions particularly for C-arm imagers with flat-panel x-ray detectors. Faster acquisition times can alleviate imaging problems related to motion due to, e.g., breathing. One of the fast-growing applications of x-ray tomographic imaging is for guidance in radiation therapy or surgery. In such applications, the standard tomographic scan, where the x-ray source executes a complete data arc greater than 180 degrees, may not be possible. Limiting the angular range of the x-ray source scanning arc is thus another important form of data undersampling.', 'kwd': '-', 'title': u'Compressive sensing in medical imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3999310/', 'p': u'Repeated x-ray computed tomography (CT) scans are often required in several specific applications such as perfusion imaging, image-guided biopsy needle, image-guided intervention, and radiotherapy with noticeable benefits. However, the associated cumulative radiation dose significantly increases as comparison with that used in the conventional CT scan, which has raised major concerns in patients. In this study, to realize radiation dose reduction by reducing the x-ray tube current and exposure time (mAs) in repeated CT scans, we propose a prior-image induced nonlocal (PINL) regularization for statistical iterative reconstruction via the penalized weighted least-squares (PWLS) criteria, which we refer to as \u201cPWLS-PINL\u201d. Specifically, the PINL regularization utilizes the redundant information in the prior image and the weighted least-squares term considers a data-dependent variance estimation, aiming to improve current low-dose image quality. Subsequently, a modified iterative successive over-relaxation algorithm is adopted to optimize the associative objective function. Experimental results on both phantom and patient data show that the present PWLS-PINL method can achieve promising gains over the other existing methods in terms of the noise reduction, low-contrast object detection and edge detail preservation.An anthropomorphic torso phantom (Radiology Support Devices, Inc., Long Beach, CA) was used for the experimental data acquisition, as shown in Fig. 1(a). The phantom was scanned by a clinical CT scanner (Siemens SOMATOM Sensation 16 CT) at three exposure levels, i.e., 17, 40, 100 mAs. For each exposure level, the tube voltage was set at 120 kVp and the phantom was scanned in a cine mode at a fixed bed position. Fig. 1(b) shows the CT image reconstructed by the FBP method with an optimized Hamming filter from the sinogram data acquired at 100 mAs, 120 kVp. The deformed images were simulated by mechanically performing a cosine transform warped distortion on the images reconstructed by the FBP method from the sinogram data acquired at 100 mAs, 120 kVp. To obtain the registered prior images, the deformed images were registered to the images reconstructed by the FBP method from the sinogram data acquired at 17 and 40 mAs, respectively.', 'kwd': u'X-ray computed tomography, prior image, statistical iterative reconstruction, penalized weighted least-squares, regularization', 'title': u'Iterative Reconstruction for X-Ray Computed Tomography using Prior-Image Induced Nonlocal Regularization'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4896130/', 'p': u'We describe X-ray computed tomography (CT) datasets from three specimens recovered from Early Cretaceous lakebeds of China that illustrate the forensic interpretation of CT imagery for paleontology. Fossil vertebrates from thinly bedded sediments often shatter upon discovery and are commonly repaired as amalgamated mosaics grouted to a solid backing slab of rock or plaster. Such methods are prone to inadvertent error and willful forgery, and once required potentially destructive methods to identify mistakes in reconstruction. CT is an efficient, nondestructive alternative that can disclose many clues about how a specimen was handled and repaired. These annotated datasets illustrate the power of CT in documenting specimen integrity and are intended as a reference in applying CT more broadly to evaluating the authenticity of comparable fossils.(Figs 1,\u200b,22,\u200b,33,\u200b,44,\u200b,55,\u200b,6;6; see Table 1 for scanning parameters; Table 2 for data output; Table 3 for movies; see also Supplementary Figures; Data Citation 1; additional information is available at: http://digimorph.org/specimens/Confuciusornis_sp/skeleton/). This unnumbered specimen was provided to us for scanning in 1998 by Mr Guan Jian of the Beijing Museum of Natural History, as an early test of whether specimens from the newly discovered Liaoning basin were amenable to CT scanning59. It was reportedly collected from the lower Yixian Formation, but its precise locality within the Liaoning basin is unknown and it came to us with no other documentation. It is now housed in the Institute for Vertebrate Paleontology and Paleoanthropology in Beijing.', 'kwd': u'Research data, Palaeontology, X-ray tomography', 'title': u'X-ray computed tomography datasets for forensic analysis of vertebrate fossils'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359488/', 'p': u'Roots are vital to plants for soil exploration and uptake of water and nutrients. Root performance is critical for growth and yield of plants, in particular when resources are limited. Since roots develop in strong interaction with the soil matrix, tools are required that can visualize and quantify root growth in opaque soil at best in 3D. Two modalities that are suited for such investigations are X-ray Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Due to the different physical principles they are based on, these modalities have their specific potentials and challenges for root phenotyping. We compared the two methods by imaging the same root systems grown in 3 different pot sizes with inner diameters of 34\xa0mm, 56\xa0mm or 81\xa0mm.Both methods successfully visualized roots of two weeks old bean plants in all three pot sizes. Similar root images and almost the same root length were obtained for roots grown in the small pot, while more root details showed up in the CT images compared to MRI. For the medium sized pot, MRI showed more roots and higher root lengths whereas at some spots thin roots were only found by CT and the high water content apparently affected CT more than MRI. For the large pot, MRI detected much more roots including some laterals than CT.', 'kwd': u'X-ray Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Root system architecture, Common bean (Phaseolus vulgaris L.) 3D imaging, Roots in soil, Non-destructive', 'title': u'Direct comparison of MRI and X-ray CT technologies for 3D imaging of root systems in soil: potential and challenges for root trait quantification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478850/', 'p': u'Rhizoctonia solani is a plant pathogenic fungus that causes significant establishment and yield losses to several important food crops globally. This is the first application of high resolution X-ray micro Computed Tomography (X-ray \u03bcCT) and real-time PCR to study host\u2013pathogen interactions in situ and elucidate the mechanism of Rhizoctonia damping-off disease over a 6-day period caused by R. solani, anastomosis group (AG) 2-1 in wheat (Triticum aestivum cv. Gallant) and oil seed rape (OSR, Brassica napus cv. Marinka). Temporal, non-destructive analysis of root system architectures was performed using RooTrak and validated by the destructive method of root washing. Disease was assessed visually and related to pathogen DNA quantification in soil using real-time PCR. R. solani AG2-1 at similar initial DNA concentrations in soil was capable of causing significant damage to the developing root systems of both wheat and OSR. Disease caused reductions in primary root number, root volume, root surface area, and convex hull which were affected less in the monocotyledonous host. Wheat was more tolerant to the pathogen, exhibited fewer symptoms and developed more complex root systems. In contrast, R. solani caused earlier damage and maceration of the taproot of the dicot, OSR. Disease severity was related to pathogen DNA accumulation in soil only for OSR, however, reductions in root traits were significantly associated with both disease and pathogen DNA. The method offers the first steps in advancing current understanding of soil-borne pathogen behavior in situ at the pore scale, which may lead to the development of mitigation measures to combat disease influence in the field.The replicate subset allocated for destructive sampling at 6 dfi (12 columns), were scanned at 2, 4, and 6 days using a Phoenix Nanotom\xae (GE Measurement & Control Solutions, Wunstorf, Germany) X-ray \u03bcCT scanner. The scanner consists of a 180 kV nanofocus X-ray tube fitted with a tungsten transmission target and a 5-megapixel (2304 \xd7 2304 pixels, 50 \xd7 50 \u03bcm pixel size) flat panel detector (Hamamatsu Photonics KK, Shizuoka, Japan). A maximum X-ray energy of 110 kV, 140 \u03bcA current and a 0.15 mm thick copper filter was used to scan each sample which consisted of 1300 projection images acquired over a 360\xb0rotation. Each projection image was the average of three images acquired with a detector exposure time of 500 ms in \u2018Fast CT mode.\u2019 The resulting isotropic voxel edge length was 19 \u03bcm (i.e., spatial resolution) and total scan time was 35 min. The total X-ray dose for each sample was calculated as 25.2 Gy over the three scans, which is below the 33 Gy threshold reported by Johnson (1936) which no detrimental effects of post-germination plant growth following exposure to X-ray radiation were observed (Zappala et al., 2013a). Reconstruction of the projection images was performed using the software datos| rec (GE Measurement & Control Solutions, Wunstorf, Germany) to produce 3-D volumetric data sets with dimension 30 \xd7 30 mm (diameter \xd7 depth).', 'kwd': u'Rhizoctonia solani, X-ray Computed Tomography, qPCR, wheat, oil seed rape, fungi, soil', 'title': u'Effects of damping-off caused by Rhizoctonia solani anastomosis group 2-1 on roots of wheat and oil seed rape quantified using X-ray Computed Tomography and real-time PCR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431860/', 'p': u'Tight gas sandstone samples are imaged at high resolution industrial X-ray computed tomography (ICT) systems to provide a three-dimensional quantitative characterization of the fracture geometries. Fracture networks are quantitatively analyzed using a combination of 2-D slice analysis and 3-D visualization and counting. The core samples are firstly scanned to produce grayscale slices, and the corresponding fracture area, length, aperture and fracture porosity as well as fracture density were measured. Then the 2-D slices were stacked to create a complete 3-D image using volume-rendering software. The open fractures (vug) are colored cyan whereas the calcite-filled fractures (high density objects) are colored magenta. The surface area and volume of both open fractures and high density fractures are calculated by 3-D counting. Then the fracture porosity and fracture aperture are estimated by 3-D counting. The fracture porosity and aperture from ICT analysis performed at atmospheric pressure are higher than those calculated from image logs at reservoir conditions. At last, the fracture connectivity is determined through comparison of fracture parameters with permeability. Distribution of fracture density and fracture aperture determines the permeability and producibility of tight gas sandstones. ICT has the advantage of performing three dimensional fracture imaging in a non-destructive way.A visual inspection of each fracture density and the average aperture were performed on each core segment. The fracture types observed in these core segments consist of extensional fractures (Fig.\xa03A,B) and the coring induced petal fractures (Fig.\xa03C). In addition, there are no fractures could be detected in some samples (Fig.\xa03D). Fractures are sometimes filled with calcites (Fig.\xa03A,B). The directional fracture measurements (azimuth and dip) were conducted on the circumferential CAT scan images. The projection or the sine curve fracture picks are conducted on a mirrored circumferential CAT scan view; so that, it appears that you are looking from within the well bore versus looking at the surface of the core (Fig.\xa03E). These mirrored images should look like and be correlatable to downhole image logs like FMI log (Fig.\xa03E).\n', 'kwd': '-', 'title': u'Three-dimensional quantitative fracture analysis of tight gas sandstones using industrial computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3905628/', 'p': u'Computed tomography (CT) is an X-ray based whole body imaging technique that is widely used in medicine. Clinically approved contrast agents for CT are iodinated small molecules or barium suspensions. Over the past seven years there has been a great increase in the development of nanoparticles as CT contrast agents. Nanoparticles have several advantages over small molecule CT contrast agents, such as long blood-pool residence times, and the potential for cell tracking and targeted imaging applications. Furthermore, there is a need for novel CT contrast agents, due to the growing population of renally impaired patients and patients hypersensitive to iodinated contrast. Micelles and lipoproteins, a micelle-related class of nanoparticle, have notably been adapted as CT contrast agents. In this review we discuss the principles of CT image formation and the generation of CT contrast. We discuss the progress in developing non-targeted, targeted and cell tracking nanoparticle CT contrast agents. We feature agents based on micelles and used in conjunction with spectral CT. The large contrast agent doses needed will necessitate careful toxicology studies prior to clinical translation. However, the field has seen tremendous advances in the past decade and we expect many more advances to come in the next decade.Cell tracking is the process of imaging the delivery and movements of cells in vivo. This is frequently achieved by labeling cells ex vivo, injecting them into the subject and using an imaging technique to track the cells over time. This was first pursued using nuclear imaging in combination with indium-111 labeling.(146) Cell tracking has been extensively pursued for MRI by loading cells with iron oxides and has been used to study stem cell therapies and monocyte behavior, for example.(147-149) This topic has barely begun to be explored for CT. The Bulte group has published several reports on tracking pancreatic islet cells that are encapsulated in alginate.(150) These capsules may be made inherently radiopaque by using barium or bismuth ions to cross-link the alginate, and CT imaging has been used to track such capsules.(151) This group has also explored loading the alginate capsules with the gadolinium labeled gold nanoparticles developed by Alric et al., which were mentioned above.(51) These capsules can be detected with CT, T1-weighted MRI and ultrasound and co-encapsulation of the cells with the nanoparticles had no effect on the cell viability (CT resolution 83 \u03bcm). Furthermore, perfluorooctylbromide nanoemulsions have also been included in these capsules, allowing their detection by CT, 19F MRI and ultrasound.(152) The resolution of the in vivo imaging experiments in this study was 353 \u03bcm. Menk et al. used gold nanoparticles coated with horse serum proteins to label cancer cells.(153) These cells were injected into brains of rats and their distribution was imaged with small animal CT systems (30 \u03bcm resolution). The application of CT to cell tracking is in its infancy, and progress will likely be challenging due to the poor sensitivity of CT and toleration by the cells of very high levels of contrast media loading. Nevertheless, we anticipate more reports on this topic in the coming years.', 'kwd': u'nanoparticle, micelle, computed tomography, X-ray, spectral CT, iodine, gold nanoparticle, lipoprotein, molecular imaging, bismuth', 'title': u'Nanoparticle Contrast Agents for Computed Tomography: A Focus on Micelles'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4750279/', 'p': u'Lung diseases (resulting from air pollution) require a widely accessible method for risk estimation and early diagnosis to ensure proper and responsive treatment. Radiomics-based fractal dimension analysis of X-ray computed tomography attenuation patterns in chest voxels of mice exposed to different air polluting agents was performed to model early stages of disease and establish differential diagnosis.To model different types of air pollution, BALBc/ByJ mouse groups were exposed to cigarette smoke combined with ozone, sulphur dioxide gas and a control group was established. Two weeks after exposure, the frequency distributions of image voxel attenuation data were evaluated. Specific cut-off ranges were defined to group voxels by attenuation. Cut-off ranges were binarized and their spatial pattern was associated with calculated fractal dimension, then abstracted by the fractal dimension -- cut-off range mathematical function. Nonparametric Kruskal-Wallis (KW) and Mann\u2013Whitney post hoc (MWph) tests were used.', 'kwd': u'Fractal dimension, Radiomics, In vivo micro-CT, Air pollution, Lung disease', 'title': u'Radiomics-based differentiation of lung disease models generated by polluted air based on X-ray computed tomography data'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4424483/', 'p': u'We introduce phase-diagram analysis, a standard tool in compressed sensing (CS), to the X-ray computed tomography (CT) community as a systematic method for determining how few projections suffice for accurate sparsity-regularized reconstruction. In CS, a phase diagram is a convenient way to study and express certain theoretical relations between sparsity and sufficient sampling. We adapt phase-diagram analysis for empirical use in X-ray CT for which the same theoretical results do not hold. We demonstrate in three case studies the potential of phase-diagram analysis for providing quantitative answers to questions of undersampling. First, we demonstrate that there are cases where X-ray CT empirically performs comparably with a near-optimal CS strategy, namely taking measurements with Gaussian sensing matrices. Second, we show that, in contrast to what might have been anticipated, taking randomized CT measurements does not lead to improved performance compared with standard structured sampling patterns. Finally, we show preliminary results of how well phase-diagram analysis can predict the sufficient number of projections for accurately reconstructing a large-scale image of a given sparsity by means of total-variation regularization.Many forms of randomness can be conceived in CT sampling. In this work, we consider two straightforward ones. The first is a fan-beam geometry denoted fanbeam_rand in which the source angular positions are no longer equi-distant but sampled uniformly from [0,360\xb0]. Second, we consider a set-up we denote random_rays of independent random rays through the image. Each ray is specified by two parameters: the angle of the ray with a fixed coordinate axis and the intersection of the ray with the orthogonal diameter of the disc-shaped image. The angle and intersection are sampled from uniform distributions on [0,180]\xb0 and [\u2212Nside/2,Nside/2], respectively, where Nside is the diameter length and the image is assumed centred around the origin.', 'kwd': u'computed tomography, compressed sensing, image reconstruction, sparsity regularization, sampling', 'title': u'How little data is enough? Phase-diagram analysis of sparsity-regularized X-ray computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4083059/', 'p': u'To realize low-dose imaging in X-ray computed tomography (CT) examination, lowering milliampere-seconds (low-mAs) or reducing the required number of projection views (sparse-view) per rotation around the body has been widely studied as an easy and effective approach. In this study, we are focusing on low-dose CT image reconstruction from the sinograms acquired with a combined low-mAs and sparse-view protocol and propose a two-step image reconstruction strategy. Specifically, to suppress significant statistical noise in the noisy and insufficient sinograms, an adaptive sinogram restoration (ASR) method is first proposed with consideration of the statistical property of sinogram data, and then to further acquire a high-quality image, a total variation based projection onto convex sets (TV-POCS) method is adopted with a slight modification. For simplicity, the present reconstruction strategy was termed as \u201cASR-TV-POCS.\u201d To evaluate the present ASR-TV-POCS method, both qualitative and quantitative studies were performed on a physical phantom. Experimental results have demonstrated that the present ASR-TV-POCS method can achieve promising gains over other existing methods in terms of the noise reduction, contrast-to-noise ratio, and edge detail preservation.Sparse-view CT image reconstruction is known as an ill-posed problem. To address this issue, Sidky et al. proposed a general iterative scheme through successive and repeated applications of POCS operator with TV minimization [19]. The associative objective function can be written as follows:\n', 'kwd': u'(110.7440) X-ray imaging, (100.3190) Inverse problems, (100.3010) Image reconstruction techniques, (110.6960) Tomography', 'title': u'Low-dose X-ray computed tomography image reconstruction with a combined low-mAs and sparse-view protocol'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693932/', 'p': u'Conceived and designed the experiments: SZ JRH SRT SM CJS TP MB SJM. Performed the experiments: SZ JRH SRT SM. Analyzed the data: SZ JRH SRT SM CJS. Contributed reagents/materials/analysis tools: SZ JRH SRT SM CJS TP MB SJM. Wrote the paper: SZ JRH SRT SM CJS TP MB SJM.X-ray Computed Tomography (CT) is a non-destructive imaging technique originally designed for diagnostic medicine, which was adopted for rhizosphere and soil science applications in the early 1980s. X-ray CT enables researchers to simultaneously visualise and quantify the heterogeneous soil matrix of mineral grains, organic matter, air-filled pores and water-filled pores. Additionally, X-ray CT allows visualisation of plant roots in situ without the need for traditional invasive methods such as root washing. However, one routinely unreported aspect of X-ray CT is the potential effect of X-ray dose on the soil-borne microorganisms and plants in rhizosphere investigations. Here we aimed to i) highlight the need for more consistent reporting of X-ray CT parameters for dose to sample, ii) to provide an overview of previously reported impacts of X-rays on soil microorganisms and plant roots and iii) present new data investigating the response of plant roots and microbial communities to X-ray exposure. Fewer than 5% of the 126 publications included in the literature review contained sufficient information to calculate dose and only 2.4% of the publications explicitly state an estimate of dose received by each sample. We conducted a study involving rice roots growing in soil, observing no significant difference between the numbers of root tips, root volume and total root length in scanned versus unscanned samples. In parallel, a soil microbe experiment scanning samples over a total of 24 weeks observed no significant difference between the scanned and unscanned microbial biomass values. We conclude from the literature review and our own experiments that X-ray CT does not impact plant growth or soil microbial populations when employing a low level of dose (<30 Gy). However, the call for higher throughput X-ray CT means that doses that biological samples receive are likely to increase and thus should be closely monitored.', 'kwd': '-', 'title': u'Effects of X-Ray Dose On Rhizosphere Studies Using X-Ray Computed Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4626840/', 'p': u'We report the development of laboratory based hyperspectral X-ray computed tomography which allows the internal elemental chemistry of an object to be reconstructed and visualised in three dimensions. The method employs a spectroscopic X-ray imaging detector with sufficient energy resolution to distinguish individual elemental absorption edges. Elemental distributions can then be made by K-edge subtraction, or alternatively by voxel-wise spectral fitting to give relative atomic concentrations. We demonstrate its application to two material systems: studying the distribution of catalyst material on porous substrates for industrial scale chemical processing; and mapping of minerals and inclusion phases inside a mineralised ore sample. The method makes use of a standard laboratory X-ray source with measurement times similar to that required for conventional computed tomography.A HEXITEC spectroscopic detector was installed in a Nikon XTH 225 system. The HEXITEC detector consists of a 1\u2009mm thick CdTe single crystal detector (20\u2009\xd7\u200920\u2009mm2) bump-bonded to a large area ASIC packaged with a high performance data acquisition system. The detector has 80\u2009\xd7\u200980 pixels on a 250\u2009\u03bcm pitch with an energy resolution of 800\u2009eV at 59.5\u2009keV and 1.5\u2009keV at 141\u2009keV17. During operation each photon event has its energy, pixel position and the frame in which it occurs recorded. Events are processed and histogrammed according to measured energy into 0.25\u2009keV wide bins. We typical use between 400\u2013800\u2009bins, depending on the maximum X-ray energy. Normally, during this process, a correction is employed to deal with photons that may have shared its energy between two or more pixels which appear to be measured as multiple lower energy photon measurements on neighbouring pixels. When the flux is sufficiently low it is possible to identify these shared events and reconstruct the correct photon energy in the correct location. However due to a high flux of radiation and therefore a high percentage occupancy of events per frame (making it very difficult to identify shared events), we did not employ a charge sharing correction strategy in this case. Previous studies have shown that this does not significantly impact on the measured position of an absorption edge24,41. An inter pixel energy calibration was performed using a correlative optimised warping algorithm using data from a flat-field fluorescence image off a series of metals42.', 'kwd': '-', 'title': u'3D chemical imaging in the laboratory by hyperspectral X-ray computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4497654/', 'p': u'Conceived and designed the experiments: WY LZ. Performed the experiments: WY. Analyzed the data: WY LZ. Contributed reagents/materials/analysis tools: WY LZ. Wrote the paper: WY LZ.In medical and industrial applications of computed tomography (CT) imaging, limited by the scanning environment and the risk of excessive X-ray radiation exposure imposed to the patients, reconstructing high quality CT images from limited projection data has become a hot topic. X-ray imaging in limited scanning angular range is an effective imaging modality to reduce the radiation dose to the patients. As the projection data available in this modality are incomplete, limited-angle CT image reconstruction is actually an ill-posed inverse problem. To solve the problem, image reconstructed by conventional filtered back projection (FBP) algorithm frequently results in conspicuous streak artifacts and gradual changed artifacts nearby edges. Image reconstruction based on total variation minimization (TVM) can significantly reduce streak artifacts in few-view CT, but it suffers from the gradual changed artifacts nearby edges in limited-angle CT. To suppress this kind of artifacts, we develop an image reconstruction algorithm based on \u21130 gradient minimization for limited-angle CT in this paper. The \u21130-norm of the image gradient is taken as the regularization function in the framework of developed reconstruction model. We transformed the optimization problem into a few optimization sub-problems and then, solved these sub-problems in the manner of alternating iteration. Numerical experiments are performed to validate the efficiency and the feasibility of the developed algorithm. From the statistical analysis results of the performance evaluations peak signal-to-noise ratio (PSNR) and normalized root mean square distance (NRMSD), it shows that there are significant statistical differences between different algorithms from different scanning angular ranges (p<0.0001). From the experimental results, it also indicates that the developed algorithm outperforms classical reconstruction algorithms in suppressing the streak artifacts and the gradual changed artifacts nearby edges simultaneously.', 'kwd': '-', 'title': u'\u21130 Gradient Minimization Based Image Reconstruction for Limited-Angle Computed Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4570663/', 'p': u'Conceived and designed the experiments: SH PB CW WO. Performed the experiments: SH CW WO. Analyzed the data: SH RML. Contributed reagents/materials/analysis tools: SH CW RML WO. Wrote the paper: SH PB CW RML WO.There is currently a significant need to improve our understanding of the factors that control a number of critical soil processes by integrating physical, chemical and biological measurements on soils at microscopic scales to help produce 3D maps of the related properties. Because of technological limitations, most chemical and biological measurements can be carried out only on exposed soil surfaces or 2-dimensional cuts through soil samples. Methods need to be developed to produce 3D maps of soil properties based on spatial sequences of 2D maps. In this general context, the objective of the research described here was to develop a method to generate 3D maps of soil chemical properties at the microscale by combining 2D SEM-EDX data with 3D X-ray computed tomography images. A statistical approach using the regression tree method and ordinary kriging applied to the residuals was developed and applied to predict the 3D spatial distribution of carbon, silicon, iron, and oxygen at the microscale. The spatial correlation between the X-ray grayscale intensities and the chemical maps made it possible to use a regression-tree model as an initial step to predict the 3D chemical composition. For chemical elements, e.g., iron, that are sparsely distributed in a soil sample, the regression-tree model provides a good prediction, explaining as much as 90% of the variability in some of the data. However, for chemical elements that are more homogenously distributed, such as carbon, silicon, or oxygen, the additional kriging of the regression tree residuals improved significantly the prediction with an increase in the R2 value from 0.221 to 0.324 for carbon, 0.312 to 0.423 for silicon, and 0.218 to 0.374 for oxygen, respectively. The present research develops for the first time an integrated experimental and theoretical framework, which combines geostatistical methods with imaging techniques to unveil the 3-D chemical structure of soil at very fine scales. The methodology presented in this study can be easily adapted and applied to other types of data such as bacterial or fungal population densities for the 3D characterization of microbial distribution.', 'kwd': '-', 'title': u'Three-Dimensional Mapping of Soil Chemical Characteristics at Micrometric Scale by Combining 2D SEM-EDX Data and 3D X-Ray CT Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5544716/', 'p': u'Creep cavitation in an ex-service nuclear steam header Type 316 stainless steel sample is investigated through a multiscale tomography workflow spanning eight orders of magnitude, combining X-ray computed tomography (CT), plasma focused ion beam (FIB) scanning electron microscope (SEM) imaging and scanning transmission electron microscope (STEM) tomography. Guided by microscale X-ray CT, nanoscale X-ray CT is used to investigate the size and morphology of cavities at a triple point of grain boundaries. In order to understand the factors affecting the extent of cavitation, the orientation and crystallographic misorientation of each boundary is characterised using electron backscatter diffraction (EBSD). Additionally, in order to better understand boundary phase growth, the chemistry of a single boundary and its associated secondary phase precipitates is probed through STEM energy dispersive X-ray (EDX) tomography. The difference in cavitation of the three grain boundaries investigated suggests that the orientation of grain boundaries with respect to the direction of principal stress is important in the promotion of cavity formation.A FEI Helios Xe plasma-FIB was used to extract a pillar of approximately 25\u2009\xb5m diameter, at the location of the cavitated boundary, for subsequent nanoscale X-ray CT. Initially, the location of the cavitated grain boundary was confirmed through milling of a cross-section and ion-beam imaging. Subsequently, annular milling was used to prepare a pillar, using a milling current of 1.3\u2009\xb5A and two subsequent polishing steps of 180\u2009nA and 59\u2009nA. The pillar was attached to a liftout probe, undercut with a 59\u2009nA beam and was then subsequently lifted out. The pillar was then attached to the tip of a copper pin using Pt deposition, before the liftout probe was detached. The ion-beam remained at an accelerating voltage of 30\u2009kV throughout. The micropillar was produced so that its long-axis is perpendicular to the direction of the macroscale crack plane, meaning the long-axis of the pillar is parallel to the principal stress direction. The plasma-FIB provides a rapid method of machining volumes at the scales of tens or hundreds of micrometres, providing milling rates up to fifty times greater than conventional Ga+ ion beam milling32, 33, which was vital to creating a micropillar of this scale at a site specific location in a reasonable time. The micropillar was attached to a pin sample holder for further analysis after lift-out.', 'kwd': '-', 'title': u'Multiscale correlative tomography: an investigation of creep cavitation in 316 stainless steel'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706545/', 'p': u'To develop a core/shell nanodimer of gold (core) and silver iodine (shell) as a dual-modal contrast-enhancing agent for biomarker targeted x-ray computed tomography (CT) and photoacoustic imaging (PAI) applications.The gold and silver iodine core/shell nanodimer (Au/AgICSD) was prepared by fusing together components of gold, silver, and iodine. The physicochemical properties of Au/AgICSD were then characterized using different optical and imaging techniques (e.g., HR- transmission electron microscope, scanning transmission electron microscope, x-ray photoelectron spectroscopy, energy-dispersive x-ray spectroscopy, Z-potential, and UV-vis). The CT and PAI contrast-enhancing effects were tested and then compared with a clinically used CT contrast agent and Au nanoparticles. To confer biocompatibility and the capability for efficient biomarker targeting, the surface of the Au/AgICSD nanodimer was modified with the amphiphilic diblock polymer and then functionalized with transferrin for targeting transferrin receptor that is overexpressed in various cancer cells. Cytotoxicity of the prepared Au/AgICSD nanodimer was also tested with both normal and cancer cell lines.', 'kwd': u'contrast agent, core\u2013shell, computer tomography, gold, silver iodine, nanoparticles, photoacoustic imaging', 'title': u'A nanocomposite of Au-AgI core/shell dimer as a dual-modality contrast agent for x-ray computed tomography and photoacoustic imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5449646/', 'p': u"Laboratory x-ray micro\u2013computed tomography (micro-CT) is a fast-growing method in scientific research applications that allows for non-destructive imaging of morphological structures. This paper provides an easily operated \u201chow to\u201d guide for new potential users and describes the various steps required for successful planning of research projects that involve micro-CT. Background information on micro-CT is provided, followed by relevant setup, scanning, reconstructing, and visualization methods and considerations. Throughout the guide, a Jackson's chameleon specimen, which was scanned at different settings, is used as an interactive example. The ultimate aim of this paper is make new users familiar with the concepts and applications of micro-CT in an attempt to promote its use in future scientific studies.The voxel size of a micro-CT image is dependent on the magnification and object size as described above. This is related to the distance of the sample from the x-ray source and the detector [4]. Voxel size and spatial resolution are two concepts that are often confused since the voxel size is the size of a pixel in 3D space, i.e., the width of one volumetric pixel (isotropic in three dimensions). This value does not consider the actual spatial resolution capability of the scan system. For example, if the x-ray spot size (focused x-ray spot from the source) becomes larger than the chosen voxel size, the spatial resolution of the system becomes poorer. That means that fewer details are detectable, despite a good voxel size, due to the actual resolution being non-optimal. Since most commercial systems limit the size of the x-ray spot to the required voxel size (or provide the user an indication of this), the actual and voxel resolution are usually the same, but this is not regularly tested or reported. It is possible to use resolution standards (such as calibrated-thickness metal wires) to confirm spatial resolution, and some reference standards exist, although a generally accepted standard for industrial CT systems does not yet exist. It is therefore possible that the amount of detail that is detectable in a scan can vary considerably from system to system, or even between different scans from the same type of system. These quality differences are either due to improper settings that may result in large x-ray spot sizes or to an improper choice of other scan parameters. The sole way of testing the scan quality is to image a small feature of known dimensions and ensure the feature is visible in the CT slice image.", 'kwd': u'3D imaging, micro-computed tomography, nano-computed tomography, non-destructive analysis, x-ray tomography', 'title': u'Laboratory x-ray micro-computed tomography: a user guideline for biological samples'}], 'Coronary Artery Disease AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111838/', 'p': u'The authors are developing a computer-aided detection system to assist radiologists in analysis of coronary artery disease in coronary CT angiograms (cCTA). This study evaluated the accuracy of the authors\u2019 coronary artery segmentation and tracking method which are the essential steps to define the search space for the detection of atherosclerotic plaques.The heart region in cCTA is segmented and the vascular structures are enhanced using the authors\u2019 multiscale coronary artery response (MSCAR) method that performed 3D multiscale filtering and analysis of the eigenvalues of Hessian matrices. Starting from seed points at the origins of the left and right coronary arteries, a 3D rolling balloon region growing (RBG) method that adapts to the local vessel size segmented and tracked each of the coronary arteries and identifies the branches along the tracked vessels. The branches are queued and subsequently tracked until the queue is exhausted. With Institutional Review Board approval, 62 cCTA were collected retrospectively from the authors\u2019 patient files. Three experienced cardiothoracic radiologists manually tracked and marked center points of the coronary arteries as reference standard following the 17-segment model that includes clinically significant coronary arteries. Two radiologists visually examined the computer-segmented vessels and marked the mistakenly tracked veins and noisy structures as false positives (FPs). For the 62 cases, the radiologists marked a total of 10191 center points on 865 visible coronary artery segments.', 'kwd': u'coronary arteries, vessel segmentation, computer-aided detection, coronary artery diseases, atherosclerotic plaque, multiscale filtering', 'title': u'Computerized analysis of coronary artery disease: Performance evaluation of segmentation and tracking of coronary arteries in CT angiograms'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5456060/', 'p': u'Optical coherence tomography (OCT) is an established catheter-based imaging modality for the assessment of coronary artery disease and the guidance of stent placement during percutaneous coronary intervention. Manual analysis of large OCT datasets for vessel contours or stent struts detection is time-consuming and unsuitable for real-time applications. In this study, a fully automatic method was developed for detection of both vessel contours and stent struts. The method was applied to in vitro OCT scans of eight stented silicone bifurcation phantoms for validation purposes. The proposed algorithm comprised four main steps, namely pre-processing, lumen border detection, stent strut detection, and three-dimensional point cloud creation. The algorithm was validated against manual segmentation performed by two independent image readers. Linear regression showed good agreement between automatic and manual segmentations in terms of lumen area (r>0.99). No statistically significant differences in the number of detected struts were found between the segmentations. Mean values of similarity indexes were >95% and >85% for the lumen and stent detection, respectively. Stent point clouds of two selected cases, obtained after OCT image processing, were compared to the centerline points of the corresponding stent reconstructions from micro computed tomography, used as ground-truth. Quantitative comparison between the corresponding stent points resulted in median values of ~150 \u03bcm and ~40 \u03bcm for the total and radial distances of both cases, respectively. The repeatability of the detection method was investigated by calculating the lumen volume and the mean number of detected struts per frame for seven repeated OCT scans of one selected case. Results showed low deviation of values from the median for both analyzed quantities. In conclusion, this study presents a robust automatic method for detection of lumen contours and stent struts from OCT as supported by focused validation against both manual segmentation and micro computed tomography and by good repeatability.To test the repeatability of the lumen border detection algorithm, the lumen volume of each case was calculated as the sum of the lumen area per frame multiplied by the distance between the slices. The extremes of the stent were used as landmarks to establish the same region of interest between acquisitions. Regarding the strut detection algorithm, the mean of the number of detected struts per frame was computed for each case.', 'kwd': '-', 'title': u'Reconstruction of stented coronary arteries from optical coherence tomography images: Feasibility, validation, and repeatability of a segmentation method'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4922813/', 'p': u'After a decade of clinical use of coronary computed tomographic angiography (CCTA) to evaluate the anatomic severity of coronary artery disease, new methods of deriving functional information from CCTA have been developed. These methods utilize the anatomic information provided by CCTA in conjunction with computational fluid dynamics to calculate fractional flow reserve (FFR) values from CCTA image data sets. Computed tomography-derived FFR (CT-FFR) enables the identification of lesion-specific drop noninvasively. A three-dimensional CT-FFR modeling technique, which provides FFR values throughout the coronary tree (HeartFlow FFRCT analysis), has been validated against measured FFR and is now approved by the US Food and Drug Administration for clinical use. This technique requires off-site supercomputer analysis. More recently, a one-dimensional computational analysis technique (Siemens cFFR), which can be performed on on-site workstations, has been developed and is currently under investigation. This article reviews CT-FFR technology and clinical evidence for its use in stable patients with suspected coronary artery disease.Intermediate degrees of stenosis (30%\u201370%) present the greatest challenge in the diagnosis of CAD. Since hemodynamically significant lesions are occasionally observed in intermediate lesions with <70% stenosis,13 the use of invasive FFR is recommended to evaluate the function of intermediate coronary lesions as a class IIa indication.6 However, given the relatively lower prevalence of lesion-specific pressure drop caused by intermediate stenosis compared to that of severe stenosis in the FAME study,13 CT-derived FFR would be of great use for assessing the functional significance of intermediate lesions to avoid unnecessary ICA and help in treatment decision making. Table 2 provides a summary of the studies of FFRCT and cFFR. Similar to the overall diagnostic accuracy of CT-derived FFR, all studies demonstrated high diagnostic performance for intermediate stenosis, with the highest accuracy and specificity for FFRCT.24,31,38,39', 'kwd': u'fractional flow reserve, coronary computed tomographic angiography, FFRCT, cFFR', 'title': u'Noninvasive FFR derived from coronary CT angiography in the management of coronary artery disease: technology and clinical update'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5018003/', 'p': u'The authors are developing an automated method to identify the best-quality coronary arterial segment from multiple-phase coronary CT angiography (cCTA) acquisitions, which may be used by either interpreting physicians or computer-aided detection systems to optimally and efficiently utilize the diagnostic information available in multiple-phase cCTA for the detection of coronary artery disease.After initialization with a manually identified seed point, each coronary artery tree is automatically extracted from multiple cCTA phases using our multiscale coronary artery response enhancement and 3D rolling balloon region growing vessel segmentation and tracking method. The coronary artery trees from multiple phases are then aligned by a global registration using an affine transformation with quadratic terms and nonlinear simplex optimization, followed by a local registration using a cubic B-spline method with fast localized optimization. The corresponding coronary arteries among the available phases are identified using a recursive coronary segment matching method. Each of the identified vessel segments is transformed by the curved planar reformation (CPR) method. Four features are extracted from each corresponding segment as quality indicators in the original computed tomography volume and the straightened CPR volume, and each quality indicator is used as a voting classifier for the arterial segment. A weighted voting ensemble (WVE) classifier is designed to combine the votes of the four voting classifiers for each corresponding segment. The segment with the highest WVE vote is then selected as the best-quality segment. In this study, the training and test sets consisted of 6 and 20 cCTA cases, respectively, each with 6 phases, containing a total of 156 cCTA volumes and 312 coronary artery trees. An observer preference study was also conducted with one expert cardiothoracic radiologist and four nonradiologist readers to visually rank vessel segment quality. The performance of our automated method was evaluated by comparing the automatically identified best-quality segments identified by the computer to those selected by the observers.', 'kwd': u'coronary CT angiography, image analysis, computer-aided reading', 'title': u'Coronary artery analysis: Computer-assisted selection of best-quality segments in multiple-phase coronary CT angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4499869/', 'p': u'The initial course of atherosclerotic disease is thought to begin in early adulthood. In young adults lesions in the arterial vessel wall have been observed surprisingly frequently13 but the prognostic relevance of early, adaptive or reversible changes like \u201cfatty streak\u201d or intimal thickening remains a matter of debate. Pathology studies seek to integrate autopsy findings from various stages of atherosclerosis to provide a putative sequence of events4. In brief, intimal thickening is observed early in the disease process. The early atherosclerotic lesion is composed of smooth muscle cells and is affected by increased macrophage and lipid influx. If this process continues, a necrotic core is formed and the lesion progresses to a fibrous cap atheroma. The necrotic core contains lipids and apoptotic macrophages. A stable fibrous cap may prevent rupture of the lesion. If the fibrous cap loses matrix proteins and smooth muscle cells, a thin cap atheroma can result. Intraplaque hemorrhage is also seen frequently in this entity, leading to further enlargement of the lipid core. The risk of plaque rupture is increased as the fibrous cap thins and the lipid core enlarges14. The \u201cfibrocalcific plaque\u201d is considered to be a feature of more stable plaque, although the processes involved in calcification are not fully understood.It is generally conceived that therapeutic intervention for atherosclerosisis most effective when started at an early stage of the progressive disease process 15. Imaging tools have provided a substantial database of knowledge regarding disease burden. Imaging of the larger surface vessels (carotid or femoral arteries) has been extensively used to detect early systemic vascular pathology 16. Calcium detection using non-contrast CT provides a direct approach to assessing coronary atherosclerosis burden. The coronary artery calcium score has strong predictive power for cardiovascular events in asymptomatic subjects17. However, calcium deposition is felt to be a late event in the formation of atherosclerotic plaque. The relevance of non-calcified plaque is emphasized by prospective IVUS studies that show coronary fibroatheroma without significant calcification confers an elevated risk for myocardial infarction 18. Non-calcified plaque is more common than calcified plaque in asymptomatic individuals younger than 45 years. The ability to noninvasively image non-calcified plaque or wall thickening of the coronary arteries using MRI or CT enables the detection of earlier stages of atherosclerotic disease 19, 20.', 'kwd': u'imaging, coronary disease, plaque, atherosclerosis', 'title': u'Noninvasive Imaging of Atherosclerotic Plaque Progression: Status of Coronary CT Angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3750415/', 'p': u'Author contributions: Guarantors of integrity of entire study, Y.H., J.L.N., G.S.K.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; manuscript final version approval, all authors; literature research, Y.H., T.W., D.L.B., G.S.K.; clinical studies, S.S., J.L.N.; statistical analysis, Y.H., T.W., G.S.K.; and manuscript editing, Y.H., T.W., J.S.C., S.S., S.D.T., D.L.B., G.S.K.To provide proof of concept for a diagnostic method to assess diffuse coronary artery disease (CAD) on the basis of coronary computed tomography (CT) angiography.', 'kwd': '-', 'title': u'CT-based Diagnosis of Diffuse Coronary Artery Disease on the Basis of Scaling Power Laws'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4355954/', 'p': u'Epicardial fat may play a role in the pathogenesis of coronary artery disease (CAD). We explored the relationship of epicardial fat volume (EFV) with the presence and severity of CAD or myocardial perfusion abnormalities in a diverse, symptomatic patient population.In a diverse population of symptomatic patients referred for invasive coronary angiography, we did not find associations of epicardial fat volume with the presence and severity of coronary artery disease or with myocardial perfusion abnormalities. The clinical significance of quantifying epicardial fat volume remains uncertain but may relate to the pathophysiology of acute coronary events rather than the presence of atherosclerotic disease.', 'kwd': u'epicardial fat, pericardial fat, coronary artery disease, coronary artery calcification, coronary artery stenosis, myocardial ischemia', 'title': u'Lack of Association Between Epicardial Fat Volume and Extent of Coronary Artery Calcification, Severity of Coronary Artery Disease, or Presence of Myocardial Perfusion Abnormalities in a Diverse, Symptomatic Patient Population: Results from the CORE320 Multicenter Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2947500/', 'p': u'Conceived and designed the experiments: WJM JW JMAvE RMB TL. Performed the experiments: SCG. Analyzed the data: SCG MEK AGK SS TL. Contributed reagents/materials/analysis tools: RJvdG. Wrote the paper: SCG MEK AGK SS MK RJvdG WJM JW JMAvE RMB TL. Data interpretation and supervision: MEK JvE TL. Statistical analysis: AK. Assistance with data acquisition in early phase of study: MK. Developed MR method: RB. Handled funding: TL.Magnetic resonance imaging (MRI) is sensitive to early atherosclerotic changes such as positive remodeling in patients with coronary artery disease (CAD). We assessed prevalence, quality, and extent of coronary atherosclerosis in a group of healthy subjects compared to patients with confirmed CAD.', 'kwd': '-', 'title': u'Visualization of Coronary Wall Atherosclerosis in Asymptomatic Subjects and Patients with Coronary Artery Disease Using Magnetic Resonance Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3840080/', 'p': u'A collaborative framework was initiated to establish a community resource of ground truth segmentations from cardiac MRI. Multi-site, multi-vendor cardiac MRI datasets comprising 95 patients (73 men, 22 women; mean age 62.73 \xb1 11.24 years) with coronary artery disease and prior myocardial infarction, were randomly selected from data made available by the Cardiac Atlas Project (Fonseca et al., 2011). Three semi- and two fully-automated raters segmented the left ventricular myocardium from short-axis cardiac MR images as part of a challenge introduced at the STACOM 2011 MICCAI workshop (Suinesiaputra et al., 2012). Consensus myocardium images were generated based on the Expectation-Maximization principle implemented by the STAPLE algorithm (Warfield et al., 2004). The mean sensitivity, specificity, positive predictive and negative predictive values ranged between 0.63-0.85, 0.60-0.98, 0.56-0.94 and 0.83-0.92, respectively, against the STAPLE consensus. Spatial and temporal agreement varied in different amounts for each rater. STAPLE produced high quality consensus images if the region of interest was limited to the area of discrepancy between raters. To maintain the quality of the consensus, an objective measure based on the candidate automated rater performance distribution is proposed. The consensus segmentation based on a combination of manual and automated raters were more consistent than any particular rater, even those with manual input. The consensus is expected to improve with the addition of new automated contributions. This resource is open for future contributions, and is available as a test bed for the evaluation of new segmentation algorithms, through the Cardiac Atlas Project (www.cardiacatlas.org).The Guide-Point Modeling technique (Li et al., 2010) was used to assist the fitting of a finite element cardiac model to the CMR data. This approach involves human observer input to refine the segmentation results by positioning a small number of guide points interactively on a sparse subset of slices and frames. Both long axis and short axis images were included in the analysis. The model incorporated the basal margin of the left ventricle as a plane, which was least squares fit to points placed by the user on the hinge points of the mitral valve in the long axis images. The model surfaces were influenced by the placement of user-defined guide points, and the automatic generation of edge points as well as the automated tracking of contours through all frames using non-rigid registration. The model was spatially and temporally consistent to reduce the amount of user interaction. However, inconsistency in breath-hold position can lead to mismatches between the short and long axis images. Images were manually shifted in-plane to compensate for breath-hold mis-registration, but individual slices may show errors in segmentation due to inconsistency with surrounding images in space and time. This expert-guided method has been previously validated in animals against autopsy LV mass, in patients with regional wall motion abnormalities against manually drawn contours, and in healthy volunteers against flow-derived measurements of cardiac output (Young et al., 2000). This method required expert approval of all slices and for all frames.', 'kwd': '-', 'title': u'A Collaborative Resource to Build Consensus for Automated Left Ventricular Segmentation of Cardiac MR Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3175011/', 'p': u'To evaluate our prototype method for segmentation and tracking of the coronary arterial tree, which is the foundation for a computer-aided detection (CADe) system to be developed to assist radiologists in detecting non-calcified plaques in coronary CT angiography (cCTA) scans.The heart region was first extracted by a morphological operation and an adaptive thresholding method based on expectation-maximization (EM) estimation. The vascular structures within the heart region were enhanced and segmented using a multiscale coronary response (MSCAR) method that combined 3D multiscale filtering, analysis of the eigen values of Hessian matrices and EM estimation segmentation. After the segmentation of vascular structures, the coronary arteries were tracked by a 3D dynamic balloon tracking (DBT) method. The DBT method started at two manually identified seed points located at the origins of the left and right coronary arteries (LCA and RCA) for extraction of the arterial trees. The coronary arterial trees of a data set containing 20 ECG-gated contrast-enhanced cCTA scans were extracted by our MSCAR-DBT method and a clinical GE Advantage workstation. Two experienced thoracic radiologists visually examined the coronary arteries on the original cCTA scans and the rendered volume of segmented vessels to count the untracked false-negative (FN) segments and false positives (FPs) for both methods.', 'kwd': u'Computer-aided detection, coronary artery tracking, vessel segmentation', 'title': u'Automated coronary artery tree extraction in coronary CT angiography using a multiscale enhancement and dynamic balloon tracking (MSCAR-DBT) method'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3841469/', 'p': u'Coronary artery disease (CAD) is one of the leading causes of death in the US and a substantial health-care burden in all industrialized societies. In recent years we have witnessed a constant strive towards the development and the clinical application of novel or improved detection methods as well as therapies. Particularly, noninvasive imaging is a decisive component in the cardiovascular field. Image fusion is the ability of combining into a single integrated display the anatomical as well as the physiological data retrieved by separated modalities. Clinical evidence suggests that it represents a promising strategy in CAD assessment and risk stratification by significantly improving the diagnostic power of each modality independently considered and of the traditional side-by-side interpretation. Numerous techniques and approaches taken from the image registration field have been implemented and validated in the context of CAD assessment and management. Although its diagnostic power is widely accepted, additional technical developments are still needed to become a routinely used clinical tool.Clinical evidence suggests that image fusion can be a reliable and useful tool in the hands of clinicians for a more accurate diagnosis of CAD, specifically for ambiguous and borderline cases. Additional technical developments in the extraction of anatomical information are still in order for the whole procedure to become fully automated and enter the clinical practice.', 'kwd': u'image fusion, CAD diagnosis, computed tomography angiography, nuclear imaging', 'title': u'Multimodality image fusion for diagnosing coronary artery disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3906085/', 'p': u'Conceived and designed the experiments: TL GSK. Performed the experiments: TL. Analyzed the data: TL TW YH. Contributed reagents/materials/analysis tools: TL TW YH. Wrote the paper: TL GSK. Data Collection: BKK.Accurate computed tomography (CT)-based reconstruction of coronary morphometry (diameters, length, bifurcation angles) is important for construction of patient-specific models to aid diagnosis and therapy. The objective of this study is to validate the accuracy of patient coronary artery lumen area obtained from CT images based on intravascular ultrasound (IVUS).', 'kwd': '-', 'title': u'IVUS Validation of Patient Coronary Artery Lumen Area Obtained from CT Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4263628/', 'p': u'Author contributions: Guarantors of integrity of entire study, J.A.C.L., J.B.M., D.A.B.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; manuscript final version approval, all authors; literature research, A.C.K., G.C., C.T.S., J.A.C.L., D.L.L., D.A.B.; clinical studies, C.T.S., D.L.L., J.B.M., D.A.B.; statistical analysis, H.T.M., G.C., C.T.S., D.L.L.; and manuscript editing, A.C.K., H.T.M., G.C., C.T.S., B.D.R., J.A.C.L., D.L.L., J.B.M., D.A.B.The extent of coronary plaque in asymptomatic diabetic patients is related to body mass index and duration of diabetes.', 'kwd': '-', 'title': u'Coronary Artery Plaque Volume and Obesity in Patients with Diabetes: The Factor-64 Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4213417/', 'p': u'Computed tomography (CT) angiography represents the most important technical development in CT imaging and it has challenged invasive angiography in the diagnostic evaluation of cardiovascular abnormalities. Over the last decades, technological evolution in CT imaging has enabled CT angiography to become a first-line imaging modality in the diagnosis of cardiovascular disease. This review provides an overview of the diagnostic applications of CT angiography (CTA) in cardiovascular disease, with a focus on selected clinical challenges in some common cardiovascular abnormalities, which include abdominal aortic aneurysm (AAA), aortic dissection, pulmonary embolism (PE) and coronary artery disease. An evidence-based review is conducted to demonstrate how CT angiography has changed our approach in the diagnosis and management of cardiovascular disease. Radiation dose reduction strategies are also discussed to show how CT angiography can be performed in a low-dose protocol in the current clinical practice.CT angiography represents the most important development in CT imaging, and it has evolved from the initial role of serving as a supplementary modality to an essential tool that plays an important role in the diagnosis and management of cardiovascular disease which involves arterial system in the body. Technological advancements in CT data acquisition and image processing techniques have enabled this technique to become a routine imaging modality in daily clinical practice. With emergence of novel CT scanner geometries, advanced data reconstruction and postprocessing techniques, CT angiography will continue to play a dominant role in the diagnosis of cardiovascular disease, prediction of disease extent and assistance of clinicians in effective patient management.', 'kwd': u'Cardiovascular disease, computed tomography angiography (CTA), diagnosis, visualisation', 'title': u'CT angiography in the diagnosis of cardiovascular disease: a transformation in cardiovascular CT practice'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5594598/', 'p': u'The use of coronary MR angiography (CMRA) in patients with coronary artery disease (CAD) remains limited due to the long scan times, unpredictable and often non-diagnostic image quality secondary to respiratory motion artifacts. The purpose of this study was to evaluate CMRA with image-based respiratory navigation (iNAV CMRA) and compare it to gold standard invasive x-ray coronary angiography in patients with CAD.Consecutive patients referred for CMR assessment were included to undergo iNAV CMRA on a 1.5\xa0T scanner. Coronary vessel sharpness and a visual score were assigned to the coronary arteries. A diagnostic reading was performed on the iNAV CMRA data, where a lumen narrowing >50% was considered diseased. This was compared to invasive x-ray findings.', 'kwd': u'Coronary MR angiography, Image navigators, Respiratory motion correction, Coronary artery disease', 'title': u'Diagnostic performance of image navigated coronary CMR angiography in patients with coronary artery disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422837/', 'p': u'During the last decade coronary computed tomography angiography (CTA) has become the preeminent non-invasive imaging modality to detect coronary artery disease (CAD) with high accuracy. However, CTA has a limited value in assessing the hemodynamic significance of a given stenosis due to a modest specificity and positive predictive value. In recent years, different CT techniques for detecting myocardial ischemia have emerged, such as CT-derived fractional flow reserve (FFR-CT), transluminal attenuation gradient (TAG), and myocardial CT perfusion (CTP) imaging. Myocardial CTP imaging can be performed with a single static scan during first pass of the contrast agent, with monoenergetic or dual-energy acquisition, or as a dynamic, time-resolved scan during stress by using coronary vasodilator agents (adenosine, dipyridamole, or regadenoson). A number of CTP techniques are available, which can assess myocardial perfusion in both a qualitative, semi-quantitative or quantitative manner. Once used primarily as research tools, these modalities are increasingly being used in routine clinical practice. All these techniques offer the substantial advantage of combining anatomical and functional evaluation of flow-limiting coronary stenosis in the same examination that would be beneficial for clinical decision-making. This review focuses on the state-of the-art and future trends of these evolving imaging modalities in the field of cardiology for the physiologic assessments of CAD.The FFR-CT method allows the extraction of \u2018stress induced\u2019 quantitative functional information from an anatomic CTA of at least moderate quality acquired at rest without adenosine infusion. The method uses computational fluid dynamics with simulated hyperemia to calculate the FFR measurement at any point in the vascular tree (39). The concept of coronary FFR, defined as the ratio of the mean coronary pressure distal to a coronary stenosis to the mean aortic pressure during maximal coronary blood flow, has evolved into an accepted functional measure of stenosis severity since first proposed 15 years ago (22). FFR has now become the invasive gold standard for assessing lesion-specific ischemia. A FFR value less than 0.80 or less than 0.75 identifies hemodynamic significance of coronary stenosis (7,8). FFR-CT correlates well with invasive-derived FFR measurements in patients with suspected or known CAD (17,39,40). One of the temporary drawbacks is that FFR-CT needs extreme computational ability and analysis time, which hampers widespread dissemination.', 'kwd': u'Cardiac computed tomography, coronary artery disease (CAD), stress imaging, myocardial perfusion imaging, myocardial blood flow quantification', 'title': u'Myocardial blood flow quantification for evaluation of coronary artery disease by computed tomography'}], 'Risk Stratification AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1479368/', 'p': u'In recent years a number of algorithms for cardiovascular risk assessment has been proposed to the medical community. These algorithms consider a number of variables and express their results as the percentage risk of developing a major fatal or non-fatal cardiovascular event in the following 10 to 20 yearsThe use of predictive algorithms to assess individual absolute risk of cardiovascular future events is currently hampered by methodological and mathematical flaws. The use of newer approaches, such as fuzzy logic and artificial neural networks, linked to artificial intelligence, seems to better address both the challenge of increasing complexity resulting from a correlation between predisposing factors, data on the occurrence of cardiovascular events, and the prediction of future events on an individual level.', 'kwd': '-', 'title': u'How artificial intelligence tools can be used to assess individual patient risk in cardiovascular disease: problems with the current methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3818807/', 'p': u'Recently, the greatest statistical computational challenge in genetic epidemiology is to identify and characterize the genes that interact with other genes and environment factors that bring the effect on complex multifactorial disease. These gene-gene interactions are also denoted as epitasis in which this phenomenon cannot be solved by traditional statistical method due to the high dimensionality of the data and the occurrence of multiple polymorphism. Hence, there are several machine learning methods to solve such problems by identifying such susceptibility gene which are neural networks (NNs), support vector machine (SVM), and random forests (RFs) in such common and multifactorial disease. This paper gives an overview on machine learning methods, describing the methodology of each machine learning methods and its application in detecting gene-gene and gene-environment interactions. Lastly, this paper discussed each machine learning method and presents the strengths and weaknesses of each machine learning method in detecting gene-gene interactions in complex human disease.All trees of RF are frown to their full extent without pruning because each tree of a RF is grown using random feature selection to select a training set (bootstrap sample) from the original data [31]. Based on [32], a classifiers decision tree of RF is grown as follows.', 'kwd': '-', 'title': u'A Review for Detecting Gene-Gene Interactions Using Machine Learning Methods in Genetic Epidemiology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5391398/', 'p': u'In 2016, 13 topics were selected as major research advances in gynecologic oncology. For ovarian cancer, study results supporting previous ones regarding surgical preventive strategies were reported. There were several targeted agents that showed comparable responses in phase III trials, including niraparib, cediranib, and nintedanib. On the contrary to our expectations, dose-dense weekly chemotherapy regimen failed to prove superior survival outcomes compared with conventional triweekly regimen. Single-agent non-platinum treatment to prolong platinum-free-interval in patients with recurrent, partially platinum-sensitive ovarian cancer did not improve and even worsened overall survival (OS). For cervical cancer, we reviewed robust evidences of larger-scaled population-based study and cost-effectiveness of nonavalent vaccine for expanding human papillomavirus (HPV) vaccine coverage. Standard of care treatment of locally advanced cervical cancer (LACC) was briefly reviewed. For uterine corpus cancer, new findings about appropriate surgical wait time from diagnosis to surgery were reported. Advantages of minimally invasive surgery over conventional laparotomy were reconfirmed. There were 5 new gene regions that increase the risk of developing endometrial cancer. Regarding radiation therapy, Post-Operative Radiation Therapy in Endometrial Cancer (PORTEC)-3 quality of life (QOL) data were released and higher local control rate of image-guided adaptive brachytherapy was reported in LACC. In addition, 4 general oncology topics followed: chemotherapy at the end-of-life, immunotherapy with reengineering T-cells, actualization of precision medicine, and artificial intelligence (AI) to make personalized cancer therapy real. For breast cancer, adaptively randomized trials, extending aromatase inhibitor therapy, and ribociclib and palbociclib were introduced.Five new gene regions that increase the risk of developing endometrial cancer were identified by a meta-analysis of 3 endometrial cancer genome-wide association study (GWAS) [50]: previous GWAS from 2 population studies (the UK Studies of Epidemiology and Risk factors in Cancer Heredity [SEARCH, n=681] and the Australian National Endometrial Cancer Study [ANECS, n=606]) and genotypes generated using Illumina Infinium 610K arrays, the National Study of Endometrial Cancer (NSECG), and the Collaborative Oncological Gene-environment Study (COGS) initiative. In this study, a total of 7,737 endometrial cancer cases and 37,144 controls without cancer of European ancestry were investigated. Five novel risk loci included likely regulatory regions on chromosomes 13q22.1, 6q22.31, 8q24.21, 15q15.1, and 14q32.33. Those 5 novel regions contained at least one endometrial cancer risk single nucleotide polymorphism (SNP) with Pmeta<10\u22127 and most strongly associated SNP in each region was genotyped: rs11841589 (OR=1.15; 95% CI=1.11\u20131.21; p=4.83\xd710\u221211), rs13328298 (OR=1.13; 95% CI=1.09\u20131.18; p=3.73\xd710\u221210), rs4733613 (OR=0.84; 95% CI=0.80\u20130.89; p=3.09\xd710\u22129), rs937213 (OR=0.90; 95% CI=0.86\u20130.93; p=1.77\xd710\u22128), and rs2498796 (OR=0.89; 95% CI=0.85\u20130.93; p=3.55\xd710\u22128), respectively. All the 5 SNPs were associated with endometrial cancer at genome-wide significance (p<5\xd710\u22128). Specifically, functional studies of the 13q22.1 locus showed that rs9600103 is located in a region of active chromatin that interacts with promoter region of the Kruppel-like factor 5 (KLF5) (pairwise r2=0.98 with rs11841589). KLF5, a transcription factor associated with cell cycle regulation, is thought to be active during the development of the uterus as well as tumorigenesis. Given in vitro suppression of gene expression by rs9600103-T endometrial cancer protective allele in allele-specific luciferase reporter assays using Ishikawa cells, regulation of KLF5 expression could be implicated in tumorigenesis of endometrial cancer.', 'kwd': u'Precision Medicine, Artificial Intelligence, Genital Neoplasms, Female, Ovarian Neoplasms, Breast Neoplasms', 'title': u'Major clinical research advances in gynecologic cancer in 2016: 10-year special edition'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5653644/', 'p': '-', 'kwd': '-', 'title': u'Metabolomics for the masses: The future of metabolomics in a personalized world'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4153716/', 'p': u'Conceived and designed the experiments: MD M. Rammerstorfer TP HB M. Ramharter. Performed the experiments: TP AB AM FL. Analyzed the data: FR GD MD HB M. Ramharter. Contributed reagents/materials/analysis tools: MD M. Rammerstorfer GD AB AM TP. Contributed to the writing of the manuscript: FR M. Ramharter FL GD TP HB.Bacteraemia is a frequent and severe condition with a high mortality rate. Despite profound knowledge about the pre-test probability of bacteraemia, blood culture analysis often results in low rates of pathogen detection and therefore increasing diagnostic costs. To improve the cost-effectiveness of blood culture sampling, we computed a risk prediction model based on highly standardizable variables, with the ultimate goal to identify via an automated decision support tool patients with very low risk for bacteraemia.', 'kwd': '-', 'title': u'A Risk Prediction Model for Screening Bacteremic Patients: A Cross Sectional Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4738045/', 'p': u'The aim of this study is to present an objective method based on support vector machines (SVMs) and gravitational search algorithm (GSA) which is initially utilized for recognition the pattern among risk factors and hypertension (HTN) to stratify and analysis HTN\u2019s risk factors in an Iranian urban population.This community-based and cross-sectional research has been designed based on the probabilistic sample of residents of Isfahan, Iran, aged 19 years or over from 2001 to 2007. One of the household members was randomly selected from different age groups. Selected individuals were invited to a predefined health center to be educated on how to collect 24-hour urine sample as well as learning about topographic parameters and blood pressure measurement. The data from both the estimated and measured blood pressure [for both systolic blood pressure (SBP) and diastolic blood pressure (DBP)] demonstrated that optimized SVMs have a highest estimation potential.', 'kwd': u'Support Vector Machines, Gravitational Search Algorithm, High Blood Pressure', 'title': u'Advanced method used for hypertension\u2019s risk factors strati\ufb01cation: support vector machines and gravitational search algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422742/', 'p': u'The management of thyroid nodules, one of the main clinical challenges in endocrine clinical practice, is usually straightforward. Although the most important concern is ruling out malignancy, there are grey areas where uncertainty is frequently present: the nodules labelled as indeterminate by cytology and the extent of therapy when thyroid cancer is diagnosed pathologically. There is evidence that the current available precision medicine tools (from all the \u201c-omics\u201d to molecular analysis, fine-tuning imaging or artificial intelligence) may help to fill present gaps in the future. We present here a commentary on some of the current challenges faced by endocrinologists in the field of thyroid nodules and cancer, and illustrate how precision medicine may improve their diagnostic and therapeutic capabilities in the future.Several immunocytochemical markers have been proposed to differentiate benign from malignant nodules in fine-needle aspiration samples [6]. Some of them are listed in Table \u200bTable1.1. Currently no immunomarker has demonstrated enough diagnostic accuracy to be used alone. However, the combined analysis of galectin-3 and Hector Battifora mesothelial-1 has shown acceptable sensitivity and specificity to identify malignant tumours [7]. In this regard, other promising markers are being studied, such as CD44 [8] or Ki-67 [9].', 'kwd': u'Precision medicine, Thyroid cancer, Thyroid nodule, Differentiated thyroid cancer, Medullary thyroid cancer', 'title': u'Nodular Thyroid Disease and Thyroid Cancer in the Era of Precision Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4298653/', 'p': u'Mortality prediction models for patients with perforated peptic ulcer (PPU) have not yielded consistent or highly accurate results. Given the complex nature of this disease, which has many non-linear associations with outcomes, we explored artificial neural networks (ANNs) to predict the complex interactions between the risk factors of PPU and death among patients with this condition.ANN modelling using a standard feed-forward, back-propagation neural network with three layers (i.e., an input layer, a hidden layer and an output layer) was used to predict the 30-day mortality of consecutive patients from a population-based cohort undergoing surgery for PPU. A receiver-operating characteristic (ROC) analysis was used to assess model accuracy.', 'kwd': u'Peptic ulcer perforation, Gastroduodenal ulcers, Mortality, Prediction, Prognosis, Outcome assessment, Computer simulation', 'title': u'Predicting outcomes in patients with perforated gastroduodenal ulcers: artificial neural network modelling indicates a highly complex disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520404/', 'p': '-', 'kwd': u'Electronic health records, Temporal analysis, Progression of kidney function loss, Risk stratification', 'title': u'Incorporating temporal EHR data in predictive models for risk stratification of renal function deterioration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3706990/', 'p': u"Tumor heterogeneity is a limiting factor in cancer treatment and in the discovery of biomarkers to personalize it. We describe a computational purification tool, ISOpure, which directly addresses the effects of variable contamination by normal tissue in clinical tumor specimens. ISOpure uses a set of tumor expression profiles and a panel of healthy tissue expression profiles to generate a purified cancer profile for each tumor sample, and an estimate of the proportion of RNA originating from cancerous cells. Applying ISOpure before identifying gene signatures leads to significant improvements in the prediction of prognosis and other clinical variables in lung and prostate cancer.Our regularization strategy incorporates the Dirichlet probability density function into our scoring functions. This choice allows us to use the statistical inference method described below to estimate the parameter values. The Dirichlet distribution is a continuous multivariate distribution over discrete probability distributions (that is, vectors of pre-determined size that contain non-negative elements that sum to one). We use the Dirichlet for both \u03b8n and cn because they are both discrete probability distributions. The probability density function associated with the Dirichlet has two parameters (termed hyper-parameters because they are the parameters of distributions over model parameters): a mean vector (which determines the mean of the Dirichlet distribution) and a scalar strength parameter that controls how quickly the score decreases from the mode of the Dirichlet distribution. We also estimate the following hyper-parameters from the tumor data: \xa0\u03bd, kn (for n = 1 to N), k', and \u03c9. These additional parameters are formally defined below in the statistical model provided in equations (3 to 9), but in brief \xa0\u03bd represents both the mean and strength of a Dirichlet distribution over \u03b8n; kn represents the strength parameter of the Dirichlet distribution over cn given m; k' represents the strength parameter of the Dirichlet distribution over m; \u03c9 represents the weights on the normal profiles br used to make the weighted combination that forms the mean parameter vector for the Dirichlet distribution over m.", 'kwd': '-', 'title': u'Computational purification of individual tumor gene expression profiles leads to significant improvements in prognostic prediction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4747508/', 'p': u'Conceived and designed the experiments: TKY DWK. Analyzed the data: TKY SBC EO. Contributed reagents/materials/analysis tools: EO. Wrote the paper: TKY SBC JSP DWK.Knee osteoarthritis (OA) is the most common joint disease of adults worldwide. Since the treatments for advanced radiographic knee OA are limited, clinicians face a significant challenge of identifying patients who are at high risk of OA in a timely and appropriate way. Therefore, we developed a simple self-assessment scoring system and an improved artificial neural network (ANN) model for knee OA.', 'kwd': '-', 'title': u'Simple Scoring System and Artificial Neural Network for Knee Osteoarthritis Risk Prediction: A Cross-Sectional Study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4309789/', 'p': u'MicroRNAs (miRNAs) are small, noncoding RNA species with a length of 20\u201322 nucleotides that are recognized as essential regulators of relevant molecular mechanisms, including carcinogenesis. Current investigations show that miRNAs are detectable not only in different tissue types but also in a wide range of biological fluids, either free or trapped in circulating microvesicles. miRNAs were proven to be involved in cell communication, both in pathological and physiological processes. Evaluation of the global expression patterns of miRNAs provides key opportunities with important practical applications, taking into account that they modulate essential biological processes such as epithelial to mesenchymal transition, which is a mechanism relevant in bladder cancer. miRNAs collected from biological specimens can furnish valuable evidence with regard to bladder cancer oncogenesis, as they also have been linked to clinical outcomes in urothelial carcinoma. Therefore, a single miRNA or a signature of multiple miRNAs may improve risk stratification of patients and may supplement the histological diagnosis of urological tumors, particularly for bladder cancer.The challenges that clinicians face when caring for patients with bladder cancer are the difficulties of early diagnosis, disease recurrence, and progression. Current prognostic strategies, such as tumor grade, stage, size, and number of foci, have restricted utility for clinicians because they do not specifically exhibit the clinical outcomes.6 Diagnosis and monitoring strategies for bladder cancer have been based on the integration of cystoscopy and urinary cytology data.7 As a consequence, researchers have been searching for novel biomarkers, and an important research direction was focusing on the role of microRNAs (miRNAs) in the development of bladder cancer (Figure 2).', 'kwd': u'bladder cancer, miRNA, prognostic, diagnostic', 'title': u'Clinical and pathological implications of miRNA in bladder cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3046173/', 'p': u'Conceived and designed the experiments: JFR ELE. Performed the experiments: JFR ELE PCE WAR. Analyzed the data: JFR ELE. Contributed reagents/materials/analysis tools: DFR. Wrote the paper: JFR DFR. Performed the feature extraction methods: JRV. Gave important feedback on statistical procedures: ELE. Made the PCA-based feature selection and classification experiments: JRV. Gave important feedback and medical validation of the reported results: WAR. Gave important scientific background: DFR. Suggested suitable classification schemes and architectures to overcome the classification problems: DFR. Gave important feedback for the enhancement of the paper: DFR PCE.Statistical, spectral, multi-resolution and non-linear methods were applied to heart rate variability (HRV) series linked with classification schemes for the prognosis of cardiovascular risk. A total of 90 HRV records were analyzed: 45 from healthy subjects and 45 from cardiovascular risk patients. A total of 52 features from all the analysis methods were evaluated using standard two-sample Kolmogorov-Smirnov test (KS-test). The results of the statistical procedure provided input to multi-layer perceptron (MLP) neural networks, radial basis function (RBF) neural networks and support vector machines (SVM) for data classification. These schemes showed high performances with both training and test sets and many combinations of features (with a maximum accuracy of 96.67%). Additionally, there was a strong consideration for breathing frequency as a relevant feature in the HRV analysis.', 'kwd': '-', 'title': u'Heart Rate Variability Dynamics for the Prognosis of Cardiovascular Risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540429/', 'p': u'We propose a novel approach for ICU patient risk stratification by combining the learned \u201ctopic\u201d structure of clinical concepts (represented by UMLS codes) extracted from the unstructured nursing notes with physiologic data (from SAPS-I) for hospital mortality prediction. We used Hierarchical Dirichlet Processes (HDP), a non-parametric topic modeling technique, to automatically discover \u201ctopics\u201d as shared groups of co-occurring UMLS clinical concepts. We evaluated the potential utility of the inferred topic structure in predicting hospital mortality using the nursing notes of 14,739 adult ICU patients (mortality 14.6%) from the MIMIC II database. Our results indicate that learned topic structure from the first 24-hour ICU nursing notes significantly improved the performance of the SAPS-I algorithm for hospital mortality prediction. The AUC for predicting hospital mortality from the first 24 hours of physiologic data and nursing text notes was 0.82. Using the physiologic data alone with the SAPS-I algorithm, an AUC of 0.72 was achieved. Thus, the clinical topics that were extracted and used to augment the SAPS-I algorithm significantly improved the performance of the baseline algorithm.Nursing notes from the first 24 hour ICU stay of each adult patient in MIMIC II (version 2.5) were extracted. Patients whose SAPS-1 [9] score could not be determined due to missing data were excluded, as were patients whose lengths-of-stay were less than 24 hours. For patients with multiple ICU stays or multiple hospital stays, notes from the first ICU stay of the first hospital stay were used.', 'kwd': '-', 'title': u'Risk Stratification of ICU Patients Using Topic Models Inferred from Unstructured Progress Notes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4303550/', 'p': u'Multivariate pattern analysis (MVPA) methods have become an important tool in neuroimaging, revealing complex associations and yielding powerful prediction models. Despite methodological developments and novel application domains, there has been little effort to compile benchmark results that researchers can reference and compare against. This study takes a significant step in this direction. We employed three classes of state-of-the-art MVPA algorithms and common types of structural measurements from brain Magnetic Resonance Imaging (MRI) scans to predict an array of clinically relevant variables (diagnosis of Alzheimer\u2019s, schizophrenia, autism, and attention deficit and hyperactivity disorder; age, cerebrospinal fluid derived amyloid\u2013\u03b2 levels and mini-mental state exam score). We analyzed data from over 2,800 subjects, compiled from six publicly available datasets. The employed data and computational tools are freely distributed (https://www.nmr.mgh.harvard.edu/lab/mripredict), making this the largest, most comprehensive, reproducible benchmark image-based prediction experiment to date in structural neuroimaging. Finally, we make several observations regarding the factors that influence prediction performance and point to future research directions. Unsurprisingly, our results suggest that the biological footprint (effect size) has a dramatic influence on prediction performance. Though the choice of image measurement and MVPA algorithm can impact the result, there was no universally optimal selection. Intriguingly, the choice of algorithm seemed to be less critical than the choice of measurement type. Finally, our results showed that cross-validation estimates of performance, while generally optimistic, correlate well with generalization accuracy on a new dataset.We conducted a mass-univariate analysis to map regions where cortical thickness is associated with clinical variables of interest. For this analysis, we used the thickness values sampled onto the highest resolution template, fsaverage, which contains over 140k vertices on each hemisphere, and smoothed on the cortical surface with a Gaussian-like filter of a 10 mm FWHM. We then applied a general linear model at each vertex, where the outcome was thickness and the independent variables were age, gender and the clinical variable. The p-value associated with the clinical variables was then saved for each vertex (see Fig. 3). When identifying cortical areas of significant associations, we applied the false discovery rate (Benjamini and Hochberg, 1995) (FDR, q = 0.05) to correct for multiple comparisons. The total area of significant associations was then computed as the sum of the areas corresponding to the significant vertices in fsaverage.', 'kwd': u'Image-based prediction, Computer aided diagnosis, machine learning, MRI', 'title': u'Clinical prediction from structural brain MRI scans: A large-scale empirical study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4096359/', 'p': u'Using a reduced subset of SNPs in a linear mixed model can improve power for genome-wide association studies, yet this can result in insufficient correction for population stratification. We propose a hybrid approach using principal components that does not inflate statistics in the presence of population stratification and improves power over standard linear mixed models.We analyzed data from 10,204 MS cases and 5429 controls [the National Blood Service (NBS) and the 1958 Birth Cohort (1958BC)] genotyped on Illumina arrays made available to researchers via WTCCC2 (http://wtccc.org.uk/ccc2/). We follow the quality-control standards in Yang et al. (2014). Although Sawcer et al. (2011) analyzed United Kingdom (UK) and non-UK samples separately followed by meta-analysis in most of their analyses, the data made available to researchers include both UK and non-UK cases but only UK controls. We retained all samples to maximize sample size. We considered markers that were present in each of MS, NBS, and 1958BC data sets and removed markers with >0.5% missing data, P < 0.01 for allele-frequency difference between NBS and 1958BC, P < 0.05 for deviation from Hardy\u2013Weinberg equilibrium, P < 0.05 for differential missingness between cases and controls, or minor allele frequency <0.1% in any data set, leaving 360,557 markers. The 75 known associated markers were defined by including, for each MS-associated marker listed in the National Human Genome Research Institute (NHGRI) GWAS catalog (http://genome.gov/gwastudies/), a single best tag at r2 > 0.4 from the set of 360,557 markers if available.', 'kwd': u'mixed models, population stratification, GWAS', 'title': u'Improving the Power of GWAS and Avoiding Confounding from Population Stratification with PC-Select'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4994706/', 'p': u'The proliferation of large genomic databases offers the potential to perform increasingly larger-scale genome-wide association studies (GWAS). Due to privacy concerns, however, access to these data is limited, greatly reducing their usefulness for research. Here, we introduce a computational framework for performing GWAS that adapts principles of differential privacy\u2014a cryptographic theory that facilitates secure analysis of sensitive data\u2014to, for the first time, both protect private phenotype information (e.g., disease status) and correct for population stratification. This framework enables us to produce privacy-preserving GWAS results based on EIGENSTRAT and linear mixed model (LMM)-based statistics, both of which correct for population stratification. We test our differentially private statistics, PrivSTRAT and PrivLMM, on simulated and real GWAS datasets and find they are able to protect privacy while returning meaningful results. Our framework can be used to securely query private genomic datasets to discover which specific genomic alterations may be associated with a disease, thus increasing the availability of these valuable datasets.\n', 'kwd': '-', 'title': u'Enabling Privacy-Preserving GWAS in Heterogeneous Human Populations'}], 'Risk Score AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3768292/', 'p': u'Celiac disease (CD) is a difficult-to-diagnose condition because of its multiple clinical presentations and symptoms shared with other diseases. Gold-standard diagnostic confirmation of suspected CD is achieved by biopsying the small intestine.To develop a clinical decision\u2013support system (CDSS) integrated with an automated classifier to recognize CD cases, by selecting from experimental models developed using intelligence artificial techniques.', 'kwd': u'Decision support systems, clinical Celiac disease, Artificial intelligence', 'title': u'Artificial intelligence techniques applied to the development of a decision\u2013support system for diagnosing celiac disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1479368/', 'p': u'In recent years a number of algorithms for cardiovascular risk assessment has been proposed to the medical community. These algorithms consider a number of variables and express their results as the percentage risk of developing a major fatal or non-fatal cardiovascular event in the following 10 to 20 yearsThe use of predictive algorithms to assess individual absolute risk of cardiovascular future events is currently hampered by methodological and mathematical flaws. The use of newer approaches, such as fuzzy logic and artificial neural networks, linked to artificial intelligence, seems to better address both the challenge of increasing complexity resulting from a correlation between predisposing factors, data on the occurrence of cardiovascular events, and the prediction of future events on an individual level.', 'kwd': '-', 'title': u'How artificial intelligence tools can be used to assess individual patient risk in cardiovascular disease: problems with the current methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689880/', 'p': u"This paper introduces Lynx, an intelligent system for personal safety at home environments, oriented to elderly people living independently, which encompasses a decision support machine for automatic home risk prevention, tested in real-life environments to respond to real time situations. The automatic system described in this paper prevents such risks by an advanced analytic methods supported by an expert knowledge system. It is minimally intrusive, using plug-and-play sensors and machine learning algorithms to learn the elder's daily activity taking into account even his health records. If the system detects that something unusual happens (in a wide sense) or if something is wrong relative to the user's health habits or medical recommendations, it sends at real-time alarm to the family, care center, or medical agents, without human intervention. The system feeds on information from sensors deployed in the home and knowledge of subject physical activities, which can be collected by mobile applications and enriched by personalized health information from clinical reports encoded in the system. The system usability and reliability have been tested in real-life conditions, with an accuracy larger than 81%. The ultimate goal of automatic summaries is to feed the telecare platform with the most relevant clinical data obtained by an unassisted way from medical summaries and move the therapeutics procedures, treatments, or medical recommendations from medical summaries to universAAL ontology. So, the anomaly engine and the predicting intentions engine are capable of learning about personal living habits of the patients, highly correlated with their clinical conditions and prescriptions. To build this integration, thanks to semantic annotation process, we need only to join the most relevant concepts (principal diseases, diagnosis, treatment, and procedures) in the control platform (see Figure 9) with a \u201csame_as\u201d link between ontologies, by the paradigm of Linked Data recommendations [39].", 'kwd': '-', 'title': u'Lynx: Automatic Elderly Behavior Prediction in Home Telecare'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2646316/', 'p': u'The goal of personalised medicine in the intensive care unit (ICU) is to predict which diagnostic tests, monitoring interventions and treatments translate to improved outcomes given the variation between patients. Unfortunately, processes such as gene transcription and drug metabolism are dynamic in the critically ill; that is, information obtained during static non-diseased conditions may have limited applicability. We propose an alternative way of personalising medicine in the ICU on a real-time basis using information derived from the application of artificial intelligence on a high-resolution database. Calculation of maintenance fluid requirement at the height of systemic inflammatory response was selected to investigate the feasibility of this approach.The Multi-parameter Intelligent Monitoring for Intensive Care II (MIMIC II) is a database of patients admitted to the Beth Israel Deaconess Medical Center ICU in Boston. Patients who were on vasopressors for more than six hours during the first 24 hours of admission were identified from the database. Demographic and physiological variables that might affect fluid requirement or reflect the intravascular volume during the first 24 hours in the ICU were extracted from the database. The outcome to be predicted is the total amount of fluid given during the second 24 hours in the ICU, including all the fluid boluses administered.', 'kwd': '-', 'title': u'An artificial intelligence tool to predict fluid requirement in the intensive care unit: a proof-of-concept study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1964229/', 'p': '-', 'kwd': '-', 'title': u'Artificial intelligence in medicine.'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}], 'Arterial Coronary Syndrome AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042924/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part one'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4101190/', 'p': u'This work deals with the development of an intelligent approach for clinical decision making in the diagnosis of the Sleep Apnea/Hypopnea Syndrome, SAHS, from the analysis of respiratory signals and oxygen saturation in arterial blood, SaO2. In order to accomplish the task the proposed approach makes use of different artificial intelligence techniques and reasoning processes being able to deal with imprecise data. These reasoning processes are based on fuzzy logic and on temporal analysis of the information. The developed approach also takes into account the possibility of artifacts in the monitored signals. Detection and characterization of signal artifacts allows detection of false positives. Identification of relevant diagnostic patterns and temporal correlation of events is performed through the implementation of temporal constraints.Analysis of the respiratory signals involves analysis of airflow and abdominal and thoracic excursions for the detection and quantification of respiratory pauses. These pauses are characterized by intervals with amplitude reductions with respect to the normal respiration. On the other hand analysis of arterial blood oxygen saturation signal, SaO2, is performed in order to detect and quantify de-saturation and re-saturation intervals indicative of the presence of apneic events. Main objective is localization of specific apneic evidences in the respiratory activity of the patient to be correlated in time forming diagnostic patterns.', 'kwd': u'Artificial Intelligence in Medicine, Decision Support Systems, Fuzzy Logic, Intelligent Monitoring, Signal Processing, Sleep Apneas, Temporal Reasoning.', 'title': u'Intelligent Approach for Analysis of Respiratory Signals and Oxygen Saturation in the Sleep Apnea/Hypopnea Syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4523666/', 'p': u'Automatic diagnosis of the Sleep Apnea-Hypopnea Syndrome (SAHS) has become an important area of research due to the growing interest in the field of sleep medicine and the costs associated with its manual diagnosis. The increment and heterogeneity of the different techniques, however, make it somewhat difficult to adequately follow the recent developments. A literature review within the area of computer-assisted diagnosis of SAHS has been performed comprising the last 15 years of research in the field. Screening approaches, methods for the detection and classification of respiratory events, comprehensive diagnostic systems, and an outline of current commercial approaches are reviewed. An overview of the different methods is presented together with validation analysis and critical discussion of the current state of the art.The authors declare that there is no conflict of interests regarding the publication of this paper.', 'kwd': '-', 'title': u'Computer-Assisted Diagnosis of the Sleep Apnea-Hypopnea Syndrome: A Review'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408160/', 'p': u'See Glossary (Chapter 29) for explanation of terms.The NHANES 2011 to 2014 data are used in this Update to present\nestimates of the percentage of people with high lipid values, DM, overweight,\nand obesity. The NHIS is used for the prevalence of cigarette smoking and\nphysical inactivity. Data for students in grades 9 through 12 are obtained from\nthe YRBSS.', 'kwd': u'AHA Scientific Statements, cardiovascular diseases, epidemiology, risk factors, statistics, stroke', 'title': u'Heart Disease and Stroke Statistics\u20142017\nUpdate'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2646316/', 'p': u'The goal of personalised medicine in the intensive care unit (ICU) is to predict which diagnostic tests, monitoring interventions and treatments translate to improved outcomes given the variation between patients. Unfortunately, processes such as gene transcription and drug metabolism are dynamic in the critically ill; that is, information obtained during static non-diseased conditions may have limited applicability. We propose an alternative way of personalising medicine in the ICU on a real-time basis using information derived from the application of artificial intelligence on a high-resolution database. Calculation of maintenance fluid requirement at the height of systemic inflammatory response was selected to investigate the feasibility of this approach.The Multi-parameter Intelligent Monitoring for Intensive Care II (MIMIC II) is a database of patients admitted to the Beth Israel Deaconess Medical Center ICU in Boston. Patients who were on vasopressors for more than six hours during the first 24 hours of admission were identified from the database. Demographic and physiological variables that might affect fluid requirement or reflect the intravascular volume during the first 24 hours in the ICU were extracted from the database. The outcome to be predicted is the total amount of fluid given during the second 24 hours in the ICU, including all the fluid boluses administered.', 'kwd': '-', 'title': u'An artificial intelligence tool to predict fluid requirement in the intensive care unit: a proof-of-concept study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3654146/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 36th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4456713/', 'p': u'Despite a vast literature, atherosclerosis and the associated ischemia/reperfusion injuries remain today in many ways a mystery. Why do atheromatous plaques make and store a supply of cholesterol and sulfate within the major arteries supplying the heart? Why are treatment programs aimed to suppress certain myocardial infarction risk factors, such as elevated serum homocysteine and inflammation, generally counterproductive?Our methods are based on an extensive search of the literature in atherosclerotic cardiovascular disease as well as in the area of the unique properties of water, the role of biosulfates in the vascular wall, and the role of electromagnetic fields in vascular flow. Our investigation reveals a novel pathology linked to atherosclerosis that better explains the observed facts than the currently held popular view.', 'kwd': '-', 'title': u'A novel hypothesis for atherosclerosis as a cholesterol sulfate deficiency syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4429500/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 37th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493079/', 'p': '-', 'kwd': '-', 'title': u'36th International Symposium on Intensive Care and Emergency Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3848855/', 'p': u'The classification of Acute Coronary Syndrome (ACS), using artificial intelligence (AI), has recently drawn the attention of the medical researchers. Using this approach, patients with myocardial infarction can be differentiated from those with unstable angina. The present study aims to develop an integrated model, based on the feature selection and classification, for the automatic classification of ACS.A dataset containing medical records of 809 patients suspected to suffer from ACS was used. For each subject, 266 clinical factors were collected. At first, a feature selection was performed based on interviews with 20 cardiologists; thereby 40 seminal features for classifying ACS were selected. Next, a feature selection algorithm was also applied to detect a subset of the features with the best classification accuracy. As a result, the feature numbers considerably reduced to only seven. Lastly, based on the seven selected features, eight various common pattern recognition tools for classification of ACS were used.', 'kwd': u'Acute coronary syndrome, Artificial intelligence, Clinical decision support systems, Classification, Diagnosis', 'title': u'Application of pattern recognition tools for classifying acute coronary syndrome: an integrated medical modeling'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5042923/', 'p': '-', 'kwd': '-', 'title': u'ESICM LIVES 2016: part two'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5052591/', 'p': u'Frequency domain measures of heart rate variability (HRV) are associated with adverse events after a myocardial infarction. However, patterns in the traditional frequency domain (measured in Hz, or cycles per second) may capture different cardiac phenomena at different heart rates. An alternative is to consider frequency with respect to heartbeats, or beatquency. We compared the use of frequency and beatquency domains to predict patient risk after an acute coronary syndrome. We then determined whether machine learning could further improve the predictive performance. We first evaluated the use of pre-defined frequency and beatquency bands in a clinical trial dataset (N\u2009=\u20092302) for the HRV risk measure LF/HF (the ratio of low frequency to high frequency power). Relative to frequency, beatquency improved the ability of LF/HF to predict cardiovascular death within one year (Area Under the Curve, or AUC, of 0.730 vs. 0.704, p\u2009<\u20090.001). Next, we used machine learning to learn frequency and beatquency bands with optimal predictive power, which further improved the AUC for beatquency to 0.753 (p\u2009<\u20090.001), but not for frequency. Results in additional validation datasets (N\u2009=\u20092255 and N\u2009=\u2009765) were similar. Our results suggest that beatquency and machine learning provide valuable tools in physiological studies of HRV.Our work primarily utilized electrocardiographic (ECG) recordings obtained from a clinical trial of patients after NSTEACS19. The dataset consists of all 2,302 patients in the placebo arm, and contains 93 cardiovascular deaths within the median follow-up of one year. We focused on the placebo arm because the treatment arm was prescribed ranolazine, a drug that may have anti-arrhythmic properties20 and thus affect ECG measures. We used this dataset to compare frequency and beatquency LF/HF and to train and test machine learning models. If not otherwise indicated, all results in this work refer to this dataset. In addition, we employed two additional \u201choldout datasets\u201d, for further validation of the machine learning models that were developed, using the dataset described above. These datasets are described in the Supplementary Information. Patient characteristics are reported in Table 1. For all three datasets, up to 7 days of ambulatory ECG signals recorded at 128\u2009Hz are available for each patient. In this work, we used the first 24\u2009hours from each patient to compute the heart rate time series. The protocol was approved by the local or central Institutional Review Board at all participating centers.', 'kwd': '-', 'title': u'Beatquency domain and machine learning improve prediction of cardiovascular death after acute coronary syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4815360/', 'p': u'The endothelins comprise three structurally similar 21-amino acid peptides. Endothelin-1 and -2 activate two G-protein coupled receptors, ETA and ETB, with equal affinity, whereas endothelin-3 has a lower affinity for the ETA subtype. Genes encoding the peptides are present only among vertebrates. The ligand-receptor signaling pathway is a vertebrate innovation and may reflect the evolution of endothelin-1 as the most potent vasoconstrictor in the human cardiovascular system with remarkably long lasting action. Highly selective peptide ETA and ETB antagonists and ETB agonists together with radiolabeled analogs have accurately delineated endothelin pharmacology in humans and animal models, although surprisingly no ETA agonist has been discovered. ET antagonists (bosentan, ambrisentan) have revolutionized the treatment of pulmonary arterial hypertension, with the next generation of antagonists exhibiting improved efficacy (macitentan). Clinical trials continue to explore new applications, particularly in renal failure and for reducing proteinuria in diabetic nephropathy. Translational studies suggest a potential benefit of ETB agonists in chemotherapy and neuroprotection. However, demonstrating clinical efficacy of combined inhibitors of the endothelin converting enzyme and neutral endopeptidase has proved elusive. Over 28 genetic modifications have been made to the ET system in mice through global or cell-specific knockouts, knock ins, or alterations in gene expression of endothelin ligands or their target receptors. These studies have identified key roles for the endothelin isoforms and new therapeutic targets in development, fluid-electrolyte homeostasis, and cardiovascular and neuronal function. For the future, novel pharmacological strategies are emerging via small molecule epigenetic modulators, biologicals such as ETB monoclonal antibodies and the potential of signaling pathway biased agonists and antagonists.Despite the identification of ECE-1 as a rate limiting enzyme in the synthesis of ET-1, there has been much less development of selective small molecule inhibitors that could potentially reduce levels of ET-1 in pathophysiological conditions compared with the considerable effort to discover receptor antagonists. PD159790 was developed to be selective for ECE-1 compared with NEP (Ahn et al., 1998). The compound has been validated experimentally by altering the pH of endothelial cells in culture: at the optimum for ECE-1 activity, pH 6.9, PD159790 inhibited Big-ET-1 conversion but not at the optimum for ECE-2, pH 5.4 (Russell and Davenport, 1999a). In addition, the compound had no effect on the alternative pathway for ET-1 metabolism via chymase generation of ET-1(1-31) (Maguire et al., 2001).', 'kwd': '-', 'title': u'Endothelin'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3534347/', 'p': '-', 'kwd': '-', 'title': u'B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405523/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 38th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2948592/', 'p': u'We describe semantic relation (SR) classification on medical discharge summaries. We focus on relations targeted to the creation of problem-oriented records. Thus, we define relations that involve the medical problems of patients.We represent patients\u2019 medical problems with their diseases and symptoms. We study the relations of patients\u2019 problems with each other and with concepts that are identified as tests and treatments. We present an SR classifier that studies a corpus of patient records one sentence at a time. For all pairs of concepts that appear in a sentence, this SR classifier determines the relations between them. In doing so, the SR classifier takes advantage of surface, lexical, and syntactic features and uses these features as input to a support vector machine. We apply our SR classifier to two sets of medical discharge summaries, one obtained from the Beth Israel-Deaconess Medical Center (BIDMC), Boston, MA and the other from Partners Healthcare, Boston, MA.', 'kwd': u'Lexical context, support vector machines, relation classification for the problem-oriented record, medical language processing', 'title': u'Semantic Relations for Problem-Oriented Medical Records'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4757170/', 'p': '-', 'kwd': '-', 'title': u'WFITN 2015 Abstracts Oral Abstracts'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3808723/', 'p': u'\nBackground. Coronary artery disease (CAD) is a complex, multifactorial disease in which personality seems to play a role but with no definition in combination with other risk factors. Objective. To explore the nonlinear and simultaneous pathways between traditional and personality traits risk factors and coronary stenosis by Artificial Neural Networks (ANN) data mining analysis. Method. Seventy-five subjects were examined for traditional cardiac risk factors and personality traits. Analyses were based on a new data mining method using a particular artificial adaptive system, the autocontractive map (AutoCM). Results. Several traditional Cardiovascular Risk Factors (CRF) present significant relations with coronary artery plaque (CAP) presence or severity. Moreover, anger turns out to be the main factor of personality for CAP in connection with numbers of traditional risk factors. Hidden connection map showed that anger, hostility, and the Type D personality subscale social inhibition are the core factors related to the traditional cardiovascular risk factors (CRF) specifically by hypertension. Discussion. This study shows a nonlinear and simultaneous pathway between traditional risk factors and personality traits associated with coronary stenosis in CAD patients without history of cardiovascular disease. In particular, anger seems to be the main personality factor for CAP in addition to traditional risk factors.The following traditional cardiac risk factors were examined: hypertension: arterial blood pressure \u2265 140/90\u2009mm\u2009Hg or taking antihypertensive medications [16]; diabetes: nonfasting plasma glucose concentration of at least 200\u2009mg/dL (11.1\u2009mmol/L), or fasting plasma glucose level of at least 126\u2009mg/dL (7.0\u2009mmol/L), or being treated with antidiabetic medication; overweight: body mass index (BMI) (calculated as weight divided by height squared) \u226527\u2009kg/m2 (WHO); dyslipidemia: total serum cholesterol level is higher than 240\u2009mg/dL or a serum triglyceride level is 200\u2009mg/dL or more (or both) or use of a lipid-lowering agent; smoking: at least one cigarette per day or quit smoking during the previous year; family history of CAD: a first degree or second degree relative with premature cardiovascular disease (age \u2264 55 years).', 'kwd': '-', 'title': u'Combining Personality Traits with Traditional Risk Factors for Coronary Stenosis: An Artificial Neural Networks Solution in Patients with Computed Tomography Detected Coronary Artery Disease'}], 'Tomography AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3929431/', 'p': u'3D mapping of the human body by X-ray CT provides a novel method of tracing the meridian systems, hence providing useful knowledge toward the modernization of clinical acupuncture practice.The author (J.K.) would like to thank K.S. Soh, PhD, and M.S. Chung, MD, PhD, for helpful advice. The current authors used the Digital Korean data that were produced and distributed by the Catholic Institute for Applied Anatomy, College of Medicine, Catholic University of Korea and the Korean Institute of Science and Technology Information.', 'kwd': u'Acupuncture Points, Digital Korean Data, X-Ray Computed Tomography, Surface Reconstruction, 3-Dimensional Image Process', 'title': u'Positioning Standardized Acupuncture Points on the Whole Body Based on X-Ray Computed Tomography Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4405523/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 38th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/', 'p': u'The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements.Under a mild assumption on the activation function, a two-layer neural network with a finite number of hidden units can approximate any continuous function (63), and thus it is regarded as universal approximator. However, it is also possible to approximate complex functions to the same accuracy using a \u2018deep\u2019 architecture, i.e., more than two layers, with much fewer number of units in total (3). Hence, it is possible to reduce the number of trainable parameters, thus allowing to train with a relatively small dataset (64).', 'kwd': u'Medical image analysis, deep learning, unsupervised feature learning', 'title': u'Deep Learning in Medical Image Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4429500/', 'p': '-', 'kwd': '-', 'title': u'Abstracts from the 37th Annual Meeting of the Society of General Internal Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4114443/', 'p': u'This paper is to report the new imaging of gastric cancers without the use of imaging agents. Both gastric normal regions and gastric cancer regions can be distinguished by using the principal component analysis (PCA) based on the gray level co-occurrence matrix (GLCM).Human gastric cancer BGC823 cells were implanted into the stomachs of nude mice. Then, 3, 5, 7, 9 or 11\xa0days after cancer cells implantation, the nude mice were sacrificed and their stomachs were removed. X-ray in-line phase contrast imaging (XILPCI), an X-ray phase contrast imaging method, has greater soft tissue contrast than traditional absorption radiography and generates higher-resolution images. The gastric specimens were imaged by an XILPCIs\u2019 charge coupled device (CCD) of 9\xa0\u03bcm image resolution. The PCA of the projective images\u2019 region of interests (ROIs) based on GLCM were extracted to discriminate gastric normal regions and gastric cancer regions. Different stages of gastric cancers were classified by using support vector machines (SVMs).', 'kwd': u'X-ray in-line phase contrast imaging, X-ray absorption imaging, Gastric cancer, Principal component analysis, Support vector machine', 'title': u'Investigation of gastric cancers in nude mice using X-ray in-line phase contrast imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4511115/', 'p': u'In this study, we investigated the effectiveness of a novel Iterative Reconstruction (IR) method coupled with Dual-Dictionary Learning (DDL) for image reconstruction in a dedicated breast Computed Tomography (CT) system based on a Cadmium-Zinc-Telluride (CZT) photon-counting detector and compared it to the Filtered-Back-Projection (FBP) method with the ultimate goal of reducing the number of projections necessary for reconstruction without sacrificing image quality. Postmortem breast samples were scanned in a fan-beam CT system and were reconstructed from 100\u2013600 projections with both IR and FBP methods. The Contrast-to-Noise Ratio (CNR) between the glandular and adipose tissues of the postmortem breast samples was calculated to compare the quality of images reconstructed from IR and FBP. The spatial resolution of the two reconstruction techniques was evaluated using Aluminum (Al) wires with diameters of 643, 813, 1020, 1290 and 1630 \xb5m in a plastic epoxy resin phantom with diameter of 13 cm. Both the spatial resolution and the CNR were improved with IR compared to FBP for the images reconstructed from the same number of projections. In comparison with FBP reconstruction, the CNR was improved from 3.4 to 7.5 by using the IR method with 6-fold fewer projections while maintaining the same spatial resolution. The study demonstrated that the IR method coupled with DDL could significantly reduce the required number of projections for a CT reconstruction compared to FBP method while achieving a much better CNR and maintaining the same spatial resolution. From this, the radiation dose and scanning time can potentially be reduced by a factor of approximately 6 by using this IR method for image reconstruction in a CZT-based breast CT system.Figure 2(a) illustrates the construction of the high resolution phantom used in this study, which is motivated by a previous study (Shen et al., 2010). A cylinder with 13 cm in diameter and 2 cm in length is constructed of resin as the phantom base and an insert with 5 fine Al wires of various diameters (643, 813, 1020, 1290 and 1630 \xb5m in diameter) is placed in this base. The resin is chosen not only for its similar x-ray attenuation to breast tissue (0.2076 cm2/g for resin and 0.2186 cm2/g for breast tissue at 50 keV) (Hubbell and Seltzer, 1995), but also for its low cost and convenience in fabricating the phantom base and inserting Al wires. The Al wires in this insert are orientated vertically, and the profiles extracted from reconstructed CT images are proposed to study spatial resolution. These wires are arranged with enough space between each of them to minimize interacted artifacts in the image reconstruction. Figure 2(b) shows a photo of this high-resolution phantom and Al wires inside. Postmortem breast samples were obtained from Willed Body Program in School of Medicine at University of California Irvine, sealed in plastic bags. 6 samples were selected for this study with the mass varying from 114 to 924 gram and the breast density varying from 21% to 72%. These samples were placed in a cylindrical container approximately 10 cm in diameter made of high-density polyethylene plastic during the image acquisition.', 'kwd': u'Iterative reconstruction, Dual-dictionary learning, Contrast-to-noise ratio, Spatial resolution, Spectral breast computed tomography', 'title': u'Dual-Dictionary Learning-Based Iterative Image Reconstruction for Spectral Computed Tomography Application'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5234134/', 'p': u'Metal objects implanted in the bodies of patients usually generate severe streaking artifacts in reconstructed images of X-ray computed tomography, which degrade the image quality and affect the diagnosis of disease. Therefore, it is essential to reduce these artifacts to meet the clinical demands.In this work, we propose a Gaussian diffusion sinogram inpainting metal artifact reduction algorithm based on prior images to reduce these artifacts for fan-beam computed tomography reconstruction. In this algorithm, prior information that originated from a tissue-classified prior image is used for the inpainting of metal-corrupted projections, and it is incorporated into a Gaussian diffusion function. The prior knowledge is particularly designed to locate the diffusion position and improve the sparsity of the subtraction sinogram, which is obtained by subtracting the prior sinogram of the metal regions from the original sinogram. The sinogram inpainting algorithm is implemented through an approach of diffusing prior energy and is then solved by gradient descent. The performance of the proposed metal artifact reduction algorithm is compared with two conventional metal artifact reduction algorithms, namely the interpolation metal artifact reduction algorithm and normalized metal artifact reduction algorithm. The experimental datasets used included both simulated and clinical datasets.', 'kwd': u'Metal artifact reduction, Sinogram inpainting, Gaussian diffusion, Prior image, X-ray CT', 'title': u'Gaussian diffusion sinogram inpainting for X-ray CT metal artifact reduction'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4482362/', 'p': u'This review describes the diffusion model for light transport in tissues and the medical applications of diffuse light. Diffuse optics is particularly useful for measurement of tissue hemodynamics, wherein quantitative assessment of oxy- and deoxy-hemoglobin concentrations and blood flow are desired. The theoretical basis for near-infrared or diffuse optical spectroscopy (NIRS or DOS, respectively) is developed, and the basic elements of diffuse optical tomography (DOT) are outlined. We also discuss diffuse correlation spectroscopy (DCS), a technique whereby temporal correlation functions of diffusing light are transported through tissue and are used to measure blood flow. Essential instrumentation is described, and representative brain and breast functional imaging and monitoring results illustrate the workings of these new tissue diagnostics.Most of the following theoretical discussion will be given in the frequency-domain, with the time-domain solution given for a common case. Frequency-domain sources induce fluence rate disturbances that behave in many ways like overdamped waves. To appreciate this point, we start with the diffusion equation for the fluence rate (Equation (9)) and assume the source term has DC and AC parts and can be written in the form S(r, t) = SDC(r) + SAC(r)e\u2212i\u03c9t. Then we look for the solutions that oscillate at the same angular frequency as the source. These AC solutions will have the following general form\n', 'kwd': '-', 'title': u'Diffuse Optics for Tissue Monitoring and Tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5664813/', 'p': u'Wheat is one of\xa0the most widely grown crop in temperate climates for food and animal feed. In order to meet the demands of the predicted population increase in an ever-changing climate, wheat production needs to dramatically increase. Spike and grain traits are critical determinants of final yield and grain uniformity a commercially desired trait, but their analysis is laborious and often requires destructive harvest. One of the current challenges is to develop an accurate, non-destructive method for spike and grain trait analysis capable of handling large populations.In this study we describe the development of a robust method for the accurate extraction and measurement of spike and grain morphometric parameters from images acquired by X-ray micro-computed tomography (\u03bcCT). The image analysis pipeline developed automatically identifies plant material of interest in \u03bcCT images, performs image analysis, and extracts morphometric data. As a proof of principle, this integrated methodology was used to analyse the spikes from a population of wheat plants subjected to high temperatures under two different water regimes. Temperature has a negative effect on spike height and grain number with the middle of the spike being the most affected region. The data also confirmed that increased grain volume was correlated with the decrease in grain number under mild stress.', 'kwd': u'X-ray micro computed tomography, \u03bcCT, Image analysis, 3D vision, Grain traits, Wheat, Temperature', 'title': u'Non-destructive, high-content analysis of wheat grain traits using X-ray micro computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5659258/', 'p': u'Author contributions: E.L.D., C.J., K.P.K., and N.K. designed research; E.L.D., W.G.R., D.G., X.X., and N.K. performed research; E.L.D., W.G.R., J.A.P., H.F., V.d.A., K.F., J.T.V., and N.K. contributed unpublished reagents/analytic tools; E.L.D. and D.G. analyzed data; E.L.D., J.A.P., K.P.K., and N.K. wrote the paper.', 'kwd': u'Automated segmentation, cell counting, electron microscopy, neocortex, neuroanatomy, X-ray microtomography', 'title': u'Quantifying Mesoscale Neuroanatomy Using X-Ray Microtomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481066/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - A - Postergraduate Educational Programme'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5341795/', 'p': u'Rapid development in the performance of sophisticated optical components, digital image sensors, and computer abilities along with decreasing costs has enabled three-dimensional (3-D) optical measurement to replace more traditional methods in manufacturing and quality control. The advantages of 3-D optical measurement, such as noncontact, high accuracy, rapid operation, and the ability for automation, are extremely valuable for inline manufacturing. However, most of the current optical approaches are eligible for exterior instead of internal surfaces of machined parts. A 3-D optical measurement approach is proposed based on machine vision for the 3-D profile measurement of tiny complex internal surfaces, such as internally threaded holes. To capture the full topographic extent (peak to valley) of threads, a side-view commercial rigid scope is used to collect images at known camera positions and orientations. A 3-D point cloud is generated with multiview stereo vision using linear motion of the test piece, which is repeated by a rotation to form additional point clouds. Registration of these point clouds into a complete reconstruction uses a proposed automated feature-based 3-D registration algorithm. The resulting 3-D reconstruction is compared with x-ray computed tomography to validate the feasibility of our proposed method for future robotically driven industrial 3-D inspection.Yuanzheng Gong is a PhD student in mechanical engineering at the University of Washington. He received his BS degree and PhD in mechanical engineering from the University of Science and Technology of China in 2009 and University of Washington in 2016. His main research interests include three-dimensional reconstruction, optical metrology, stereo vision, and machine vision. He is a student member of SPIE and IEEE.', 'kwd': u'three-dimensional surface reconstruction, three-dimensional measurement, point cloud registration, machine vision, optical metrology', 'title': u'Three-dimensional measurement of small inner surface profiles using feature-based 3-D panoramic registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3533621/', 'p': '-', 'kwd': '-', 'title': u'ECR 2011 Book of Abstracts - A - Postgraduate Educational Programme'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3888983/', 'p': u'We report here a facile thermal decomposition approach to creating tungsten oxide nanorods (WO2.9 NRs) with a length of 13.1 \xb1 3.6\u2005nm and a diameter of 4.4 \xb1 1.5\u2005nm for tumor theranostic applications. The formed WO2.9 NRs were modified with methoxypoly(ethylene glycol) (PEG) carboxyl acid via ligand exchange to have good water dispersability and biocompatibility. With the high photothermal conversion efficiency irradiated by a 980\u2005nm laser and the better X-ray attenuation property than clinically used computed tomography (CT) contrast agent Iohexol, the formed PEGylated WO2.9 NRs are able to inhibit the growth of the model cancer cells in vitro and the corresponding tumor model in vivo, and enable effective CT imaging of the tumor model in vivo. Our \u201ckilling two birds with one stone\u201d strategy could be extended for fabricating other nanoplatforms for efficient tumor theranostic applications.The aqueous solution of PEGylated WO2.9 NRs (100\u2005\u03bcg/mL) showed a blue color with strong absorption in the NIR region (Fig. 2a), which was attributed to the strong localized surface plasmon resonances (LSPR) of the NRs59. The strong NIR absorption of PEGylated WO2.9 NRs motivated us to investigate their potential application as photothermal agents. The temperature increase of the aqueous solution in the presence of the PEGylated NRs as a function of the NR concentration (60 to 1200\u2005\u03bcg/mL) under the 980\u2005nm laser irradiation shows that the solution temperature increase can reach 41.7\xb0C at the NR concentration of 1200\u2005\u03bcg/mL, and higher concentration of NRs leads to more prominent temperature increase (Figs. 2b and S2a). In contrast, the water solution in the absence of NRs does not show any obvious temperature increase under similar experimental conditions (Fig. 2b). To assess the NIR photostability of PEGylated WO2.9 NRs, five cycles of Laser on/off were performed by irradiating the aqueous solution of PEGylated WO2.9 NRs via a 980\u2005nm laser for 10\u2005min (Laser on), followed by cooling down to room temperature without NIR laser irradiation for 30\u2005min (Laser off). As shown in Fig. 2c, the temperature increases of 20.1\xb0C and 30.3\xb0C were able to be achieved after the first laser on for the NR concentration of 100 and 750\u2005\u03bcg/mL, respectively. No significant change in the temperature increase was observed after five cycles. Furthermore, the absorbance of the NRs (180\u2005\u03bcg/mL) at 980\u2005nm remained stable even after ten cycles of laser irradiation (Fig. S2b).', 'kwd': '-', 'title': u'Tungsten Oxide Nanorods: An Efficient Nanoplatform for Tumor CT Imaging and Photothermal Therapy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3202718/', 'p': '-', 'kwd': u'three-dimensional, middle-ear, models, X-ray, computer, tomography', 'title': u'\nThree-Dimensional Modelling of the Middle-Ear Ossicular Chain Using a Commercial High-Resolution X-Ray CT Scanner\n'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5337239/', 'p': '-', 'kwd': u'Pulmonary image analysis, Computer-aided detection, Computer-aided diagnosis, Image processing, Machine learning, Deep learning', 'title': u'Fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5330543/', 'p': u'Kawasaki disease (KD) is an acute childhood disease complicated by coronary artery aneurysms, intima thickening, thrombi, stenosis, lamellar calcifications, and disappearance of the media border. Automatic classification of the coronary artery layers (intima, media, and scar features) is important for analyzing optical coherence tomography (OCT) images recorded in pediatric patients. OCT has been known as an intracoronary imaging modality using near-infrared light which has recently been used to image the inner coronary artery tissues of pediatric patients, providing high spatial resolution (ranging from 10 to 20 \u03bcm). This study aims to develop a robust and fully automated tissue classification method by using the convolutional neural networks (CNNs) as feature extractor and comparing the predictions of three state-of-the-art classifiers, CNN, random forest (RF), and support vector machine (SVM). The results show the robustness of CNN as the feature extractor and random forest as the classifier with classification rate up to 96%, especially to characterize the second layer of coronary arteries (media), which is a very thin layer and it is challenging to be recognized and specified from other tissues.Generating an ensemble of trees using random vectors, which control the growth of each tree in the ensemble significantly increases the classification accuracy. Random Forest works efficiently on large data sets, carries a very low risk of overfitting, and is a robust classifier for noisy data. The trees are grown based on the CART methodology to maximum size without pruning. Two important factors which affect the Random Forest accuracy are the strength, s, of each tree and the correlation, \u03c1, between them. Generalization error for Random Forest classifier is proportional to the ratio \u03c1/s2. Hence, the smaller this ratio, the better functioning of Random Forest will be concluded. The correlation between trees is reduced by random selection of subset of features at each node to split on [43,44]. To improve the performance of the classifier in our experiments, we started from 100 trees and increase the number of trees to 1000. The optimal number of trees is chosen by considering the Out Of Bag (OOB) error rate. By setting the number of trees to 241, the error rate is low, almost close to the minimum error rate, and fewer number of trees reduces the computational burden; so, classifier performance is faster. The number of randomly selected predictors (another tuning parameter in Random Forest) is set to 7. Random Forest training and validation is described in section 2.3.4.', 'kwd': u'(100.0100) Image processing, (100.2960) Image analysis, (100.4996) Pattern recognition, neural networks, (110.0110) Imaging systems, (110.2960) Image analysis, (110.4500) Optical coherence tomography', 'title': u'Deep feature learning for automatic tissue classification of coronary artery using optical coherence tomography'}], 'Risk Score AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684738/', 'p': u'Coronary computed tomography angiography (CTA) can be used to detect and quantitatively assess high-risk plaque features.To validate the ROMICAT score, which was derived using semi-automated quantitative measurements of high-risk plaque features, for the prediction of ACS.', 'kwd': u'coronary computed tomography angiography, acute coronary syndrome, coronary atherosclerotic plaque, acute chest pain, risk score', 'title': u'Computed Tomography-Based High-Risk Coronary Plaque Score to Predict Acute Coronary Syndrome Among Patients With Acute Chest Pain \u2013 Results from the ROMICAT II Trial'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4669991/', 'p': u'LIVER & BILIARY I \u2013 HALL 7__________Disclosure of Interest: None declared', 'kwd': '-', 'title': u'UEG Week 2015 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4364461/', 'p': '-', 'kwd': '-', 'title': u'Abstracts for the 15th International Congress on Schizophrenia Research (ICOSR)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4833122/', 'p': u'In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients\u2014manually annotated by up to four raters\u2014and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%\u201385%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.Table II contains an overview of the methods used by the participating groups in both challenges. In 2012, four out of the twelve participants used generative models, one was a generative-discriminative approach, and five were discriminative; seven used some spatially regularizing model component. Two methods required manual initialization. The two automated segmentation methods that topped the list of competitors during the on-site challenge of the first benchmark used a discriminative probabilistic approach relying on a random forest classifier, boosting the popularity of this approach in the second year. As a result, in 2013 participants employed one generative model, one discriminative-generative model, and eight discriminative models out of which a total of four used random forests as the central learning algorithm; seven had a processing step that enforced spatial regularization. One method required manual initialization. A detailed description of each method is available in the workshop proceedings,3 as well as in the Appendix/Online Supporting Information.', 'kwd': u'MRI, Brain, Oncology/tumor, Image segmentation, Benchmark', 'title': u'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4318357/', 'p': u'The theory of task-based assessment of image quality is reviewed in the context of imaging with ionizing radiation, and objective figures of merit (FOMs) for image quality are summarized. The variation of the FOMs with the task, the observer and especially with the mean number of photons recorded in the image is discussed. Then various standard methods for specifying radiation dose are reviewed and related to the mean number of photons in the image and hence to image quality. Current knowledge of the relation between local radiation dose and the risk of various adverse effects is summarized, and some graphical depictions of the tradeoffs between image quality and risk are introduced. Then various dose-reduction strategies are discussed in terms of their effect on task-based measures of image quality.Popescu and Myers [32] recently published a paper describing a signal-search paradigm, along with a phantom design, that enables the evaluation and comparison of iterative reconstruction algorithms in terms of signal detectability vs dose performance using a few tens of images. Standard commercial phantoms for CT quality assessment have limited utility for quantitative evaluation of image quality, because of the limited area available for deriving the statistics of a model observer in the signal-absent condition. Moreover, such phantoms yield only one signal size/contrast combination per image. Phantoms that allow for multiple realizations of background-only and signal-present regions in each image, along with the addition of search as a component of the detection task, enable the investigator to customize the signal detectability level through adjustment of the search region area so that meaningful image quality comparisons can be made across algorithms or imaging systems with a fairly small number of images.', 'kwd': '-', 'title': u'Task-based measures of image quality and their relation to radiation dose and patient risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4803197/', 'p': u'Conceived and designed the experiments: MM FB KA AD. Performed the experiments: FB KA AD. Analyzed the data: FB KA AD. Contributed reagents/materials/analysis tools: MM FB KA AD. Wrote the paper: FB KA AD MM.The body image concern (BIC) continuum ranges from a healthy and positive body image, to clinical diagnoses of abnormal body image, like body dysmorphic disorder (BDD). BDD and non-clinical, yet high-BIC participants have demonstrated a local visual processing bias, characterised by reduced inversion effects. To examine whether this bias is a potential marker of BDD, the visual processing of individuals across the entire BIC continuum was examined. Dysmorphic Concern Questionnaire (DCQ; quantified BIC) scores were expected to correlate with higher discrimination accuracy and faster reaction times of inverted stimuli, indicating reduced inversion effects (occurring due to increased local visual processing). Additionally, an induced global or local processing bias via Navon stimulus presentation was expected to alter these associations. Seventy-four participants completed the DCQ and upright-inverted face and body stimulus discrimination task. Moderate positive associations were revealed between DCQ scores and accuracy rates for inverted face and body stimuli, indicating a graded local bias accompanying increases in BIC. This relationship supports a local processing bias as a marker for BDD, which has significant assessment implications. Furthermore, a moderate negative relationship was found between DCQ score and inverted face accuracy after inducing global processing, indicating the processing bias can temporarily be reversed in high BIC individuals. Navon stimuli were successfully able to alter the visual processing of individuals across the BIC continuum, which has important implications for treating BDD.', 'kwd': '-', 'title': u'Altering Visual Perception Abnormalities: A Marker for Body Image Concern'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5523839/', 'p': u'CAIDE Dementia Risk Score is the first validated tool for estimating dementia risk based on a midlife risk profile.This observational study investigated longitudinal associations of CAIDE Dementia Risk Score with brain MRI, amyloid burden evaluated with PIB-PET, and detailed cognition measures.', 'kwd': u'Aging, cognition, dementia, neuroimaging', 'title': u'Associations of CAIDE Dementia Risk Score with MRI, PIB-PET measures, and\xa0cognition'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4912513/', 'p': '-', 'kwd': '-', 'title': u'French Intensive Care Society, International congress \u2013 R\xe9animation 2016'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4244172/', 'p': '-', 'kwd': '-', 'title': u'2014 Joint ACTRIMS-ECTRIMS Meeting (MSBoston 2014) MS Journal Online: Poster Session 1'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4212306/', 'p': '-', 'kwd': '-', 'title': u'UEG Week 2014 Poster Presentations'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4519303/', 'p': u'Conceived and designed the experiments: MT KK TE LTW OAA. Performed the experiments: KK MT FB MM TK. Analyzed the data: MT KK LTW FB CLB. Contributed reagents/materials/analysis tools: SD OAA IM IA. Wrote the paper: MT KK LTW OAA FB CLB TK TE IA IM.Bipolar disorder (BD) is a highly heritable disorder with polygenic inheritance. Among the most consistent findings from functional magnetic imaging (fMRI) studies are limbic hyperactivation and dorsal hypoactivation. However, the relation between reported brain functional abnormalities and underlying genetic risk remains elusive. This is the first cross-sectional study applying a whole-brain explorative approach to investigate potential influence of BD case-control status and polygenic risk on brain activation.', 'kwd': '-', 'title': u'Altered Brain Activation during Emotional Face Processing in Relation to Both Diagnosis and Polygenic Risk of Bipolar Disorder'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4573236/', 'p': '-', 'kwd': u'Breast cancer, Computer-aided detection (CAD), Near-term breast cancer risk stratification, Mammographic density feature analysis, Full-field digital mammography (FFDM)', 'title': u'Assessment of a four-view mammographic image feature based fusion model to predict near-term breast cancer risk'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3481065/', 'p': '-', 'kwd': '-', 'title': u'ECR 2012 Book of Abstracts - B - Scientific Sessions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4896250/', 'p': u'Even though antihistamines are the mainstay in the treatment of chronic spontaneous urticaria (CSU), some CSU patients have not responded to antihistamines. Many clinicians have accepted the efficacy of steroids in the treatment of CSU. There is, however, little evidence supporting steroid use in antihistamine-resistant CSU. The purpose of this study was to demonstrate the efficacy of, and suggest a regimen for, oral steroid in the treatment of CSU patients who were refractory to a high dosage of antihistamines. We conducted a retrospective chart review of all patients diagnosed with urticaria between Feburary 1, 2012, and December 31, 2014. A total of 98 patients with CSU were included. Of these, 16 patients (16.3%) were antihistamine-resistant and prescribed a 2-week course of steroid. Thirteen patients (81.2%) were successfully controlled with antihistamines only after stopping the first course. Second course of steroid induced remission additionally in two patients (12.5%). No adverse events and complications associated with oral steroid were observed over the study period. This study demonstrated the excellence of a 2-week course of oral corticosteroid in antihistamine-resistant CSU and propose standardized corticosteroid treatment regimen.Subcutaneous immunotherapy (SCIT) is a clinically effective treatment in atopic dermatitis. However, there was no mouse model to understand the mechanism of house dust mite immunotherapy in atopic dermatitis. The aim of this study was to establish a mouse model of SCIT mouse model of house dust mite induced atopic dermatitis. Female NC/Nga mice were treated with Dermatophagoides farinae (D.farinae) body extract ointment for 4 weeks to induce atopic dermatitis like skin lesion. Then we separated the mice into 2 groups, control group and immunotherapy (IT) group. Both groups were continuously treated with D.farinae body extract ointment for 4 weeks, however, only immunotherapy (IT) group was treated with 8 injections of D.farinae (100 ug subcutaneouse) twice a week along with D.farinae body extract ointment treatment. Subsequently, clinical severity of skin lesion, histological features, total IgE, IL-4 and IL-10 were measured. We observed that AD-like skin lesions of IT group were milder than control group. In histological examination, epidermis was thinner in IT group than control group. The level of total IgE was decreased in IT group than control group at 3 weeks from the beginning of immunotherapy, while the level of IgG4 was remarkably increased in IT group than control group from the beginning of immunotherapy. Increased level of IL-10 in IT group was observed at 2 weeks from the beginning of immunotherapy than control group. However, there was no significant difference of IL-4 between IT group and control group. In the present study, we have established a house dust mite SCIT mouse model and demonstrated that house dust mite SCIT might improve atopic dermatitis-like skin lesion by decreasing total IgE and increasing the IgG4 and Il-10. Using this model, it will be possible to find novel way to potentiate the effects of allergen immunotherapy.', 'kwd': '-', 'title': u'XXIV World Allergy Congress 2015'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4943498/', 'p': u'Schizophrenia is a serious and chronic mental illness which has a profound effect on the health and well-being related with the well-known nature of psychotic symptoms. The exercise has the potential to improve the life of people with schizophrenia improving physical health and alleviating psychiatric symptoms. However, most people with schizophrenia remains sedentary and lack of access to exercise programs are barriers to achieve health benefits. The aim of this study is to evaluate the effect of exercise on I) the type of intervention in mental health, II) in salivary levels of alpha-amylase and cortisol and serum levels of S100B and BDNF, and on III) the quality of life and self-perception of the physical domain of people with schizophrenia. The sample consisted of 31 females in long-term institutions in the Casa de Sa\xfade Rainha Santa Isabel, with age between 25 and 63, and with diagnosis of schizophrenia according to the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR). Physical fitness was assessed by the six-minute walk distance test (6MWD). Biological variables were determined by ELISA (Enzyme-Linked Immunosorbent Assay). Psychological variables were assessed using SF-36, PSPP-SCV, RSES and SWLS tests. Walking exercise has a positive impact on physical fitness (6MWD \u2013 p\u2009=\u20090.001) and physical components of the psychological tests ([SF-36] physical functioning p\u2009<\u20090.05; [PSPP-SCV] functionality p\u2009<\u20090.05 and SWLS p\u2009<\u20090.05 of people with schizophrenia. The walking program enhances the quality of life and self-perception of the physical domain and physical fitness of people with schizophrenia.\u115f', 'kwd': '-', 'title': u'Proceedings of the 3rd IPLeiria\u2019s International Health Congress'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4864935/', 'p': u'Genomics and proteomics are nowadays the dominant techniques for novel biomarker discovery. However, histopathology images contain a wealth of information related to the tumor histology, morphology and tumor-host interactions that is not accessible through these techniques. Thus, integrating the histopathology images in the biomarker discovery workflow could potentially lead to the identification of new image-based biomarkers and the refinement or even replacement of the existing genomic and proteomic signatures. However, extracting meaningful and robust image features to be mined jointly with genomic (and clinical, etc.) data represents a real challenge due to the complexity of the images.We developed a framework for integrating the histopathology images in the biomarker discovery workflow based on the bag-of-features approach \u2013 a method that has the advantage of being assumption-free and data-driven. The images were reduced to a set of salient patterns and additional measurements of their spatial distribution, with the resulting features being directly used in a standard biomarker discovery application. We demonstrated this framework in a search for prognostic biomarkers in breast cancer which resulted in the identification of several prognostic image features and a promising multimodal (imaging and genomic) prognostic signature. The source code for the image analysis procedures is freely available.', 'kwd': u'Histopathology images, Image analysis, Biomarker discovery, Gene expression, Multimodal data mining', 'title': u'Joint analysis of histopathology image features and gene expression in breast cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3959006/', 'p': u"Most of the erythrocyte related diseases are detectable by hematology images analysis. At the first step of this analysis, segmentation and detection of blood cells are inevitable. In this study, a novel method using a line operator and watershed algorithm is rendered for erythrocyte detection and segmentation in blood smear images, as well as reducing over-segmentation in watershed algorithm that is useful for segmentation of different types of blood cells having partial overlap. This method uses gray scale structure of blood cell, which is obtained by exertion of Euclidian distance transform on binary images. Applying this transform, the gray intensity of cell images gradually reduces from the center of cells to their margins. For detecting this intensity variation structure, a line operator measuring gray level variations along several directional line segments is applied. Line segments have maximum and minimum gray level variations has a special pattern that is applicable for detections of the central regions of cells. Intersection of these regions with the signs which are obtained by calculating of local maxima in the watershed algorithm was applied for cells\u2019 centers detection, as well as a reduction in over-segmentation of watershed algorithm. This method creates 1300 sign in segmentation of 1274 erythrocytes available in 25 blood smear images. Accuracy and sensitivity of the proposed method are equal to 95.9% and 97.99%, respectively. The results show the proposed method's capability in detection of erythrocytes in blood smear images.In this study, an algorithm was introduced for over-segmentation reduction in the watershed algorithm in the microscopic images of blood cells and also for locating of them by the line operator. By use of this algorithm, 1300 markers are created as centers for 1274 erythrocytes in 25 blood smear images. It is a promising result to erythrocyte locating or counting, and also over-segmentation reduction using watershed algorithm to segment them.", 'kwd': u'Blood smear images, line operator, watershed algorithm', 'title': u'Detection and Segmentation of Erythrocytes in Blood Smear Images Using a Line Operator and Watershed Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5123388/', 'p': '-', 'kwd': '-', 'title': u'5th International Symposium on Focused Ultrasound'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493079/', 'p': '-', 'kwd': '-', 'title': u'36th International Symposium on Intensive Care and Emergency Medicine'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4793253/', 'p': u'Cardiac computed tomography (CCT) is a reliable and accurate tool for diagnosis of coronary artery diseases and is also frequently used in surgery guidance. Low-dose scans should be considered in order to alleviate the harm to patients caused by X-ray radiation. However, low dose CT (LDCT) images tend to be degraded by quantum noise and streak artifacts. In order to improve the cardiac LDCT image quality, a 3D sparse representation-based processing (3D SR) is proposed by exploiting the sparsity and regularity of 3D anatomical features in CCT. The proposed method was evaluated by a clinical study of 14 patients. The performance of the proposed method was compared to the 2D spares representation-based processing (2D SR) and the state-of-the-art noise reduction algorithm BM4D. The visual assessment, quantitative assessment and qualitative assessment results show that the proposed approach can lead to effective noise/artifact suppression and detail preservation. Compared to the other two tested methods, 3D SR method can obtain results with image quality most close to the reference standard dose CT (SDCT) images.Both the 2D SR method and the proposed 3D SR method include a dictionary training step and an OMP step. In our experiment, it takes 6.27\u2009seconds to train a dictionary for the general 2D SR method. Since the dictionary size (512\u2009\xd7\u20091000) and the training set are much larger than 2D SR method, the training step in 3D SR processing is rather computational intensive and takes about 217.67\u2009seconds. Fortunately, the dictionaries, once trained, can be used to process all the LDCT cases as demonstrated in the above experiments (refer also to41). Table 4 lists the average computation cost required in the operations following the dictionary training only. We can see that, with the trained dictionary available, the 2D SR method requires about 0.90\u2009seconds (in average) to process one 512\u2009\xd7\u2009512 slice; the BM4D method takes 3703.74\u2009seconds (in average) to process one 512\u2009\xd7\u2009512\u2009\xd7\u2009270 CT data set (about 13.72\u2009seconds per 2-D slice) and the 3D SR method requires 843.67\u2009seconds (in average) to process one 512\u2009\xd7\u2009512\u2009\xd7\u2009270 CT data set (about 3.12\u2009seconds per 2-D slice).', 'kwd': '-', 'title': u'Improving Low-dose Cardiac CT Images based on 3D Sparse Representation'}], 'Risk Stratification AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481086/', 'p': u'Breast cancer is the most frequently diagnosed cancer in women. However, the exact cause(s) of breast cancer still remains unknown. Early detection, precise identification of women at risk, and application of appropriate disease prevention measures are by far the most effective way to tackle breast cancer. There are more than 70 common genetic susceptibility factors included in the current non-image-based risk prediction models (e.g., the Gail and the Tyrer-Cuzick models). Image-based risk factors, such as mammographic densities and parenchymal patterns, have been established as biomarkers but have not been fully incorporated in the risk prediction models used for risk stratification in screening and/or measuring responsiveness to preventive approaches. Within computer aided mammography, automatic mammographic tissue segmentation methods have been developed for estimation of breast tissue composition to facilitate mammographic risk assessment. This paper presents a comprehensive review of automatic mammographic tissue segmentation methodologies developed over the past two decades and the evidence for risk assessment/density classification using segmentation. The aim of this review is to analyse how engineering advances have progressed and the impact automatic mammographic tissue segmentation has in a clinical environment, as well as to understand the current research gaps with respect to the incorporation of image-based risk factors in non-image-based risk prediction models.Clinical evaluation has indicated that SFM and FFDM are similar in their ability to detect cancer [107]; however, FFDM is more effective at finding cancer in certain groups of the population, such as women who are premenopausal or perimenopausal, under the age of 50, and have dense breasts. This indicates that in this subgroup some anatomical regions are better visualised by FFDM than SFM. In particular, FFDM demonstrated improved image quality with significantly better depiction of the nipple, skin, pectoral muscle, and especially contrast in parenchymal and fatty tissue [108]. Note that digital mammography imaging generates two types of images for analysis, raw (\u201cfor processing\u201d) and vendor postprocessed (\u201cfor presentation\u201d), of which postprocessed images are commonly used in clinical practice.', 'kwd': '-', 'title': u'A Review on Automatic Mammographic Density and Parenchymal Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5677362/', 'p': u'The segmentation of a high spatial resolution remote sensing image is a critical step in geographic object-based image analysis (GEOBIA). Evaluating the performance of segmentation without ground truth data, i.e., unsupervised evaluation, is important for the comparison of segmentation algorithms and the automatic selection of optimal parameters. This unsupervised strategy currently faces several challenges in practice, such as difficulties in designing effective indicators and limitations of the spectral values in the feature representation. This study proposes a novel unsupervised evaluation method to quantitatively measure the quality of segmentation results to overcome these problems. In this method, multiple spectral and spatial features of images are first extracted simultaneously and then integrated into a feature set to improve the quality of the feature representation of ground objects. The indicators designed for spatial stratified heterogeneity and spatial autocorrelation are included to estimate the properties of the segments in this integrated feature set. These two indicators are then combined into a global assessment metric as the final quality score. The trade-offs of the combined indicators are accounted for using a strategy based on the Mahalanobis distance, which can be exhibited geometrically. The method is tested on two segmentation algorithms and three testing images. The proposed method is compared with two existing unsupervised methods and a supervised method to confirm its capabilities. Through comparison and visual analysis, the results verified the effectiveness of the proposed method and demonstrated the reliability and improvements of this method with respect to other methods.A novel unsupervised method is proposed for evaluating the segmentation quality of VHR remote sensing images. This method uses a multidimensional spectral\u2013spatial feature set as the feature image, which is captured from a raw image using a bilateral filter and a Gabor wavelet filter. Based on this integrated feature set, q\xa0and MI, which respectively denote the spatial stratified heterogeneity and spatial autocorrelation, are computed to indicate the property of each segmentation result from different aspects. These two indicators are then combined into a single overall metric dM using a strategy of measuring the Mahalanobis distance of the quality points in the MI\xa0\u2212\xa0q\xa0space to reveal the segmentation quality. Evaluations of reference segmentation of two synthetic images and three remote sensing images indicate that applying the proposed method to a feature enhanced image yields superior results relative to the original image. The MRS and MSS segmentation algorithms with different parameters were applied to the three remote sensing images to produce multiple segmentation results for evaluation. The experimental results show that indicators q\xa0 and MI appropriately reflect the changes at different segmentation scales, and the combined metric dM clearly reveals the segmentation quality when applied to different algorithms and different parameters. The effectiveness of the combined metric, dM,\xa0 is further demonstrated by comparing two existing unsupervised measures and one supervised method. The results demonstrate the superior potential and robust performance of the proposed method.', 'kwd': u'high spatial resolution remote sensing, image segmentation, unsupervised segmentation evaluation, spatial stratified heterogeneity, statistical features', 'title': u'A Novel Unsupervised Segmentation Quality Evaluation Method for Remote Sensing Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4000389/', 'p': u'Due to rapid advances in radiation therapy (RT), especially image guidance and treatment adaptation, a fast and accurate segmentation of medical images is a very important part of the treatment. Manual delineation of target volumes and organs at risk is still the standard routine for most clinics, even though it is time consuming and prone to intra- and interobserver variations. Automated segmentation methods seek to reduce delineation workload and unify the organ boundary definition. In this paper, the authors review the current autosegmentation methods particularly relevant for applications in RT. The authors outline the methods\u2019 strengths and limitations and propose strategies that could lead to wider acceptance of autosegmentation in routine clinical practice. The authors conclude that currently, autosegmentation technology in RT planning is an efficient tool for the clinicians to provide them with a good starting point for review and adjustment. Modern hardware platforms including GPUs allow most of the autosegmentation tasks to be done in a range of a few minutes. In the nearest future, improvements in CT-based autosegmentation tools will be achieved through standardization of imaging and contouring protocols. In the longer term, the authors expect a wider use of multimodality approaches and better understanding of correlation of imaging with biology and pathology.Among the methods which do not use prior-knowledge for segmentation, region based methods such as adaptive thresholding1 and graph cuts2 as well as edge detection based methods, e.g., watershed segmentation3 have been used for radiotherapy planning.4, 5 Moreover, different types of deformable models6 such as geodesic active contours7 have been applied.8 In order to take advantage of the flexibility of these methods and at the same time compensate their deficiency of using prior-knowledge, especially approaches based on graph cuts and deformable models are often used in combination with atlas- or model-based segmentation methods in hybrid approaches (see Sec. 2C).', 'kwd': u'segmentation, radiation therapy, image processing', 'title': u'Vision 20/20: Perspectives on automated image segmentation for radiotherapy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537099/', 'p': u'Pretreatment risk stratification is key for personalized medicine. While many physicians rely on an \u201ceyeball test\u201d to assess whether patients will tolerate major surgery or chemotherapy, \u201ceyeballing\u201d is inherently subjective and difficult to quantify. The concept of morphometric age derived from cross-sectional imaging has been found to correlate well with outcomes such as length of stay, morbidity, and mortality. However, the determination of the morphometric age is time intensive and requires highly trained experts. In this study, we propose a fully automated deep learning system for the segmentation of skeletal muscle cross-sectional area (CSA) on an axial computed tomography image taken at the third lumbar vertebra. We utilized a fully automated deep segmentation model derived from an extended implementation of a fully convolutional network with weight initialization of an ImageNet pre-trained model, followed by post processing to eliminate intramuscular fat for a more accurate analysis. This experiment was conducted by varying window level (WL), window width (WW), and bit resolutions in order to better understand the effects of the parameters on the model performance. Our best model, fine-tuned on 250 training images and ground truth labels, achieves 0.93\xa0\xb1\xa00.02 Dice similarity coefficient (DSC) and 3.68\xa0\xb1\xa02.29% difference between predicted and ground truth muscle CSA on 150 held-out test cases. Ultimately, the fully automated segmentation system can be embedded into the clinical environment to accelerate the quantification of muscle and expanded to volume analysis of 3D datasets.We reformatted the manually tuned muscle segmentation maps created by domain experts as described previously into acceptable input for convolutional neural networks (CNN). As shown in Fig. \u200bFig.1,1, the axial images and their corresponding color-coded images served as original input data and ground truth labels, respectively. The main challenge for muscle segmentation is the accurate differentiation of muscle tissue from neighboring organs due to their overlapping HU ranges. We manually drew a boundary between organs and muscle, setting the inside region as additional segmentation class (\u201cInside\u201d) in an effort to train the neural network to learn distinguishing features of muscle for a precise segmentation from adjacent organs. The color-coded label images were assigned to pre-defined label indices, including 0 (black) for \u201cBackground\u201d, 1 (red) for \u201cMuscle\u201d, and 2 (green) for \u201cInside\u201d, before passing through CNNs for training as presented in Fig. \u200bFig.11.\n', 'kwd': u'Muscle segmentation, Convolutional neural networks, Computer-aided diagnosis (CAD), Computed tomography, Artificial intelligence, Deep learning', 'title': u'Pixel-Level Deep Segmentation: Artificial Intelligence Quantifies Muscle on Computed Tomography for Body Morphometric Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3165069/', 'p': u'Due to the non-uniform staining, and especially, the varying contrast between the nuclei and extra-cellular regions accross different sections (caused by inconsistent illumination), histopathological images usually exhibit non-consistent colors (e.g., see the five training images in the first row of Fig.1). Therefore, it is desirable to normalize the contrast across different section images. To do this, we transform the original section images by applying histogram equalization to each channel of the RGB color space. The second row corresponds to the transformed training images, which show more consistent lighting conditions across different section images than the original ones. From each of the five transformed images, we manually mark 150 locations in the nuclei and extracellular regions, respectively. An 11 \xd7 11 local neighborhood at each marked location is cropped as a training image patch. Therefore, a total of 750 positive (corresponding to nuclei regions) and 750 negative (corresponding to extra-cellular regions) training patches are created. The third column of Fig.1 shows these patches (darker corresponding to positive and lighter corresponding to negative).We assume that cells generally have elliptical shapes. Although cells have smoother boundaries, the images obtained from previous segmentation step are often characterized by irregular and undulated contours as shown in the first image of Fig.9. To reduce those irregularities, we use Fourier shape descriptors to smooth out the boundaries of the segmented regions [47]. Fourier descriptors provide a powerful mathematical framework to reduce the irregularities of shape boundaries by eliminating high order frequencies, which usually are representative of those irregularities. The fourth image of Fig.9 shows an example of Fourier shape smoothing of the touching cells (the first 12 harmonic components are retained for smoothing).', 'kwd': u'Histopathological image segmentation, touching-cell splitting, supervised learning, color-texture feature extraction, local fourier transform, discriminant analysis, radial-symmetry point, follicular lymphoma', 'title': u'Partitioning Histopathological Images: An Integrated Framework for Supervised Color-Texture Segmentation and Cell Splitting'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891256/', 'p': u'Accurate representation of myocardial infarct geometry is crucial to patient-specific computational modeling of the heart in ischemic cardiomyopathy. We have developed a methodology for segmentation of left ventricular (LV) infarct from clinically acquired, two-dimensional (2D), late-gadolinium enhanced cardiac magnetic resonance (LGE-CMR) images, for personalized modeling of ventricular electrophysiology. The infarct segmentation was expressed as a continuous min-cut optimization problem, which was solved using its dual formulation, the continuous max-flow (CMF). The optimization objective comprised of a smoothness term, and a data term that quantified the similarity between image intensity histograms of segmented regions and those of a set of training images. A manual segmentation of the LV myocardium was used to initialize and constrain the developed method. The three-dimensional geometry of infarct was reconstructed from its segmentation using an implicit, shape-based interpolation method. The proposed methodology was extensively evaluated using metrics based on geometry, and outcomes of individualized electrophysiological simulations of cardiac dys(function). Several existing LV infarct segmentation approaches were implemented, and compared with the proposed method. Our results demonstrated that the CMF method was more accurate than the existing approaches in reproducing expert manual LV infarct segmentations, and in electrophysiological simulations. The infarct segmentation method we have developed and comprehensively evaluated in this study constitutes an important step in advancing clinical applications of personalized simulations of cardiac electrophysiology.The workflow of our methodology for segmentation and 3D reconstruction of LV infarcts from clinically acquired SAX LGE-CMR images is illustrated in Fig. 1. Given an image, the epi- and endo-cardial boundaries of the LV were manually contoured in the image slices by an expert. The infarct was then segmented using the CMF method, for which the LV myocardium was used as the region of interest and the initialization region. We implemented two different versions of the CMF algorithm, namely a 2D approach, where each slice was segmented independently, and a 3D approach (CMF3D), where the entire stack of slices was segmented at once by means of an intermediate image with isotropic resolution that was created using nearest-neighbor interpolation method. Finally, the 3D geometry of the infarct was reconstructed from the infarct segmentations using an interpolation technique we developed based on LogOdds. Subsections B-D below describe in detail the components of the pipeline shown in Fig. 1. All image processing tasks were performed in the Matlab computing environment (Mathworks Inc., Natick, MA) installed on a personal computer equipped with a 2.3 GHz Intel Core i7 CPU, 12 GB of RAM, and the Windows operating system.', 'kwd': u'Image Segmentation, Late-Gadolinium Enhanced Magnetic Resonance Imaging, Convex Optimization, Simulations of Cardiac Electrophysiology', 'title': u'Myocardial Infarct Segmentation from Magnetic Resonance Images for Personalized Modeling of Cardiac Electrophysiology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3925785/', 'p': u'An automatic segmentation framework is proposed to segment the right ventricle (RV) in echocardiographic images. The method can automatically segment both epicardial and endocardial boundaries from a continuous echocardiography series by combining sparse matrix transform, a training model, and a localized region-based level set. First, the sparse matrix transform extracts main motion regions of the myocardium as eigen-images by analyzing the statistical information of the images. Second, an RV training model is registered to the eigen-images in order to locate the position of the RV. Third, the training model is adjusted and then serves as an optimized initialization for the segmentation of each image. Finally, based on the initializations, a localized, region-based level set algorithm is applied to segment both epicardial and endocardial boundaries in each echocardiograph. Three evaluation methods were used to validate the performance of the segmentation framework. The Dice coefficient measures the overall agreement between the manual and automatic segmentation. The absolute distance and the Hausdorff distance between the boundaries from manual and automatic segmentation were used to measure the accuracy of the segmentation. Ultrasound images of human subjects were used for validation. For the epicardial and endocardial boundaries, the Dice coefficients were 90.8 \xb1 1.7% and 87.3 \xb1 1.9%, the absolute distances were 2.0 \xb1 0.42 mm and 1.79 \xb1 0.45 mm, and the Hausdorff distances were 6.86 \xb1 1.71 mm and 7.02 \xb1 1.17 mm, respectively. The automatic segmentation method based on a sparse matrix transform and level set can provide a useful tool for quantitative cardiac imaging.Some implementation details are described below. The algorithm was implemented in MATLAB language. The reproduction of SMT covariance estimation was based on the previous work (Bachega et al 2010, Cao et al 2011, Qin et al 2013). On the initialization step, the criterion for choosing the training model parameter k was that the sum of the first k eigen-values was bigger than 95%, which was 4 for our training model. Its corresponding weight parameters bi were set in the range [\u22120.5, 0.5] for the GA initialization and searching. The GA crossover and mutation parameters were 0.8 and 0.1, respectively. Its stop condition was that the fitness function kept stable for more than 20 generations or when the maximum generation reaches. On the bock matching step, the block size is 50 \xd7 50 pixels and the maximum searching range is 10 pixels. Because this step only tracked one block, it only costs 30 s for 120 continuous images. The weighted parameter \u03bb in the level set, which is related to shape prior term, was set as 0.4, because RV segmentation in 2D echocardiography needed more shape prior to avoid the poorer imaging quality.', 'kwd': '-', 'title': u'Automatic segmentation of right ventricular ultrasound images using sparse matrix transform and a level set'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5469297/', 'p': u'The Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) is an ongoing, longitudinal, multicenter study designed to develop clinical, imaging, genetic, and biochemical biomarkers for the early detection and tracking of Alzheimer\u2019s disease (AD). The initial study, ADNI-1, enrolled 400 subjects with early mild cognitive impairment (MCI), 200 with early AD, and 200 cognitively normal elderly controls. ADNI-1 was extended by a 2-year Grand Opportunities grant in 2009 and by a competitive renewal, ADNI-2, which enrolled an additional 550 participants and will run until 2015. This article reviews all papers published since the inception of the initiative and summarizes the results to the end of 2013. The major accomplishments of ADNI have been as follows: (1) the development of standardized methods for clinical tests, magnetic resonance imaging (MRI), positron emission tomography (PET), and cerebrospinal fluid (CSF) biomarkers in a multicenter setting; (2) elucidation of the patterns and rates of change of imaging and CSF biomarker measurements in control subjects, MCI patients, and AD patients. CSF biomarkers are largely consistent with disease trajectories predicted by \u03b2-amyloid cascade (Hardy, J Alzheimer\u2019s Dis 2006;9(Suppl 3):151\u20133) and tau-mediated neurodegeneration hypotheses for AD, whereas brain atrophy and hypometabolism levels show predicted patterns but exhibit differing rates of change depending on region and disease severity; (3) the assessment of alternative methods of diagnostic categorization. Currently, the best classifiers select and combine optimum features from multiple modalities, including MRI, [18F]-fluorodeoxyglucose-PET, amyloid PET, CSF biomarkers, and clinical tests; (4) the development of blood biomarkers for AD as potentially noninvasive and low-cost alternatives to CSF biomarkers for AD diagnosis and the assessment of \u03b1-syn as an additional biomarker; (5) the development of methods for the early detection of AD. CSF biomarkers, \u03b2-amyloid 42 and tau, as well as amyloid PET may reflect the earliest steps in AD pathology in mildly symptomatic or even nonsymptomatic subjects and are leading candidates for the detection of AD in its preclinical stages; (6) the improvement of clinical trial efficiency through the identification of subjects most likely to undergo imminent future clinical decline and the use of more sensitive outcome measures to reduce sample sizes. Multimodal methods incorporating APOE status and longitudinal MRI proved most highly predictive of future decline. Refinements of clinical tests used as outcome measures such as clinical dementia rating-sum of boxes further reduced sample sizes; (7) the pioneering of genome-wide association studies that leverage quantitative imaging and biomarker phenotypes, including longitudinal data, to confirm recently identified loci, CR1, CLU, and PICALM and to identify novel AD risk loci; (8) worldwide impact through the establishment of ADNI-like programs in Japan, Australia, Argentina, Taiwan, China, Korea, Europe, and Italy; (9) understanding the biology and pathobiology of normal aging, MCI, and AD through integration of ADNI biomarker and clinical data to stimulate research that will resolve controversies about competing hypotheses on the etiopathogenesis of AD, thereby advancing efforts to find disease-modifying drugs for AD; and (10) the establishment of infrastructure to allow sharing of all raw and processed data without embargo to interested scientific investigators throughout the world.Alzheimer\u2019s disease (AD), the most common form of dementia, is a complex disease characterized by an accumulation of \u03b2-amyloid (A\u03b2) plaques and neurofibrillary tangles composed of tau amyloid fibrils [1] associated with synapse loss and neurodegeneration leading to memory impairment and other cognitive problems. There is currently no known treatment that slows the progression of this disorder. According to the 2014 World Alzheimer report, there are an estimated 44 million people worldwide living with dementia at a total cost of more than US$600 billion in 2010, and the incidence of AD throughout the world is expected to triple by 2050. There is a pressing need to find and validate biomarkers to both predict future clinical decline and for use as outcome measures in clinical trials of disease-modifying agents to facilitate phase II\u2013III studies and foster the development of innovative drugs [2]. To this end, Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) was conceived at the beginning of the millennium and began as a North American multicenter collaborative effort funded by public and private interests in October 2004. Although special issues focused on North American ADNI have been published in Alzheimer\u2019s and Dementia [3] and Neurobiology of Aging [4] in addition to a number of other review articles [5\u201312], the purpose of this review is to provide a detailed and comprehensive overview of the approximately 500 papers that have been published as a direct result of ADNI to the end of 2013. The original review [350] covered approximately 200 papers to the end of 2010. The first update [351] detailed an additional 150 papers published from 2011 to mid-2012, and this material is highlighted in yellow. The current iteration adds around 200 more publications from mid-2012 to the end of 2013, and these are highlighted in green. To mid-2014, an additional 70 publications indicate the continuing impact of ADNI.', 'kwd': u'Alzheimer\u2019s disease, Mild cognitive impairment, Amyloid, Tau, Biomarker', 'title': u'2014 Update of the Alzheimer\u2019s Disease Neuroimaging Initiative: A review of papers published since its inception'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2975631/', 'p': u'Conceived and designed the experiments: REL NAP PN. Performed the experiments: XY YZ SD. Analyzed the data: XY YZ REL SD NAP. Wrote the paper: XY YZ SD NAP. Critically reviewed the paper: REL PN.Most extremely preterm newborns exhibit cerebral atrophy/growth disturbances and white matter signal abnormalities on MRI at term-equivalent age. MRI brain volumes could serve as biomarkers for evaluating the effects of neonatal intensive care and predicting neurodevelopmental outcomes. This requires detailed, accurate, and reliable brain MRI segmentation methods. We describe our efforts to develop such methods in high risk newborns using a combination of manual and automated segmentation tools. After intensive efforts to accurately define structural boundaries, two trained raters independently performed manual segmentation of nine subcortical structures using axial T2-weighted MRI scans from 20 randomly selected extremely preterm infants. All scans were re-segmented by both raters to assess reliability. High intra-rater reliability was achieved, as assessed by repeatability and intra-class correlation coefficients (ICC range: 0.97 to 0.99) for all manually segmented regions. Inter-rater reliability was slightly lower (ICC range: 0.93 to 0.99). A semi-automated segmentation approach was developed that combined the parametric strengths of the Hidden Markov Random Field Expectation Maximization algorithm with non-parametric Parzen window classifier resulting in accurate white matter, gray matter, and CSF segmentation. Final manual correction of misclassification errors improved accuracy (similarity index range: 0.87 to 0.89) and facilitated objective quantification of white matter signal abnormalities. The semi-automated and manual methods were seamlessly integrated to generate full brain segmentation within two hours. This comprehensive approach can facilitate the evaluation of large cohorts to rigorously evaluate the utility of regional brain volumes as biomarkers of neonatal care and surrogate endpoints for neurodevelopmental outcomes.', 'kwd': '-', 'title': u'Comprehensive Brain MRI Segmentation in High Risk Preterm Newborns'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3621376/', 'p': u'T2-weighted cardiovascular magnetic resonance (CMR) is clinically-useful for imaging the ischemic area-at-risk and amount of salvageable myocardium in patients with acute myocardial infarction (MI). However, to date, quantification of oedema is user-defined and potentially subjective.We describe a highly automatic framework for quantifying myocardial oedema from bright blood T2-weighted CMR in patients with acute MI. Our approach retains user input (i.e. clinical judgment) to confirm the presence of oedema on an image which is then subjected to an automatic analysis. The new method was tested on 25 consecutive acute MI patients who had a CMR within 48\xa0hours of hospital admission. Left ventricular wall boundaries were delineated automatically by variational level set methods followed by automatic detection of myocardial oedema by fitting a Rayleigh-Gaussian mixture statistical model. These data were compared with results from manual segmentation of the left ventricular wall and oedema, the current standard approach.', 'kwd': u'Myocardial oedema, Bright blood T2-weighted CMR, Rayleigh-Gaussian mixture model, Level set', 'title': u'Highly automatic quantification of myocardial oedema in patients with acute myocardial infarction using bright blood T2-weighted CMR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5145140/', 'p': u'To study the interscan reproducibility of manual versus automated segmentation of carotid artery plaque components, and the agreement between both methods, in high and lower quality MRI scans.24 patients with 30\u201370% carotid artery stenosis were planned for 3T carotid MRI, followed by a rescan within 1 month. A multicontrast protocol (T1w,T2w, PDw and TOF sequences) was used. After co-registration and delineation of the lumen and outer wall, segmentation of plaque components (lipid-rich necrotic cores (LRNC) and calcifications) was performed both manually and automated. Scan quality was assessed using a visual quality scale.', 'kwd': '-', 'title': u'Manual versus Automated Carotid Artery Plaque Component Segmentation in High and Lower Quality 3.0 Tesla MRI Scans'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4981618/', 'p': u'Brain magnetic resonance imaging provides detailed information which can be used to detect and segment white matter lesions (WML). In this work we propose an approach to automatically segment WML in Lupus patients by using T1w and fluid-attenuated inversion recovery (FLAIR) images. Lupus WML appear as small focal abnormal tissue observed as hyperintensities in the FLAIR images. The quantification of these WML is a key factor for the stratification of lupus patients and therefore both lesion detection and segmentation play an important role. In our approach, the T1w image is first used to classify the three main tissues of the brain, white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF), while the FLAIR image is then used to detect focal WML as outliers of its GM intensity distribution. A set of post-processing steps based on lesion size, tissue neighborhood, and location are used to refine the lesion candidates. The proposal is evaluated on 20 patients, presenting qualitative, and quantitative results in terms of precision and sensitivity of lesion detection [True Positive Rate (62%) and Positive Prediction Value (80%), respectively] as well as segmentation accuracy [Dice Similarity Coefficient (72%)]. Obtained results illustrate the validity of the approach to automatically detect and segment lupus lesions. Besides, our approach is publicly available as a SPM8/12 toolbox extension with a simple parameter configuration.This study included 20 Lupus patients. The brain MRIs were performed between 2014 and 2015 at Hospital Cl\xednic, University of Barcelona, Spain, the main national referral institution for lupus. All scans were performed at three Tesla Siemens MAGNETOM TIM Trio scanner, using a 32-channel head coil, with the same protocol including 3D T1 and 3D FLAIR, with a voxel size = 1 \xd7 1 \xd7 1mm3. The lesions were semiautomatically annotated on FLAIR images by an expert neuroradiologist. They present a lesion volume mean and range (min-max) per patient of 0.217 [11\u22121459] mm3. This study was carried out in accordance with the ethical recommendations of the Hospital Cl\xednic committee (IDIBAPS, Barcelona), with written informed consent from all subjects.', 'kwd': u'magnetic resonance images, lupus disease, image analysis, automatic lesion detection and segmentation', 'title': u'Automated Detection of Lupus White Matter Lesions in MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3416877/', 'p': u'This study was in compliance with the Health Insurance Portability and Accountability Act (HIPAA) and received institutional review board (IRB) approval. Anonymized full-field DM images acquired as part of a separate IRB-approved multimodality breast cancer screening trial previously completed in our department (July 2007\u2013March 2008) were retrospectively analyzed. Women who participated in the trial were asymptomatic volunteers who presented for annual screening mammography and had given written informed consent before their participation. Bilateral, MLO view DM imaging was performed using a full-field digital mammography unit (Senographe DS; GE Healthcare, Chalfont St Giles, UK). The raw digital mammograms were acquired at a 100 \u03bcm isotropic resolution using a 14-bit gray-level depth. The raw mammograms were processed using PremiumView\u2122 (GE Healthcare), a vendor-specific, embedded adaptive histogram equalization algorithm41, 42 which produces 12-bit gray-level postprocessed images. Of the 83 women originally enrolled in the trial, two were excluded from this analysis: one due to a diagnosis of breast cancer, the other due to insufficient image quality. The remaining 81 women were, therefore, available for retrospective breast density analysis.Area-based breast PD% was estimated by a trained breast-imaging radiologist for these 81 women on a per-breast basis using a validated, interactive, image-thresholding tool for breast PD% estimation (Cumulus, Ver. 4.0, Univ. Toronto) (Ref. 43) in both the raw and processed images, for a total of 324 digital mammograms. As a result of the different visualization of the breast tissue between raw and postprocessed images, tailored approaches were adopted for the estimation breast PD%.39 Briefly, for the postprocessed DM images, the digital mammograms were first windowed by the radiologist for optimal display. Following this, the background air region was excluded via a manually determined intensity threshold, therefore, allowing the breast boundary to be designated. The pectoral muscle region was subsequently excluded via manual delineation of the pectoral muscle edge. The remaining portion of the image was designated as the breast tissue, and the total area of this region is computed by the software. Following identification of the breast, a second, user-defined gray-level intensity threshold is selected in order to define the gray-level cut-off between fibroglandular and adipose tissue, and those pixels within the delineated breast region above this gray-level threshold are designated as fibroglandular tissue. PD% is then computed as the percentage of the breast area occupied by fibroglandular tissue. A similar process is used to estimate PD% from raw images, except that since the raw images are not optimized for clinical visualization and interpretation, the digital mammographic image were rewindowed by the radiologist before each of the segmentation and thresholding steps described above. Two readings per image were performed by the radiologist, each six months apart, and the average of the two reading was considered as our gold-standard in order to minimize the effects of intrareader variability. The intrareader correlation and 95% confidence interval (CI) between the two radiologist PD% readings was found to be r = 0.95 (p < 0.001; CI: 0.93\u20130.97) for the raw DM images and r = 0.93 (p < 0.001; CI: 0.90\u20130.95) for the processed DM images. Table \u200bTable11 provides the distribution of BIRADs density categories assigned to the 324 DM images in this dataset.', 'kwd': u'digital mammography, breast density, breast cancer risk estimation, quantitative imaging', 'title': u'Estimation of breast percent density in raw and processed full field digital mammography images via adaptive fuzzy c-means clustering and support vector machine segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4745818/', 'p': u"This paper presents a method for the automatic 3D segmentation of the ascending aorta from coronary computed tomography angiography (CCTA). The segmentation is performed in three steps. First, the initial seed points are selected by minimizing a newly proposed energy function across the Hough circles. Second, the ascending aorta is segmented by geodesic distance transformation. Third, the seed points are effectively transferred through the next axial slice by a novel transfer function. Experiments are performed using a database composed of 10 patients' CCTA images. For the experiment, the ground truths are annotated manually on the axial image slices by a medical expert. A comparative evaluation with state-of-the-art commercial aorta segmentation algorithms shows that our approach is computationally more efficient and accurate under the DSC (Dice Similarity Coefficient) measurements.In this paper, we developed a method for the fast and fully automatic extraction of the ascending aorta from CCTA images. The method incorporates two separated algorithms, a circular Hough transformation for initialization and the Geodesic distance algorithm for segmentation. The circular Hough transformation algorithm could automatically find the most probable ascending aortic circle in the axial image. The initial seeding was found, and the VOI was defined after detecting the aortic circle. Then, the boundary of the ascending aorta was repeatedly delineated slice by slice by the geodesic distance algorithm until the aortic arch or valve was reached or until the ascending aorta was completely segmented. The proposed method was evaluated and compared with the results from other commercial workstations using 10 data sets. For all of the datasets, the proposed method segmented the ascending aorta successfully. The proposed method showed a highly accurate DSC with ground truth, and high performance regarding the computation time was measured. Future work will focus on the segmentation of the aortic valve to overcome the aforementioned limitations mentioned in Section 4.", 'kwd': '-', 'title': u'Geodesic Distance Algorithm for Extracting the Ascending Aorta from 3D CT Images'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5355004/', 'p': u'One in six men will develop prostate cancer in his life time. Early\ndetection and accurate diagnosis of the disease can improve cancer survival and\nreduce treatment costs. Recently, imaging of prostate cancer has greatly\nadvanced since the introduction of multi-parametric magnetic resonance imaging\n(mp-MRI). Mp-MRI consists of T2-weighted sequences combined with functional\nsequences including dynamic contrast-enhanced MRI, diffusion-weighted MRI, and\nMR spectroscopy imaging. Due to the big data and variations in imaging\nsequences, detection can be affected by multiple factors such as observer\nvariability and visibility and complexity of the lesions. In order to improve\nquantitative assessment of the disease, various computer-aided detection systems\nhave been designed to help radiologists in their clinical practice. This review\npaper presents an overview of literatures on computer-aided detection of\nprostate cancer with mp-MRI, which include the technology and its applications.\nThe aim of the survey is threefold: an introduction for those new to the field,\nan overview for those working in the field, and a reference for those searching\nfor literature on a specific application.The functional MR imaging data, like DCE-MRI and MRS, are more complex\nand larger in amounts than anatomic MR imaging. There are clinical needs to\ndevelop fast, cost-effective, supportive techniques, such as computer-aided\nanalysis tools, for easy and more reproducible diagnosis of prostate cancer.\nResearchers have focused on developing CADx methodology for automated prostate\nMRS classification and DCE-MRI analysis. Because all functional MR imaging\ntechniques have their strengths and shortcomings, single technique cannot\nadequately detect and characterize PCa. The combination of anatomic (T2W) images\nand functional techniques has been shown to increase the accuracy of MR imaging\nfor diagnosis of PCa. Table 1 compares\nthe performance of the major published prostate CADx systems [13, 14, 16\u201318, 22, 26, 27, 36, 37, 39, 51, 52, 54\u201357, 62, 122, 129, 140\u2013151]. Chan et\nal. were one of the first groups who implemented an mp-MRI CADx system for the\ndiagnosis of prostate cancer [122]. In\ntheir approach they used line-scan diffusion, T2 and T2-weighted images to\nidentify predefined areas of the peripheral zone of the prostate for the\npresence of prostate cancer. Viswanath et al. [129] present an mp-MRI CADx system for PCa detection by integrating\nfunctional and structural information obtained via DCE and T2W MRI. Liu et al.\n[141] present fuzzy MRF models for\nprostate cancer detection of multispectral MR prostate images. Tiwari et al.\n[55] investigated the use of MR\nspectroscopy in combination with T2W MRI to identify the voxels that are\naffected by prostate cancer. They also introduced the use of wavelet embedding\nto map MRS and T2-W texture features into a common space. In a study by Peng et\nal. [27], the combination of\n10th percentile ADC, average ADC, and T2-weighted skewness with\nCADx yielded an AUC value of 0.95 in differentiating prostate cancer from normal\ntissue. The combination achieved higher accuracy than any MR parameter alone. In\na more recent study by Litjens et al [62], they developed a fully automated computer-aided detection system\nwhich consists of two stages. The first (detection) stage consists of\nsegmentation of the prostate on the transversal T2W MRI, extraction of voxel\nfeatures from the image volumes, classification of the voxels and candidate\nselection. The second (diagnosis) stage consists of candidate segmentation,\ncandidate feature extraction and candidate classification. The system was\nevaluated on a large consecutive cohort of 347 patients, and yielded an AUC\nvalue of 0.889.', 'kwd': u'Prostate cancer, MR imaging, Image Quantification, Computer-aided Detection', 'title': u'Computer-Aided Detection of Prostate Cancer with MRI: Technology and\nApplications'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4263651/', 'p': u'Author contributions: Guarantors of integrity of entire study, B.M.E., R.J.H.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; approval of final version of submitted manuscript, all authors; literature research, B.M.E., D.C.W., W.B.P., T.F.C.; clinical studies, B.M.E., H.J.K., D.C.W., W.B.P., J.N.C., R.J.H., A.L., P.L.N., T.F.C.; statistical analysis, B.M.E., H.J.K., T.F.C.; and manuscript editing, B.M.E., H.J.K., W.B.P., A.L., P.L.N., T.F.C.Multivariate survival analyses, visual observations, and receiver operating characteristic comparisons helped demonstrate that, compared with conventional segmentation, contrast-enhanced T1-weighted subtraction maps of contrast-enhancing lesions produce better stratification of high- and low-risk patients treated with bevacizumab and therefore should be used in future clinical trials involving evaluation of antiangiogenic therapies in brain tumors.', 'kwd': '-', 'title': u'Recurrent Glioblastoma Treated with Bevacizumab: Contrast-enhanced T1-weighted Subtraction Maps Improve Tumor Delineation and Aid Prediction of Survival in a Multicenter Clinical Trial'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3925465/', 'p': u'The University of Washington Animal Care Committee approved the experimental protocol. Five male Sprague-Dawley rats weighing between 244 and 276 g were anesthetized by intraperitoneal injection of 80 mg/kg ketamine and 10 mg/kg xylazine, sufficient to prevent withdrawal of paw after pinch. A tracheostomy was performed, and an internal jugular vein was cannulated. The animals were mechanically ventilated at a rate of 50 breaths/min with a tidal volume of approximately 3 mL, using a piston pump ventilator. The rats were then deeply anesthetized with an intraperitoneal injection of ketamine/xylazine; their chests widely opened, and exsanguinated. The pulmonary artery was cannulated; the main aorta tied off, and blood was injected to fill the vasculature (1.3 \xb1 0.5 mL). The lungs were removed from the chest, filled via the trachea with Optimal Cutting Temperature media (OCT, Sakura Finetek Inc., Torrance, CA) until they appeared inflated to total lung capacity (10.5\xb10.7 mL) and then frozen. The frozen lung was surrounded by a mixture of 99.25% OCT and 0.75% India ink and returned to the freezer. The ink was used to create a contrast between the lung parenchyma and the surrounding OCT.The serial block-face imaging cryomicrotome (Barlow Scientific, Inc. Olympia WA) is a computer driven instrument that acquires multi-spectral digital images of en face tissue blocks that are serially sliced. It enables the three-dimensional imaging of fluorescence and fluorescent microspheres at a microscopic level. The instrument has previously been described in detail [2], but has been upgraded recently. The instrument comprises a cryostatic microtome, a Redlake MegaPlus II ES3200 camera (San Diego CA) with a resolution of 2184 \xd7 1472 pixels, a Micro-Nikkor 200 mm f/4D IFED lens (Nikon Corp, Tokyo Japan), a metal halide lamp (PE300BF Cermax, Excelitas Technologies, Fremont CA), a set of excitation/emission filters that allow isolation of specific spectral channels, and a computer controlling the device and storing images. The microtome serially sections thin slices of a frozen tissue block of up to 54\xd736 mm in size along the x- and y-direction. Following every cut, images of the tissue block are captured with different pairings of excitation/emission filters. Note that the tissue slices themselves are not utilized and are discarded. The sequence of two-dimensional images results in a three-dimensional image with potentially several spectral channels for each voxel.', 'kwd': u'airway segmentation, rat lung, serial block-face imaging cryomicrotome', 'title': u'Airway Tree Segmentation in Serial Block-Face Cryomicrotome Images of Rat Lungs'}]}