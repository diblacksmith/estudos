{'Tomography AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5153359/', 'p': u'The VESSEL12 (VESsel SEgmentation in the Lung) challenge objectively compares the performance of different algorithms to identify vessels in thoracic computed tomography (CT) scans. Vessel segmentation is fundamental in computer aided processing of data generated by 3D imaging modalities. As manual vessel segmentation is prohibitively time consuming, any real world application requires some form of automation. Several approaches exist for automated vessel segmentation, but judging their relative merits is difficult due to a lack of standardized evaluation. We present an annotated reference dataset containing 20 CT scans and propose nine categories to perform a comprehensive evaluation of vessel segmentation algorithms from both academia and industry. Twenty algorithms participated in the VESSEL12 challenge, held at International Symposium on Biomedical Imaging (ISBI) 2012. All results have been published at the VESSEL12 website http://vessel12.grand-challenge.org. The challenge remains ongoing and open to new participants. Our three contributions are: (1) an annotated reference dataset available online for evaluation of new algorithms; (2) a quantitative scoring system for objective comparison of algorithms; and (3) performance analysis of the strengths and weaknesses of the various vessel segmentation methods in the presence of various lung diseases.The aim of VESSEL12 Challenge, organized in conjunction with the International Symposium on Biomedical Imaging 2012 (ISBI\u201912), is to provide a public platform to compare the performance of different segmentation algorithms to identify lung vessels in thoracic computed tomography (CT) data. An additional goal is to characterize what kind of anatomical neighborhood may complicate vessel segmentation, for example the presence of nodules, dense consolidation and parenchymal or bronchial abnormalities.', 'kwd': u'Thoracic computed tomography, Lung vessels, Algorithm comparison, Segmentation, Challenge', 'title': u'Comparing algorithms for automated vessel segmentation in computed tomography scans of the lung: the VESSEL12 study'}], 'Arterial Coronary Syndrome AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}], 'Patient Assessment AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5219634/', 'p': u'Amyotrophic lateral sclerosis (ALS) is a progressive neuromuscular disease, with large variation in survival between patients. Currently, it remains rather difficult to predict survival based on clinical parameters alone. Here, we set out to use clinical characteristics in combination with MRI data to predict survival of ALS patients using deep learning, a machine learning technique highly effective in a broad range of big-data analyses. A group of 135 ALS patients was included from whom high-resolution diffusion-weighted and T1-weighted images were acquired at the first visit to the outpatient clinic. Next, each of the patients was monitored carefully and survival time to death was recorded. Patients were labeled as short, medium or long survivors, based on their recorded time to death as measured from the time of disease onset. In the deep learning procedure, the total group of 135 patients was split into a training set for deep learning (n\xa0=\xa083 patients), a validation set (n\xa0=\xa020) and an independent evaluation set (n\xa0=\xa032) to evaluate the performance of the obtained deep learning networks. Deep learning based on clinical characteristics predicted survival category correctly in 68.8% of the cases. Deep learning based on MRI predicted 62.5% correctly using structural connectivity and 62.5% using brain morphology data. Notably, when we combined the three sources of information, deep learning prediction accuracy increased to 84.4%. Taken together, our findings show the added value of MRI with respect to predicting survival in ALS, demonstrating the advantage of deep learning in disease prognostication.Each of the 135 patients was categorized according to the true survival time (i.e. time between disease onset and death): short survivors with survival up to 25\xa0months after disease onset, medium survivors with survival between 25 and 50\xa0months after disease onset, and long survivors living over 50\xa0months after disease onset (Elamin et al., 2015). The group of long survivors consisted of patients who either died after a disease duration of at least 50\xa0months or were still alive and had a disease duration of at least 50\xa0months at time of analysis.', 'kwd': u'Deep learning, Neural network, Amyotrophic lateral sclerosis, White matter connectivity, Survival, Prediction', 'title': u'Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/', 'p': u'The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements.Under a mild assumption on the activation function, a two-layer neural network with a finite number of hidden units can approximate any continuous function (63), and thus it is regarded as universal approximator. However, it is also possible to approximate complex functions to the same accuracy using a \u2018deep\u2019 architecture, i.e., more than two layers, with much fewer number of units in total (3). Hence, it is possible to reduce the number of trainable parameters, thus allowing to train with a relatively small dataset (64).', 'kwd': u'Medical image analysis, deep learning, unsupervised feature learning', 'title': u'Deep Learning in Medical Image Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869115/', 'p': u'Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name \u201cdeep patient\u201d. We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.Feature learning algorithms are usually evaluated in supervised applications to take advantage of the available manually annotated labels. Here we used the Mount Sinai data warehouse to learn the deep features and we evaluated them in predicting patient future diseases. The Mount Sinai Health System generates a high volume of structured, semi-structured and unstructured data as part of its healthcare and clinical operations, which include inpatient, outpatient and emergency room visits. Patients in the system can have as long as 12\u2009years of follow up unless they moved or changed insurance. Electronic records were completely implemented by our health system starting in 2003. The data related to patients who visited the hospital prior to 2003 was migrated to the electronic format as well but we may lack certain details of hospital visits (i.e., some diagnoses or medications may not have been recorded or transferred). The entire EHR dataset contains approximately 4.2 million de-identified patients as of March 2015, and it was made available for use under IRB approval following HIPAA guidelines. We retained all patients with at least one diagnosed disease expressed as numerical ICD-9 between 1980 and 2014, inclusive. This led to a dataset of about 1.2 million patients, with every patient having an average of 88.9 records. Then, we considered all records up to December 31, 2013 (i.e., \u201csplit-point\u201d) as training data (i.e., 34\u2009years of training information) and all the diagnoses in 2014 as testing data.', 'kwd': '-', 'title': u'Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5661078/', 'p': u'Molecular imaging enables the visualization and quantitative analysis of the alterations of biological procedures at molecular and/or cellular level, which is of great significance for early detection of cancer. In recent years, deep leaning has been widely used in medical imaging analysis, as it overcomes the limitations of visual assessment and traditional machine learning techniques by extracting hierarchical features with powerful representation capability. Research on cancer molecular images using deep learning techniques is also increasing dynamically. Hence, in this paper, we review the applications of deep learning in molecular imaging in terms of tumor lesion segmentation, tumor classification, and survival prediction. We also outline some future directions in which researchers may develop more powerful deep learning models for better performance in the applications in cancer molecular imaging.The papers on diverse applications of deep learning in different molecular imaging of cancer published from 2014 onwards were included. This review contains 25 papers and is organized according to the application of deep learning in cancer molecular imaging, including tumor lesion segmentation, cancer classification, and prediction of patient survival.  Table 1 summarizes the 13 different studies on tumor lesion segmentation, while Table 2 summarizes the 10 different studies on cancer classification. Two interesting papers on prediction of patient survival are also reviewed (Table 3). To our best knowledge, there is no previous work making such a comprehensive review on this issue. In this regard, we believe this survey can present radiologists and physicians with the application status of advanced artificial intelligent techniques in molecular images analysis and hence inspire more applications in clinical practice. Biomedical engineering researchers may also benefit from this survey by acquiring the state of the art in this field or inspiration for better models/methods in future research.', 'kwd': '-', 'title': u'Application of Deep Learning in Automated Analysis of Molecular Images in Cancer: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537095/', 'p': u'Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends.This brain tumor image segmentation challenge in conjunction with the MICCAI conference has been held annually since 2012 in order to evaluate the current state-of-the-art in automated brain tumor segmentation and compare between different methods. For this purpose, a large dataset of brain tumor MR scans and ground truth (five labels: healthy brain tissue, necrosis, edema, non-enhanced, and enhanced regions of tumors) are made publicly available. The training data has increased over the years. Currently (Brats 2015\u20132016), the training set comprises 220 subjects with high grade and 54 subjects with low-grade, and the test set comprises 53 subjects with mixed grades. All datasets have been aligned to the same anatomical template and interpolated to 1\xa0mm3 voxel resolution. Each dataset has pre-contrast T1, post contrast T1, T2, and T2 FLAIR MRI volumes. The co-registered, skull-stripped, and annotated training dataset and evaluation results of algorithms are available via the Virtual Skeleton Database (https://www.virtualskeleton.ch/).', 'kwd': u'Deep learning, Quantitative brain MRI, Convolutional neural network, Brain lesion segmentation', 'title': u'Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5651626/', 'p': u'Myelin imaging is a form of quantitative magnetic resonance imaging (MRI) that measures myelin content and can potentially allow demyelinating diseases such as multiple sclerosis (MS) to be detected earlier. Although focal lesions are the most visible signs of MS pathology on conventional MRI, it has been shown that even tissues that appear normal may exhibit decreased myelin content as revealed by myelin-specific images (i.e., myelin maps). Current methods for analyzing myelin maps typically use global or regional mean myelin measurements to detect abnormalities, but ignore finer spatial patterns that may be characteristic of MS. In this paper, we present a machine learning method to automatically learn, from multimodal MR images, latent spatial features that can potentially improve the detection of MS pathology at early stage. More specifically, 3D image patches are extracted from myelin maps and the corresponding T1-weighted (T1w) MRIs, and are used to learn a latent joint myelin-T1w feature representation via unsupervised deep learning. Using a data set of images from MS patients and healthy controls, a common set of patches are selected via a voxel-wise t-test performed between the two groups. In each MS image, any patches overlapping with focal lesions are excluded, and a feature imputation method is used to fill in the missing values. A feature selection process (LASSO) is then utilized to construct a sparse representation. The resulting normal-appearing features are used to train a random forest classifier. Using the myelin and T1w images of 55 relapse-remitting MS patients and 44 healthy controls in an 11-fold cross-validation experiment, the proposed method achieved an average classification accuracy of 87.9% (SD\xa0=\xa08.4%), which is higher and more consistent across folds than those attained by regional mean myelin (73.7%, SD\xa0=\xa013.7%) and T1w measurements (66.7%, SD\xa0=\xa010.6%), or deep-learned features in either the myelin (83.8%, SD\xa0=\xa011.0%) or T1w (70.1%, SD\xa0=\xa013.6%) images alone, suggesting that the proposed method has strong potential for identifying image features that are more sensitive and specific to MS pathology in normal-appearing brain tissues.A cohort of 55 relapsing-remitting MS (RRMS) patients and a cohort of 44 age- and gender-matched normal control (NC) subjects were included in this study. The median age and range for both groups were 45 and 30\u201360. For the RRMS patients, 63.6% (35/55) were female, and 63.5% (28/44) of the NC subjects were female. The McDonald 2010 criteria (Polman et al., 2011) were used to diagnose the patients for MS. All patients underwent a neurological assessment and were scored on the Expanded Disability Status Scale (EDSS) (Kurtzke, 1983). The median EDSS and range were 4 and 0\u20135. Informed consent from each participant and ethical approval by the local ethics committee were obtained prior to the study.', 'kwd': u'Deep learning, Multiple sclerosis, Myelin water imaging, Machine learning, Magnetic resonance imaging', 'title': u'Deep learning of joint myelin and T1w MRI features in normal-appearing brain tissue to distinguish between multiple\xa0sclerosis\xa0patients and healthy controls'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5380996/', 'p': u'Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.The dataset consisted of 74 whole-slide images of breast tumour resection samples which either retrieved from the AstraZeneca BioBank or acquired from a commercial provider (Dako Denmark A/S). Slides were obtained by cutting formalin-fixed, paraffin embedded human breast cancer samples into 4\u2009\u03bcm-thick sections, stained by IHC for HER2 demonstration (monoclonal Rabbit Anti-Human HER2 antibody, Dako Denmark A/S) and counterstained with haematoxylin using a Dako Autostainer Link48 (Dako Denmark A/S). Slides were digitized with an Aperio ScanScope whole-slide imaging microscope (Aperio, Leica Biosystems Imaging, Inc.) at a resolution of 0.49\u2009\u03bcm/pixel. The slides were reviewed to confirm the presence of invasive carcinoma and a total of 71 invasive carcinoma cases were selected for the study.', 'kwd': '-', 'title': u'Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5431941/', 'p': u'Precision medicine approaches rely on obtaining precise knowledge of the true state of health of an individual patient, which results from a combination of their genetic risks and environmental exposures. This approach is currently limited by the lack of effective and efficient non-invasive medical tests to define the full range of phenotypic variation associated with individual health. Such knowledge is critical for improved early intervention, for better treatment decisions, and for ameliorating the steadily worsening epidemic of chronic disease. We present proof-of-concept experiments to demonstrate how routinely acquired cross-sectional CT imaging may be used to predict patient longevity as a proxy for overall individual health and disease status using computer image analysis techniques. Despite the limitations of a modest dataset and the use of off-the-shelf machine learning methods, our results are comparable to previous \u2018manual\u2019 clinical methods for longevity prediction. This work demonstrates that radiomics techniques can be used to extract biomarkers relevant to one of the most widely used outcomes in epidemiological and clinical research \u2013 mortality, and that deep learning with convolutional neural networks can be usefully applied to radiomics research. Computer image analysis applied to routinely collected medical images offers substantial potential to enhance precision medicine initiatives.We created a predictive model using multivariable survival analysis (Cox regression). The model was informed by the top 5 covariates selected by minimum redundancy-maximum relevance feature selection50. These covariates were standardised, and the resulting risk score was dichotomised at the mean to create high-risk and low-risk phenotypes. Table\xa02 shows the 5-year mortality rate for high and low risk phenotypes, and the related Kaplan-Meier curves are presented in Fig.\xa02. The difference between the survival curves for of the high and low risk phenotypes is highly significant (p\u2009<\u20090.00005). The distribution of the raw mortality phenotype scores among cases and controls are presented in a box and whisker plot in Supplemental Figure\xa02.\n\n', 'kwd': '-', 'title': u'Precision Radiology: Predicting longevity using feature engineering and deep learning methods in a radiomics framework'}], 'Coronary Artery Disease AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615055/', 'p': u'957 rest/stress 99mtechnetium gated MPS NC studies from 623 consecutive patients with correlating invasive coronary angiography and 334 with low likelihood of CAD (LLK < 5% ) were assessed. Patients with stenosis \u2265 50% in left main or \u2265 70% in all other vessels were considered abnormal. Total perfusion deficit (TPD) was computed automatically. In addition, ischemic changes (ISCH) and ejection fraction changes (EFC) between stress and rest were derived by quantitative software. The SVM was trained using a group of 125 pts (25 LLK, 25 0-, 25 1-, 25 2- and 25 3-vessel CAD) using above quantitative variables and second order polynomial fitting. The remaining patients (N = 832) were categorized based on probability estimates, with CAD defined as (probability estimate \u2265 0.50). The diagnostic accuracy of SVM was also compared to visual segmental scoring by two experienced readers.Sensitivity of SVM (84%) was significantly better than ISCH (75%, p < 0.05) and EFC (31%, p < 0.05). Specificity of SVM (88%) was significantly better than that of TPD (78%, p < 0.05) and EFC (77%, p < 0.05). Diagnostic accuracy of SVM (86%) was significantly better than TPD (81%), ISCH (81%), or EFC (46%) (p < 0.05 for all). The Receiver-operator-characteristic area-under-the-curve (ROC-AUC) for SVM (0.92) was significantly better than TPD (0.90), ISCH (0.87), and EFC (0.60) (p < 0.001 for all). Diagnostic accuracy of SVM was comparable to the overall accuracy of both visual readers (85% vs. 84%, p < 0.05). ROC-AUC for SVM (0.92) was significantly better than that of both visual readers (0.87 and 0.88, p < 0.03).', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit, Support Vector Machines, Machine Learning', 'title': u'Improved Accuracy of Myocardial Perfusion SPECT for the Detection of Coronary Artery Disease by Utilizing a Support Vector Machines Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563765/', 'p': u'995 rest/stress 99mTc-sestamibi MPS studies, [650 consecutive cases with coronary angiography and 345 with likelihood of CAD < 5% (LLk)] were obtained by MPS with AC. Total perfusion deficit (TPD) for AC and NC data were compared to the visual summed stress and rest scores of 2 experienced readers. Visual reads were performed in 4 consecutive steps with the following information progressively revealed: NC data, AC+NC data, computer results, all clinical information.The diagnostic accuracy of TPD for detection of CAD was similar to both readers (NC: 82% vs. 84%, AC: 86% vs. 85\u201387% p = NS) with the exception of second reader when using clinical information (89%, p < 0.05). The Receiver-Operator-Characteristics Areas-Under-Curve (ROC-AUC) for TPD were significantly better than visual reads for NC (0.91 vs. 0.87 and 0.89, p < 0.01) and AC (0.92 vs. 0.90, p < 0.01), and it was comparable to visual reads incorporating all clinical information. Per-vessel accuracy of TPD was superior to one reader for NC (81% vs. 77%, p < 0.05) and AC (83% vs. 78%, p < 0.05) and equivalent to second reader [NC (79%) and AC (81%)]. Per-vessel ROC-AUC for NC (0.83) and AC (0.84) for TPD were better than (0.78\u20130.80 p < 0.01), and comparable to second reader (0.82\u20130.84, p = NS), for all steps.', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit', 'title': u'Comparison of Fully Automated Computer Analysis and Visual Scoring for Detection of Coronary Artery Disease from Myocardial Perfusion SPECT in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547801/', 'p': u'Conceived and designed the experiments: KPL ANA VK ZX SC SNM RMP PS IK SYS EWK TC. Performed the experiments: KPL ANA VK ZX AC VSG SG PC GKS DA JYL SYS TC. Analyzed the data: KPL ANA VK ZX DA SYS TC. Wrote the paper: KPL ANA VK ZX AC VSG SG PC GKS DA SC JYL SNM RMP PS IK SYS EWK TC.Typically, algorithms to classify phenotypes using electronic medical record (EMR) data were developed to perform well in a specific patient population. There is increasing interest in analyses which can allow study of a specific outcome across different diseases. Such a study in the EMR would require an algorithm that can be applied across different patient populations. Our objectives were: (1) to develop an algorithm that would enable the study of coronary artery disease (CAD) across diverse patient populations; (2) to study the impact of adding narrative data extracted using natural language processing (NLP) in the algorithm. Additionally, we demonstrate how to implement CAD algorithm to compare risk across 3 chronic diseases in a preliminary study.', 'kwd': '-', 'title': u'Methods to Develop an Electronic Medical Record Phenotype Algorithm to Compare the Risk of Coronary Artery Disease across 3 Chronic Disease Cohorts'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680364/', 'p': u'Artificial neural networks (ANN) might help to diagnose coronary artery disease. This study aimed to determine whether the diagnostic accuracy of an ANN-based diagnostic system and conventional quantitation are comparable.The ANN was trained to classify potentially abnormal areas as true or false based on the nuclear cardiology expert interpretation of 1001 gated stress/rest 99mTc-MIBI images at 12 hospitals. The diagnostic accuracy of the ANN was compared with 364 expert interpretations that served as the gold standard of abnormality for the validation study. Conventional summed stress/rest/difference scores (SSS/SRS/SDS) were calculated and compared with receiver operating characteristics (ROC) analysis.', 'kwd': u'Artificial intelligence, Diagnostic imaging, Coronary artery disease, Nuclear cardiology, Computer-aided diagnosis', 'title': u'Diagnostic accuracy of an artificial neural network compared with statistical quantitation of myocardial perfusion images: a Japanese multicenter study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4859156/', 'p': u'We aimed to investigate if early revascularization in patients with suspected coronary artery disease (CAD) can be effectively predicted by integrating clinical data and quantitative image features derived from perfusion SPECT (MPS) by machine learning (ML) approach.713 rest 201Thallium/stress 99mTechnetium MPS studies with correlating invasive angiography (372 revascularization events (275 PCI / 97 CABG) within 90 days after MPS (91% within 30 days) were considered. Transient ischemic dilation (TID), stress combined supine/prone total perfusion deficit (TPD), quantitative rest and stress TPD, exercise ejection fraction, and end-systolic volume along with clinical parameters including patient gender, history of hypertension and diabetes mellitus, ST-depression on baseline ECG, ECG and clinical response during stress, and post-ECG probability by boosted ensemble ML algorithm (LogitBoost) to predict revascularization events. These features were selected using an automated feature selection algorithm from all available clinical and quantitative data (33 parameters). 10-fold cross-validation was utilized to train and test the prediction model. The prediction of revascularization by ML algorithm was compared to standalone measures of perfusion and visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data.', 'kwd': u'Machine Learning, Coronary Artery Disease, Myocardial Perfusion SPECT, Revascularization, Total Perfusion Deficit', 'title': u'Prediction of Revascularization after Myocardial Perfusion SPECT by Machine Learning in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3808723/', 'p': u'\nBackground. Coronary artery disease (CAD) is a complex, multifactorial disease in which personality seems to play a role but with no definition in combination with other risk factors. Objective. To explore the nonlinear and simultaneous pathways between traditional and personality traits risk factors and coronary stenosis by Artificial Neural Networks (ANN) data mining analysis. Method. Seventy-five subjects were examined for traditional cardiac risk factors and personality traits. Analyses were based on a new data mining method using a particular artificial adaptive system, the autocontractive map (AutoCM). Results. Several traditional Cardiovascular Risk Factors (CRF) present significant relations with coronary artery plaque (CAP) presence or severity. Moreover, anger turns out to be the main factor of personality for CAP in connection with numbers of traditional risk factors. Hidden connection map showed that anger, hostility, and the Type D personality subscale social inhibition are the core factors related to the traditional cardiovascular risk factors (CRF) specifically by hypertension. Discussion. This study shows a nonlinear and simultaneous pathway between traditional risk factors and personality traits associated with coronary stenosis in CAD patients without history of cardiovascular disease. In particular, anger seems to be the main personality factor for CAP in addition to traditional risk factors.The following traditional cardiac risk factors were examined: hypertension: arterial blood pressure \u2265 140/90\u2009mm\u2009Hg or taking antihypertensive medications [16]; diabetes: nonfasting plasma glucose concentration of at least 200\u2009mg/dL (11.1\u2009mmol/L), or fasting plasma glucose level of at least 126\u2009mg/dL (7.0\u2009mmol/L), or being treated with antidiabetic medication; overweight: body mass index (BMI) (calculated as weight divided by height squared) \u226527\u2009kg/m2 (WHO); dyslipidemia: total serum cholesterol level is higher than 240\u2009mg/dL or a serum triglyceride level is 200\u2009mg/dL or more (or both) or use of a lipid-lowering agent; smoking: at least one cigarette per day or quit smoking during the previous year; family history of CAD: a first degree or second degree relative with premature cardiovascular disease (age \u2264 55 years).', 'kwd': '-', 'title': u'Combining Personality Traits with Traditional Risk Factors for Coronary Stenosis: An Artificial Neural Networks Solution in Patients with Computed Tomography Detected Coronary Artery Disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3732038/', 'p': u'We aimed to improve the diagnostic accuracy of myocardial perfusion SPECT (MPS) by integrating clinical data and quantitative image features with machine learning (ML) algorithms.1,181 rest 201Tl/stress 99mTc-sestamibi dual-isotope MPS studies [713 consecutive cases with correlating invasive coronary angiography (ICA) and suspected coronary artery disease (CAD) and 468 with low likelihood (LLk) of CAD <5%] were considered. Cases with stenosis <70% by ICA and LLk of CAD were considered normal. Total stress perfusion deficit (TPD) for supine/prone data, stress/rest perfusion change, and transient ischemic dilatation were derived by automated perfusion quantification software and were combined with age, sex, and post-electrocardiogram CAD probability by a boosted ensemble ML algorithm (Logit-Boost). The diagnostic accuracy of the model for prediction of obstructive CAD \u226570% was compared to standard prone/supine quantification and to visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data. Tenfold stratified cross-validation was performed.', 'kwd': u'Myocardial perfusion imaging, SPECT automated quantification, coronary artery disease, total perfusion deficit, machine learning', 'title': u'Improved accuracy of myocardial perfusion SPECT for detection of coronary artery disease by machine learning in a large population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253773/', 'p': u'Coronary artery disease (CAD) is the result of the accumulation of athermanous plaques within the walls of coronary arteries, which supply the myocardium with oxygen and nutrients. CAD leads to heart attacks or strokes and is, thus, one of the most important causes of death worldwide. Angiography, an imaging modality for blood vessels, is currently the most accurate method of diagnosing artery stenosis. However, the disadvantages of this method such as complications, costs, and possible side effects have prompted researchers to investigate alternative solutions.The current study aimed to use data analysis, a non-invasive and less costly method, and various data mining algorithms to predict the stenosis of arteries. Among many people who refer to hospitals due to chest pain, a great number of them are normal and as such do not need angiography. The objective of this study was to predict patients who are most probably normal using features with the highest correlations with CAD with a view to obviate angiography costs and complications. Not a substitute for angiography, this method would select high-risk cases that definitely need angiography.', 'kwd': u'Data Mining, Sensitivity and Specificity, Coronary Artery Disease', 'title': u'Diagnosing Coronary Artery Disease via Data Mining Algorithms by Considering Laboratory and Echocardiography Features'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643767/', 'p': u'The continuous uninterrupted feedback system is the essential part of any well-organized system. We propose aLYNX concept that is a possibility to use an artificial intelligence algorithm or a neural network model in decision-making system so as to avoid possible mistakes and to remind the doctors to review tactics once more in selected cases.aLYNX system includes: registry with significant factors, decisions and results; machine learning process based on this registry data; the use of the machine learning results as the adviser. We show a possibility to build a computer adviser with a neural network model for making a choice between coronary aortic bypass surgery (CABG) and percutaneous coronary intervention (PCI) in order to achieve a higher 5-year survival rate in patients with angina based on the experience of 5107 patients.', 'kwd': u'Coronary artery bypass grafting, Percutaneous coronary intervention, Artificial intelligence, Decision making', 'title': u'Artificial intelligence: Neural network model as the multidisciplinary team member in clinical decision support to avoid medical mistakes'}], 'Tomography AND image segmentation': [], 'Coronary Artery Disease AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5682365/', 'p': u'Cardiovascular diseases are one of the top causes of deaths worldwide. In developing nations and rural areas, difficulties with diagnosis and treatment are made worse due to the deficiency of healthcare facilities. A viable solution to this issue is telemedicine, which involves delivering health care and sharing medical knowledge at a distance. Additionally, mHealth, the utilization of mobile devices for medical care, has also proven to be a feasible choice. The integration of telemedicine, mHealth and computer-aided diagnosis systems with the fields of machine and deep learning has enabled the creation of effective services that are adaptable to a multitude of scenarios. The objective of this review is to provide an overview of heart disease diagnosis and management, especially within the context of rural healthcare, as well as discuss the benefits, issues and solutions of implementing deep learning algorithms to improve the efficacy of relevant medical applications.According to the World Health Organization (WHO), in 2015, cardiovascular diseases represented 31% of all global deaths (1), with ischemic heart disease often cited as the leading cause of death worldwide. Furthermore, public health statistics have shown an increase of patients with some form of cardiovascular disease in countries with low or middle gross national income (2). Although serious and often life threatening, cardiovascular disease in individuals can be managed clinically as a chronic condition, and treated with medications, diet, and regular monitoring of specific health indicators. Risk factors are fairly well defined and lifestyle changes can mitigate some risks. The motivation to prevent and manage heart disease has spurred development of numerous mHealth applications for consumer use, some of which have been scientifically assessed for efficacy (3). In this paper, we provide an overview of telemedicine and mHealth technologies applied in rural healthcare settings, using one form of cardiovascular disease for context. Additionally, we discuss the need for computer-aided diagnosis (CADx) as well as the implementation of machine and deep learning techniques in these systems. Finally, we explore the issues and solutions associated with using deep learning algorithms for medical applications.', 'kwd': u'Heart disease, rural healthcare, telemedicine, mHealth, computer-aided diagnosis, machine learning, deep learning', 'title': u'Deep learning for cardiac computer-aided diagnosis: benefits, issues & solutions'}], 'Patient Assessment AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689880/', 'p': u"This paper introduces Lynx, an intelligent system for personal safety at home environments, oriented to elderly people living independently, which encompasses a decision support machine for automatic home risk prevention, tested in real-life environments to respond to real time situations. The automatic system described in this paper prevents such risks by an advanced analytic methods supported by an expert knowledge system. It is minimally intrusive, using plug-and-play sensors and machine learning algorithms to learn the elder's daily activity taking into account even his health records. If the system detects that something unusual happens (in a wide sense) or if something is wrong relative to the user's health habits or medical recommendations, it sends at real-time alarm to the family, care center, or medical agents, without human intervention. The system feeds on information from sensors deployed in the home and knowledge of subject physical activities, which can be collected by mobile applications and enriched by personalized health information from clinical reports encoded in the system. The system usability and reliability have been tested in real-life conditions, with an accuracy larger than 81%. The ultimate goal of automatic summaries is to feed the telecare platform with the most relevant clinical data obtained by an unassisted way from medical summaries and move the therapeutics procedures, treatments, or medical recommendations from medical summaries to universAAL ontology. So, the anomaly engine and the predicting intentions engine are capable of learning about personal living habits of the patients, highly correlated with their clinical conditions and prescriptions. To build this integration, thanks to semantic annotation process, we need only to join the most relevant concepts (principal diseases, diagnosis, treatment, and procedures) in the control platform (see Figure 9) with a \u201csame_as\u201d link between ontologies, by the paradigm of Linked Data recommendations [39].", 'kwd': '-', 'title': u'Lynx: Automatic Elderly Behavior Prediction in Home Telecare'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5289061/', 'p': u'In efforts to develop reliable methods to detect the likelihood of impending suicidal behaviors, we have proposed the following.To gain a deeper understanding of the state of suicide risk by determining the combination of variables that distinguishes between groups with and without suicide risk.', 'kwd': u'suicide, affective disorders, artificial intelligence, risk factors, protective factors', 'title': u'Acute Mental Discomfort Associated with Suicide Behavior in a Clinical Sample of Patients with Affective Disorders: Ascertaining Critical Variables Using Artificial Intelligence Tools'}], 'Risk Stratification AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4739399/', 'p': u'Estimating patient risk of future emergency department (ED) revisits can guide the allocation of resources, e.g. local primary care and/or specialty, to better manage ED high utilization patient populations and thereby improve patient life qualities.We set to develop and validate a method to estimate patient ED revisit risk in the subsequent 6\xa0months from an ED discharge date. An ensemble decision-tree-based model with Electronic Medical Record (EMR) encounter data from HealthInfoNet (HIN), Maine\u2019s Health Information Exchange (HIE), was developed and validated, assessing patient risk for a subsequent 6\xa0month return ED visit based on the ED encounter-associated demographic and EMR clinical history data. A retrospective cohort of 293,461 ED encounters that occurred between January 1, 2012 and December 31, 2012, was assembled with the associated patients\u2019 1-year clinical histories before the ED discharge date, for model training and calibration purposes. To validate, a prospective cohort of 193,886 ED encounters that occurred between January 1, 2013 and June 30, 2013 was constructed.', 'kwd': u'ED revisit prediction, Prospective validation, Statistical modeling, EMR', 'title': u'Prospective stratification of patients at risk for emergency department revisit: resource utilization and population management strategy implications'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5382897/', 'p': u'Deep sternal wound infection following coronary artery bypass grafting is a\nserious complication associated with significant morbidity and mortality.\nDespite the substantial impact of deep sternal wound infection, there is a\nlack of specific risk stratification tools to predict this complication\nafter coronary artery bypass grafting. This study was undertaken to develop\na specific prognostic scoring system for the development of deep sternal\nwound infection that could risk-stratify patients undergoing coronary artery\nbypass grafting and be applied right after the surgical procedure.Between March 2007 and August 2016, continuous, prospective surveillance data\non deep sternal wound infection and a set of 27 variables of 1500 patients\nwere collected. Using binary logistic regression analysis, we identified\nindependent predictors of deep sternal wound infection. Initially we\ndeveloped a predictive model in a subset of 500 patients. Dataset was\nexpanded to other 1000 consecutive cases and a final model and risk score\nwere derived. Calibration of the scores was performed using the\nHosmer-Lemeshow test.', 'kwd': u'Coronary Artery Bypass, Wound Infection, Risk Assessment/Methods', 'title': u'Development and Validation of a Stratification Tool for Predicting\nRisk of Deep Sternal Wound Infection after Coronary Artery Bypass Grafting at a\nBrazilian Hospital'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5627253/', 'p': u'The accurate assessment of a patient\u2019s risk of adverse events remains a mainstay of clinical care. Commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history, presenting signs and symptoms, and lab values. More sophisticated methods, such as Artificial Neural Networks (ANN), form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data, yielding classifiers with improved performance. Using two cohorts consisting of patients admitted with a non-ST-segment elevation acute coronary syndrome, we constructed an ANN that identifies patients at high risk of cardiovascular death (CVD). The ANN was trained and tested using patient subsets derived from a cohort containing 4395 patients (Area Under the Curve (AUC) 0.743) and validated on an independent holdout set containing 861 patients (AUC 0.767). The ANN 1-year Hazard Ratio for CVD was 3.72 (95% confidence interval 1.04\u201314.3) after adjusting for the TIMI Risk Score, left ventricular ejection fraction, and B-type natriuretic peptide. A unique feature of our approach is that it captures small changes in the ST segment over time that cannot be detected by visual inspection. These findings highlight the important role that ANNs can play in risk stratification.The study populations consisted of two patient cohorts (Table\xa01) that were derived from two different clinical studies used in previous work to evaluate the performance of several computational biomarkers14,17,18. The first study19,20 included 6,560 patients with interpretable continuous ECG data, and the second study21 included 990 patients with interpretable ECG data. All patients in both cohorts were enrolled, and ECG collection began, within 48\u2009hours after presenting with symptoms consistent with a NSTE-ACS. From these datasets, we restricted our analysis to patients who had at least one day of continuous ECG signal and values for seven baseline characteristics: age, gender, current smoker, history of hypertension, history of diabetes, previous myocardial infarction (MI), and a history of previous angiography. These features correspond to the subset of features that were in common to both cohorts. Using these criteria, 4,395 patients from Cohort-1 and 861 patients from Cohort-2 were used in the analysis.\n', 'kwd': '-', 'title': u'Machine Learning Improves Risk Stratification After Acute Coronary Syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5596298/', 'p': u'To improve health outcomes and cut health care costs, we often need to conduct prediction/classification using large clinical datasets (aka, clinical big data), for example, to identify high-risk patients for preventive interventions. Machine learning has been proposed as a key technology for doing this. Machine learning has won most data science competitions and could support many clinical activities, yet only 15% of hospitals use it for even limited purposes. Despite familiarity with data, health care researchers often lack machine learning expertise to directly use clinical big data, creating a hurdle in realizing value from their data. Health care researchers can work with data scientists with deep machine learning knowledge, but it takes time and effort for both parties to communicate effectively. Facing a shortage in the United States of data scientists and hiring competition from companies with deep pockets, health care systems have difficulty recruiting data scientists. Building and generalizing a machine learning model often requires hundreds to thousands of manual iterations by data scientists to select the following: (1) hyper-parameter values and complex algorithms that greatly affect model accuracy and (2) operators and periods for temporally aggregating clinical attributes (eg, whether a patient\u2019s weight kept rising in the past year). This process becomes infeasible with limited budgets.This study\u2019s goal is to enable health care researchers to directly use clinical big data, make machine learning feasible with limited budgets and data scientist resources, and realize value from data.', 'kwd': u'machine learning, automated temporal aggregation, automatic model selection, care management, clinical big data', 'title': u'Automating Construction of Machine Learning Models With Clinical Big Data: Proposal Rationale and Methods'}], 'Patient Assessment AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2816803/', 'p': u'Cardiac magnetic resonance (CMR) imaging enables accurate and reproducible quantification of measurements of global and regional ventricular function, blood flow, perfusion at rest and stress as well as myocardial injury. Recent advances in MR hardware and software have resulted in significant improvements in image quality and a reduction in imaging time. Methods for automated and robust assessment of the parameters of cardiac function, blood flow and morphology are being developed. This article reviews the recent advances in image acquisition and quantitative image analysis in CMR.The diagnosis and management of cardiac disease requires a precise assessment of the parameters of cardiac morphology and function. Cardiac magnetic resonance (CMR) imaging has shown to be a versatile non-invasive imaging modality providing accurate and reproducible assessment of global and ventricular regional function, blood flow, myocardial perfusion and myocardial scar. In addition to enhancing clinical decision making, the accuracy and reproducibility of the CMR quantitative measures of cardiac function and morphology allow research studies to be carried out with fewer subjects enhancing cost effectiveness. Significant recent advances have been made in the generation of new CMR acquisition protocols as well as MR hardware enabling more rapid image acquisition. Despite these advances, the quantitative analysis of the images often still relies on manual tracing of the contours in many images, a time-consuming process. Reliable automated or semi-automated image segmentation and analysis software allowing for reproducible and rapid quantification are under development. In this paper an overview is provided on some of the recent work that has been carried out on image acquisition, computerized quantitative image analysis methods and semi-automated contour detection software for CMR imaging. The emerging clinical applications of quantitative CMR parameters are highlighted.', 'kwd': u'Cardiac MRI, Quantification', 'title': u'Quantification in cardiac MRI: advances in image acquisition and processing'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3893567/', 'p': u"Nonrigid image registration is an important, but time-consuming task in medical image analysis. In typical neuroimaging studies, multiple image registrations are performed, i.e., for atlas-based segmentation or template construction. Faster image registration routines would therefore be beneficial. In this paper we explore acceleration of the image registration package elastix by a combination of several techniques: (i) parallelization on the CPU, to speed up the cost function derivative calculation; (ii) parallelization on the GPU building on and extending the OpenCL framework from ITKv4, to speed up the Gaussian pyramid computation and the image resampling step; (iii) exploitation of certain properties of the B-spline transformation model; (iv) further software optimizations. The accelerated registration tool is employed in a study on diagnostic classification of Alzheimer's disease and cognitively normal controls based on T1-weighted MRI. We selected 299 participants from the publicly available Alzheimer's Disease Neuroimaging Initiative database. Classification is performed with a support vector machine based on gray matter volumes as a marker for atrophy. We evaluated two types of strategies (voxel-wise and region-wise) that heavily rely on nonrigid image registration. Parallelization and optimization resulted in an acceleration factor of 4\u20135x on an 8-core machine. Using OpenCL a speedup factor of 2 was realized for computation of the Gaussian pyramids, and 15\u201360 for the resampling step, for larger images. The voxel-wise and the region-wise classification methods had an area under the receiver operator characteristic curve of 88 and 90%, respectively, both for standard and accelerated registration. We conclude that the image registration package elastix was substantially accelerated, with nearly identical results to the non-optimized version. The new functionality will become available in the next release of elastix as open source under the BSD license.Data from the ADNI1 database was used. The ADNI cohort used for our experiments is adopted from the study of Cuingnet et al. (2011), from which we selected the AD patient group and the normal elderly control group. The inclusion criteria for participants were defined in the ADNI GO protocol (www.adni-info.org/Scientists/AboutADNI.aspx\\#). The patient group consisted of 137 patients (67 males, age = 76.0 \xb1 7.3 years, Mini Mental State Examination (MMSE) score = 23.2 \xb1 2.0), and the control group of 162 participants (76 males, age = 76.3 \xb1 5.4 years, MMSE = 29.2 \xb1 1.0). The participants were randomly split into two groups of the same size, a training set and a test set, while preserving the age and sex distribution (Cuingnet et al., 2011). Structural MRI (T1w) data were acquired at 1.5T according to the ADNI acquisition protocol (Jack et al., 2008).", 'kwd': u"image registration, parallelization, acceleration, OpenCL, elastix, Alzheimer's disease", 'title': u"Fast parallel image registration on CPU and GPU for diagnostic classification of Alzheimer's disease"}], 'Patient Assessment AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049832/', 'p': u'The dataset contains 18 T1-weighted MR brain images and their manual segmentations. The images provided by IBSR have been normalized into the Talairach space (rotation only) and preprocessed by intensity inhomogeneity correction routines. The images have slice thickness of 1.5 mm with in-plane resolution varying between 1.0 mm \xd7 1.0 mm and 0.84 mm \xd7 0.84 mm. The manual segmentations contain labels for gray matter, white matter and the ventricles. Notably, cerebrospinal fluid (CSF) outside of the ventricles is assigned the gray matter label in the IBSR segmentations.BET (Smith, 2002) uses a deformable model to separate the brain from other tissues in MR images. In our experiments, BET was applied with the default parameters to segment each of the 18 brain images from IBSR. The EC method was used to improve the accuracy of brain extraction relative to the brain masks in IBSR. 10 cross-validation experiments were performed. For each cross-validation evaluation, 9 subjects were randomly selected for training the EC method, and the remaining 9 for testing. The brain volumes have millions of voxels, posing a challenge for the AdaBoost learning, which requires loading all data in memory for efficient learning. For efficiency, we randomly selected 1% voxels uniformly from the working ROIs for training.', 'kwd': u'medical image segmentation, error correction, AdaBoost, hippocampal segmentation, brain extraction, brain tissue segmentation', 'title': u'A Learning-Based Wrapper Method to Correct Systematic Errors in Automatic Image Segmentation: Consistently Improved Performance in Hippocampus, Cortex and Brain Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5683197/', 'p': u'The MRBrainS131 (Mendrik et al., 2015) framework consists of MR images and manual segmentations from 20 patients (age, mean \xb1 standard deviation: 71 \xb1 4 years; 10 male, 10 female). The MR images were manually segmented in eight classes: white matter (WM), cortical grey matter (cGM), basal ganglia and thalami (BGT), cerebellum (CB), brain stem (BS), lateral ventricular cerebrospinal fluid (lvCSF), peripheral cerebrospinal fluid (pCSF), and WMH. Note that the MRBrainS13 challenge only includes evaluation of three combined tissue classes: white matter (including WMH), grey matter (including BGT) and CSF (pCSF and lvCSF) instead of all eight classes.Patients with type 2 diabetes mellitus and healthy controls were included from the Utrecht Diabetic Encephalopathy Study part 2 (UDES2) (Reijmer et al., 2013). The images used in MRBrainS13 were selected from the UDES2 cohort. From the UDES2 cohort we analysed images from 96 additional patients (age, mean \xb1 standard deviation: 71 \xb1 5 years; 58 male, 38 female; 51 with type 2 diabetes mellitus and 45 healthy controls). Reference segmentations of WMH were performed by manual outlining on the FLAIR images using relatively strict criteria (Brundel et al., 2014).', 'kwd': u'Brain MRI, Segmentation, White matter hyperintensities, Deep learning, Convolutional neural networks, Motion artefacts, Brain atrophy', 'title': u'Evaluation of a deep learning approach for the segmentation of brain tissues and white matter hyperintensities of presumed vascular origin in\xa0MRI'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706546/', 'p': u'Segmentation of the hippocampus from magnetic resonance (MR) images is a key task in the evaluation of mesial temporal lobe epilepsy (mTLE) patients. Several automated algorithms have been proposed although manual segmentation remains the benchmark. Choosing a reliable algorithm is problematic since structural definition pertaining to multiple edges, missing and fuzzy boundaries, and shape changes varies among mTLE subjects. Lack of statistical references and guidance for quantifying the reliability and reproducibility of automated techniques has further detracted from automated approaches. The purpose of this study was to develop a systematic and statistical approach using a large dataset for the evaluation of automated methods and establish a method that would achieve results better approximating those attained by manual tracing in the epileptogenic hippocampus.A template database of 195 (81 males, 114 females; age range 32\u201367 yr, mean 49.16 yr) MR images of mTLE patients was used in this study. Hippocampal segmentation was accomplished manually and by two well-known tools (FreeSurfer and hammer) and two previously published methods developed at their institution [Automatic brain structure segmentation (ABSS) and LocalInfo]. To establish which method was better performing for mTLE cases, several voxel-based, distance-based, and volume-based performance metrics were considered. Statistical validations of the results using automated techniques were compared with the results of benchmark manual segmentation. Extracted metrics were analyzed to find the method that provided a more similar result relative to the benchmark.', 'kwd': u'medical imaging, image processing, segmentation, hippocampus, temporal lobe epilepsy, magnetic resonance imaging (MRI)', 'title': u'Comparative performance evaluation of automated segmentation methods of hippocampus from magnetic resonance images of temporal lobe epilepsy patients'}], 'Angiography AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5375621/', 'p': u'Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works.The assigning of a class or label to a group of pixels, such as those labeled as tumor with use of a segmentation algorithm. For instance, if segmentation has been used to mark some part of an image as \u201cabnormal brain,\u201d the classifier might then try to determine whether the marked part represents benign or malignant tissue.', 'kwd': '-', 'title': u'Machine Learning for Medical Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890616/', 'p': u'CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32 \xd7 32 images of 10 object classes. The objects are normally centered in the images. Some example images and class categories from the Cifar10 dataset are shown in Figure 7. CifarNet has three convolution layers, three pooling layers, and one fully-connected layer. This CNN architecture, also used in [22] has about 0.15 million free parameters. We adopt it as a baseline model for the LN detection.The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. This success has revived the interest in CNNs [3] in computer vision. ImageNet consists of 1.2 million 256 \xd7 256 images belonging to 1000 categories. At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model. More details about the ImageNet dataset will be discussed in Sec. III-B. AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters. AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.', 'kwd': '-', 'title': u'Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning'}], 'Arterial Coronary Syndrome AND Image processing': [], 'Angiography AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756595/', 'p': u'We propose new methods for automatic segmentation of images based on an atlas of manually labeled scans and contours in the image. First, we introduce a Bayesian framework for creating initial label maps from manually annotated training images. Within this framework, we model various registration- and patch-based segmentation techniques by changing the deformation field prior. Second, we perform contour-driven regression on the created label maps to refine the segmentation. Image contours and image parcellations give rise to non-stationary kernel functions that model the relationship between image locations. Setting the kernel to the covariance function in a Gaussian process establishes a distribution over label maps supported by image structures. Maximum a posteriori estimation of the distribution over label maps conditioned on the outcome of the atlas-based segmentation yields the refined segmentation. We evaluate the segmentation in two clinical applications: the segmentation of parotid glands in head and neck CT scans and the segmentation of the left atrium in cardiac MR angiography images.Given a novel image I, we aim to infer its segmentation S based on an atlas that contains training images \u2110 =\xa0{I1,\xa0\u2026,\xa0In} with segmentations \U0001d4ae =\xa0{S1,\xa0\u2026,\xa0Sn}. A probabilistic label map \u2112 =\xa0{L1,\xa0\u2026,\xa0L\u03b7} specifies the likelihood for each label l \u2208 {1, ..., \u03b7} ', 'kwd': u'Image segmentation, atlas, patch, spectral clustering, Gaussian process, left atrium, parotid glands', 'title': u'Contour-Driven Atlas-Based Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532640/', 'p': u"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of \u201catlases\u201d (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 \u2013 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.", 'kwd': u'Multi-atlas segmentation, Label fusion, Survey', 'title': u'Multi-Atlas Segmentation of Biomedical Images: A Survey'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830888/', 'p': u'Cardiovascular magnetic resonance (CMR) has become a key imaging modality in clinical cardiology practice due to its unique capabilities for non-invasive imaging of the cardiac chambers and great vessels. A wide range of CMR sequences have been developed to assess various aspects of cardiac structure and function, and significant advances have also been made in terms of imaging quality and acquisition times. A lot of research has been dedicated to the development of global and regional quantitative CMR indices that help the distinction between health and pathology. The goal of this review paper is to discuss the structural and functional CMR indices that have been proposed thus far for clinical assessment of the cardiac chambers. We include indices definitions, the requirements for the calculations, exemplar applications in cardiovascular diseases, and the corresponding normal ranges. Furthermore, we review the most recent state-of-the art techniques for the automatic segmentation of the cardiac boundaries, which are necessary for the calculation of the CMR indices. Finally, we provide a detailed discussion of the existing literature and of the future challenges that need to be addressed to enable a more robust and comprehensive assessment of the cardiac chambers in clinical practice.In this section, we briefly describe the anatomy of the heart to help readers establish a better association between the outcomes of various functional analysis methods and the actual structure of the heart (see Fig.\xa01). Essentially, the heart provides the blood circulation system with indispensable pressure. By contracting and relaxing in turns, it transports blood to different parts of the body through the vessels. The septum separates the heart into two halves that consist of an atrium and a ventricle. The left atrium (LA) and left ventricle (LV) are partitioned by the mitral valve, while the right atrium (RA) and the right ventricle (RV) are partitioned by the tricuspid valve. The semilunar valves are located between the pulmonary artery or the aorta and the ventricle. The RA recycles the low-oxygen blood while the RV delivers it to the lung. After it is oxygenated, the blood flows into the LA, while the LV pumps it to the rest of the body. The myocardium, the muscular tissue of the heart has an inner and outer border: the endocardium and the epicardium, respectively.', 'kwd': u'Cardiac segmentation, MRI, Clinical assessment', 'title': u'A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3138936/', 'p': '-', 'kwd': u'Magnetic resonance angiography (MRA), time-of-flight (TOF), cerebrovascular segmentation, statistical model analysis, fast curve evolution', 'title': u'A Fast and Fully Automatic Method for Cerebrovascular Segmentation on Time-of-Flight (TOF) MRA Image'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5568763/', 'p': u'Anatomical-based partial volume correction (PVC) has been shown to improve image quality and quantitative accuracy in cardiac SPECT/CT. However, this method requires manual segmentation of various organs from contrast-enhanced computed tomography angiography (CTA) data. In order to achieve fully automatic CTA segmentation for clinical translation, we investigated the most common multi-atlas segmentation methods. We also modified the multi-atlas segmentation method by introducing a novel label fusion algorithm for multiple organ segmentation to eliminate overlap and gap voxels. To evaluate our proposed automatic segmentation, eight canine 99mTc-labeled red blood cell SPECT/CT datasets that incorporated PVC were analyzed, using the leave-one-out approach. The Dice similarity coefficient of each organ was computed. Compared to the conventional label fusion method, our proposed label fusion method effectively eliminated gaps and overlaps and improved the CTA segmentation accuracy. The anatomical-based PVC of cardiac SPECT images with automatic multi-atlas segmentation provided consistent image quality and quantitative estimation of intramyocardial blood volume, as compared to those derived using manual segmentation. In conclusion, our proposed automatic multi-atlas segmentation method of CTAs is feasible, practical, and facilitates anatomical-based PVC of cardiac SPECT/CT images.After registering each particular atlas image to the target image, the transformation matrices were obtained. Then, the candidate segmentation images were obtained by applying these transformations to the corresponding label images of the five critical organ ROIs, respectively. In our method, we performed this label propagation process for each atlas image respectively to generate all candidate segmentation image sets of five ROIs.', 'kwd': u'multi-atlas based segmentation, partial volume correction, cardiac SPECT/CT', 'title': u'Fully automatic multi-atlas segmentation of CTA for partial volume correction in cardiac SPECT/CT'}], 'Arterial Coronary Syndrome AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}], 'Angiography AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480986/', 'p': u'Disease staging involves the assessment of disease severity or progression and is used for treatment selection. In diabetic retinopathy, disease staging using a wide area is more desirable than that using a limited area. We investigated if deep learning artificial intelligence (AI) could be used to grade diabetic retinopathy and determine treatment and prognosis.The retrospective study analyzed 9,939 posterior pole photographs of 2,740 patients with diabetes. Nonmydriatic 45\xb0 field color fundus photographs were taken of four fields in each eye annually at Jichi Medical University between May 2011 and June 2015. A modified fully randomly initialized GoogLeNet deep learning neural network was trained on 95% of the photographs using manual modified Davis grading of three additional adjacent photographs. We graded 4,709 of the 9,939 posterior pole fundus photographs using real prognoses. In addition, 95% of the photographs were learned by the modified GoogLeNet. Main outcome measures were prevalence and bias-adjusted Fleiss\u2019 kappa (PABAK) of AI staging of the remaining 5% of the photographs.', 'kwd': '-', 'title': u'Applying artificial intelligence to disease staging: Deep learning for improved staging of diabetic retinopathy'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5643767/', 'p': u'The continuous uninterrupted feedback system is the essential part of any well-organized system. We propose aLYNX concept that is a possibility to use an artificial intelligence algorithm or a neural network model in decision-making system so as to avoid possible mistakes and to remind the doctors to review tactics once more in selected cases.aLYNX system includes: registry with significant factors, decisions and results; machine learning process based on this registry data; the use of the machine learning results as the adviser. We show a possibility to build a computer adviser with a neural network model for making a choice between coronary aortic bypass surgery (CABG) and percutaneous coronary intervention (PCI) in order to achieve a higher 5-year survival rate in patients with angina based on the experience of 5107 patients.', 'kwd': u'Coronary artery bypass grafting, Percutaneous coronary intervention, Artificial intelligence, Decision making', 'title': u'Artificial intelligence: Neural network model as the multidisciplinary team member in clinical decision support to avoid medical mistakes'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4859156/', 'p': u'We aimed to investigate if early revascularization in patients with suspected coronary artery disease (CAD) can be effectively predicted by integrating clinical data and quantitative image features derived from perfusion SPECT (MPS) by machine learning (ML) approach.713 rest 201Thallium/stress 99mTechnetium MPS studies with correlating invasive angiography (372 revascularization events (275 PCI / 97 CABG) within 90 days after MPS (91% within 30 days) were considered. Transient ischemic dilation (TID), stress combined supine/prone total perfusion deficit (TPD), quantitative rest and stress TPD, exercise ejection fraction, and end-systolic volume along with clinical parameters including patient gender, history of hypertension and diabetes mellitus, ST-depression on baseline ECG, ECG and clinical response during stress, and post-ECG probability by boosted ensemble ML algorithm (LogitBoost) to predict revascularization events. These features were selected using an automated feature selection algorithm from all available clinical and quantitative data (33 parameters). 10-fold cross-validation was utilized to train and test the prediction model. The prediction of revascularization by ML algorithm was compared to standalone measures of perfusion and visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data.', 'kwd': u'Machine Learning, Coronary Artery Disease, Myocardial Perfusion SPECT, Revascularization, Total Perfusion Deficit', 'title': u'Prediction of Revascularization after Myocardial Perfusion SPECT by Machine Learning in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3615055/', 'p': u'957 rest/stress 99mtechnetium gated MPS NC studies from 623 consecutive patients with correlating invasive coronary angiography and 334 with low likelihood of CAD (LLK < 5% ) were assessed. Patients with stenosis \u2265 50% in left main or \u2265 70% in all other vessels were considered abnormal. Total perfusion deficit (TPD) was computed automatically. In addition, ischemic changes (ISCH) and ejection fraction changes (EFC) between stress and rest were derived by quantitative software. The SVM was trained using a group of 125 pts (25 LLK, 25 0-, 25 1-, 25 2- and 25 3-vessel CAD) using above quantitative variables and second order polynomial fitting. The remaining patients (N = 832) were categorized based on probability estimates, with CAD defined as (probability estimate \u2265 0.50). The diagnostic accuracy of SVM was also compared to visual segmental scoring by two experienced readers.Sensitivity of SVM (84%) was significantly better than ISCH (75%, p < 0.05) and EFC (31%, p < 0.05). Specificity of SVM (88%) was significantly better than that of TPD (78%, p < 0.05) and EFC (77%, p < 0.05). Diagnostic accuracy of SVM (86%) was significantly better than TPD (81%), ISCH (81%), or EFC (46%) (p < 0.05 for all). The Receiver-operator-characteristic area-under-the-curve (ROC-AUC) for SVM (0.92) was significantly better than TPD (0.90), ISCH (0.87), and EFC (0.60) (p < 0.001 for all). Diagnostic accuracy of SVM was comparable to the overall accuracy of both visual readers (85% vs. 84%, p < 0.05). ROC-AUC for SVM (0.92) was significantly better than that of both visual readers (0.87 and 0.88, p < 0.03).', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit, Support Vector Machines, Machine Learning', 'title': u'Improved Accuracy of Myocardial Perfusion SPECT for the Detection of Coronary Artery Disease by Utilizing a Support Vector Machines Algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563765/', 'p': u'995 rest/stress 99mTc-sestamibi MPS studies, [650 consecutive cases with coronary angiography and 345 with likelihood of CAD < 5% (LLk)] were obtained by MPS with AC. Total perfusion deficit (TPD) for AC and NC data were compared to the visual summed stress and rest scores of 2 experienced readers. Visual reads were performed in 4 consecutive steps with the following information progressively revealed: NC data, AC+NC data, computer results, all clinical information.The diagnostic accuracy of TPD for detection of CAD was similar to both readers (NC: 82% vs. 84%, AC: 86% vs. 85\u201387% p = NS) with the exception of second reader when using clinical information (89%, p < 0.05). The Receiver-Operator-Characteristics Areas-Under-Curve (ROC-AUC) for TPD were significantly better than visual reads for NC (0.91 vs. 0.87 and 0.89, p < 0.01) and AC (0.92 vs. 0.90, p < 0.01), and it was comparable to visual reads incorporating all clinical information. Per-vessel accuracy of TPD was superior to one reader for NC (81% vs. 77%, p < 0.05) and AC (83% vs. 78%, p < 0.05) and equivalent to second reader [NC (79%) and AC (81%)]. Per-vessel ROC-AUC for NC (0.83) and AC (0.84) for TPD were better than (0.78\u20130.80 p < 0.01), and comparable to second reader (0.82\u20130.84, p = NS), for all steps.', 'kwd': u'Automated Quantification, Coronary Artery Disease, Myocardial Perfusion SPECT, Total Perfusion Deficit', 'title': u'Comparison of Fully Automated Computer Analysis and Visual Scoring for Detection of Coronary Artery Disease from Myocardial Perfusion SPECT in a Large Population'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2759696/', 'p': u'To determine the performance of an artificial neural network in transcranial color-coded duplex sonography (TCCS) diagnosis of middle cerebral artery (MCA) spasm. TCCS was prospectively acquired within 2 h prior to routine cerebral angiography in 100 consecutive patients (54M:46F, median age 50 years). Angiographic MCA vasospasm was classified as mild (<25% of vessel caliber reduction), moderate (25\u201350%), or severe (>50%). A Learning Vector Quantization neural network classified MCA spasm based on TCCS peak-systolic, mean, and end-diastolic velocity data. During a four-class discrimination task, accurate classification by the network ranged from 64.9% to 72.3%, depending on the number of neurons in the Kohonen layer. Accurate classification of vasospasm ranged from 79.6% to 87.6%, with an accuracy of 84.7% to 92.1% for the detection of moderate-to-severe vasospasm. An artificial neural network may increase the accuracy of TCCS in diagnosis of MCA spasm.Following a 15-min rest period in supine position, TCCS was performed on a sonographic scanner (Toshiba Applio, Toshiba Medical System, Tokyo, Japan) equipped with a 2.5 MHz 90\xb0 phased-array probe with B-mode and Doppler imaging. Proximal segments of the basal cerebral arteries were insonated via a transtemporal and were identified on grayscale and color imaging in relation to intracranial structures (Krejza et al. 2000). A 3-mm wide sample volume was placed on the color image of the proximal MCA (M1) about 10 mm distal to the terminal carotid or at the site of the highest flow velocity acceleration indicated by aliasing phenomenon. A linear marker was placed under visual guidance on the color image of the insonated vascular segment along the long axis of the vessel to determine the angle of insonation. The angle between this linear marker and the ultrasound beam, displayed automatically on the screen of the scanner, was considered a two-dimensional approximation of the angle of insonation. A typical TCCS image of MCA spasm is shown on Fig. 1. Angle-corrected peak systolic (VPS), mean (VMEAN), and end diastolic (VED) blood flow velocities were subsequently obtained. Automated blood flow velocity determinations were used, although manual tracing of the maximum frequency envelope of the Doppler waveform was used to obtain these values when a weak Doppler signal was noted. An expert radiologist reviewed the TCCS data for quality purposes and rejected 42 waveforms due to inferior quality. Further analyses were based on the remaining 158 data sets, including waveforms of the left and right MCA.', 'kwd': u'Ultrasound, Cerebral blood vessels, Vasospasm, Artificial neural networks, Transcranial Doppler, Diagnosis, Blood velocity, Velocimetry, Brain arteries, Ischemia, Stroke', 'title': u'Learning Vector Quantization Neural Networks Improve Accuracy of Transcranial Color-coded Duplex Sonography in Detection of Middle Cerebral Artery Spasm\u2014Preliminary Report'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4659889/', 'p': u'Monitoring heart failure patients through continues assessment of sign and symptoms by information technology tools lead to large reduction in re-hospitalization. Agent technology is one of the strongest artificial intelligence areas; therefore, it can be expected to facilitate, accelerate, and improve health services especially in home care and telemedicine. The aim of this article is to provide an agent-based model for chronic heart failure (CHF) follow-up management.This research was performed in 2013-2014 to determine appropriate scenarios and the data required to monitor and follow-up CHF patients, and then an agent-based model was designed.', 'kwd': u'Health Information Systems, Heart Failure, Artificial Intelligence, Multi-agent Systems', 'title': u'Chronic Heart Failure Follow-up Management Based on Agent Technology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680364/', 'p': u'Artificial neural networks (ANN) might help to diagnose coronary artery disease. This study aimed to determine whether the diagnostic accuracy of an ANN-based diagnostic system and conventional quantitation are comparable.The ANN was trained to classify potentially abnormal areas as true or false based on the nuclear cardiology expert interpretation of 1001 gated stress/rest 99mTc-MIBI images at 12 hospitals. The diagnostic accuracy of the ANN was compared with 364 expert interpretations that served as the gold standard of abnormality for the validation study. Conventional summed stress/rest/difference scores (SSS/SRS/SDS) were calculated and compared with receiver operating characteristics (ROC) analysis.', 'kwd': u'Artificial intelligence, Diagnostic imaging, Coronary artery disease, Nuclear cardiology, Computer-aided diagnosis', 'title': u'Diagnostic accuracy of an artificial neural network compared with statistical quantitation of myocardial perfusion images: a Japanese multicenter study'}], 'Angiography AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3736499/', 'p': u'We address the problem of motion artifact reduction in digital subtraction angiography (DSA) using image registration techniques. Most of registration algorithms proposed for application in DSA, have been designed for peripheral and cerebral angiography images in which we mainly deal with global rigid motions. These algorithms did not yield good results when applied to coronary angiography images because of complex nonrigid motions that exist in this type of angiography images. Multiresolution and iterative algorithms are proposed to cope with this problem, but these algorithms are associated with high computational cost which makes them not acceptable for real-time clinical applications. In this paper we propose a nonrigid image registration algorithm for coronary angiography images that is significantly faster than multiresolution and iterative blocking methods and outperforms competing algorithms evaluated on the same data sets. This algorithm is based on a sparse set of matched feature point pairs and the elastic registration is performed by means of multilevel B-spline image warping. Experimental results with several clinical data sets demonstrate the effectiveness of our approach.The algorithm here is a summary of the operations involved in the registration of two images of a digital angiographic image sequence, presented and discussed in the previous section. Given a mask image and a live image from an angiographic image sequence, the registration is accomplished by carrying out the following steps.', 'kwd': '-', 'title': u'Nonrigid Image Registration in Digital Subtraction Angiography Using Multilevel B-Spline'}], 'Risk Stratification AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4433206/', 'p': u'Conceived and designed the experiments: ZS XC. Performed the experiments: YL JL. Analyzed the data: YL JL Xudong Liu WK. Contributed reagents/materials/analysis tools: YL Xiaoyong Liu WK XZ FW. Wrote the paper: YL JL XC.Exfoliative cytology has been widely used for early diagnosis of oral squamous cell carcinoma (OSCC). Test outcome is reported as \u201cnegative\u201d, \u201catypical\u201d (defined as abnormal epithelial changes of uncertain diagnostic significance), and \u201cpositive\u201d (defined as definitive cellular evidence of epithelial dysplasia or carcinoma). The major challenge is how to properly manage the \u201catypical\u201d patients in order to diagnose OSCC early and prevent OSCC. In this study, we collected exfoliative cytology data, histopathology data, and clinical data of normal subjects (n=102), oral leukoplakia (OLK) patients (n=82), and OSCC patients (n=93), and developed a data analysis procedure for quantitative risk stratification of OLK patients. This procedure involving a step called expert-guided data transformation and reconstruction (EdTAR) which allows automatic data processing and reconstruction and reveals informative signals for subsequent risk stratification. Modern machine learning techniques were utilized to build statistical prediction models on the reconstructed data. Among the several models tested using resampling methods for parameter pruning and performance evaluation, Support Vector Machine (SVM) was found to be optimal with a high sensitivity (median>0.98) and specificity (median>0.99). With the SVM model, we constructed an oral cancer risk index (OCRI) which may potentially guide clinical follow-up of OLK patients. One OLK patient with an initial OCRI of 0.88 developed OSCC after 40 months of follow-up. In conclusion, we have developed a statistical method for qualitative risk stratification of OLK patients. This method may potentially improve cost-effectiveness of clinical follow-up of OLK patients, and help design clinical chemoprevention trial for high-risk populations.', 'kwd': '-', 'title': u'Quantitative Risk Stratification of Oral Leukoplakia with Exfoliative Cytology'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4780431/', 'p': u'Great effort has been devoted in recent years to the development of sudden cardiac risk predictors as a function of electric cardiac signals, mainly obtained from the electrocardiogram (ECG) analysis. But these prediction techniques are still seldom used in clinical practice, partly due to its limited diagnostic accuracy and to the lack of consensus about the appropriate computational signal processing implementation. This paper addresses a three-fold approach, based on ECG indices, to structure this review on sudden cardiac risk stratification. First, throughout the computational techniques that had been widely proposed for obtaining these indices in technical literature. Second, over the scientific evidence, that although is supported by observational clinical studies, they are not always representative enough. And third, via the limited technology transfer of academy-accepted algorithms, requiring further meditation for future systems. We focus on three families of ECG derived indices which are tackled from the aforementioned viewpoints, namely, heart rate turbulence (HRT), heart rate variability (HRV), and T-wave alternans. In terms of computational algorithms, we still need clearer scientific evidence, standardizing, and benchmarking, siting on advanced algorithms applied over large and representative datasets. New scenarios like electronic health recordings, big data, long-term monitoring, and cloud databases, will eventually open new frameworks to foresee suitable new paradigms in the near future.This set of indices aims to improve the robustness of the HRV measurements in RR tachograms, and for this purpose, they distribute the series of observed RR intervals by following a specific geometric pattern, based on the probability density function of normal RR intervals or their first difference, or on the sampling distribution density of normal RR interval durations. Emerging patterns are then measured and classified in different categories and measuring the range or the geometric figure scatter. For instance, when trying to match a given RR histogram with a triangle pattern shape, the parameters better approximating the histogram provide with a measurement of the scatter by means of the triangle basis. The most usual geometrical methods are the the triangular index, the differential index, and the logarithmic index (see Malik et al., 1996 for further details).', 'kwd': u'sudden cardiac death, risk stratification, computational algorithms, scientific evidence, technology transfer, heart rate variability, heart rate turbulence, T\u2013wave alternans', 'title': u'Sudden Cardiac Risk Stratification with Electrocardiographic Indices - A Review on Computational Processing, Technology Transfer, and Scientific Evidence'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3621376/', 'p': u'T2-weighted cardiovascular magnetic resonance (CMR) is clinically-useful for imaging the ischemic area-at-risk and amount of salvageable myocardium in patients with acute myocardial infarction (MI). However, to date, quantification of oedema is user-defined and potentially subjective.We describe a highly automatic framework for quantifying myocardial oedema from bright blood T2-weighted CMR in patients with acute MI. Our approach retains user input (i.e. clinical judgment) to confirm the presence of oedema on an image which is then subjected to an automatic analysis. The new method was tested on 25 consecutive acute MI patients who had a CMR within 48\xa0hours of hospital admission. Left ventricular wall boundaries were delineated automatically by variational level set methods followed by automatic detection of myocardial oedema by fitting a Rayleigh-Gaussian mixture statistical model. These data were compared with results from manual segmentation of the left ventricular wall and oedema, the current standard approach.', 'kwd': u'Myocardial oedema, Bright blood T2-weighted CMR, Rayleigh-Gaussian mixture model, Level set', 'title': u'Highly automatic quantification of myocardial oedema in patients with acute myocardial infarction using bright blood T2-weighted CMR'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537099/', 'p': u'Pretreatment risk stratification is key for personalized medicine. While many physicians rely on an \u201ceyeball test\u201d to assess whether patients will tolerate major surgery or chemotherapy, \u201ceyeballing\u201d is inherently subjective and difficult to quantify. The concept of morphometric age derived from cross-sectional imaging has been found to correlate well with outcomes such as length of stay, morbidity, and mortality. However, the determination of the morphometric age is time intensive and requires highly trained experts. In this study, we propose a fully automated deep learning system for the segmentation of skeletal muscle cross-sectional area (CSA) on an axial computed tomography image taken at the third lumbar vertebra. We utilized a fully automated deep segmentation model derived from an extended implementation of a fully convolutional network with weight initialization of an ImageNet pre-trained model, followed by post processing to eliminate intramuscular fat for a more accurate analysis. This experiment was conducted by varying window level (WL), window width (WW), and bit resolutions in order to better understand the effects of the parameters on the model performance. Our best model, fine-tuned on 250 training images and ground truth labels, achieves 0.93\xa0\xb1\xa00.02 Dice similarity coefficient (DSC) and 3.68\xa0\xb1\xa02.29% difference between predicted and ground truth muscle CSA on 150 held-out test cases. Ultimately, the fully automated segmentation system can be embedded into the clinical environment to accelerate the quantification of muscle and expanded to volume analysis of 3D datasets.We reformatted the manually tuned muscle segmentation maps created by domain experts as described previously into acceptable input for convolutional neural networks (CNN). As shown in Fig. \u200bFig.1,1, the axial images and their corresponding color-coded images served as original input data and ground truth labels, respectively. The main challenge for muscle segmentation is the accurate differentiation of muscle tissue from neighboring organs due to their overlapping HU ranges. We manually drew a boundary between organs and muscle, setting the inside region as additional segmentation class (\u201cInside\u201d) in an effort to train the neural network to learn distinguishing features of muscle for a precise segmentation from adjacent organs. The color-coded label images were assigned to pre-defined label indices, including 0 (black) for \u201cBackground\u201d, 1 (red) for \u201cMuscle\u201d, and 2 (green) for \u201cInside\u201d, before passing through CNNs for training as presented in Fig. \u200bFig.11.\n', 'kwd': u'Muscle segmentation, Convolutional neural networks, Computer-aided diagnosis (CAD), Computed tomography, Artificial intelligence, Deep learning', 'title': u'Pixel-Level Deep Segmentation: Artificial Intelligence Quantifies Muscle on Computed Tomography for Body Morphometric Analysis'}], 'Risk Score AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4756595/', 'p': u'We propose new methods for automatic segmentation of images based on an atlas of manually labeled scans and contours in the image. First, we introduce a Bayesian framework for creating initial label maps from manually annotated training images. Within this framework, we model various registration- and patch-based segmentation techniques by changing the deformation field prior. Second, we perform contour-driven regression on the created label maps to refine the segmentation. Image contours and image parcellations give rise to non-stationary kernel functions that model the relationship between image locations. Setting the kernel to the covariance function in a Gaussian process establishes a distribution over label maps supported by image structures. Maximum a posteriori estimation of the distribution over label maps conditioned on the outcome of the atlas-based segmentation yields the refined segmentation. We evaluate the segmentation in two clinical applications: the segmentation of parotid glands in head and neck CT scans and the segmentation of the left atrium in cardiac MR angiography images.Given a novel image I, we aim to infer its segmentation S based on an atlas that contains training images \u2110 =\xa0{I1,\xa0\u2026,\xa0In} with segmentations \U0001d4ae =\xa0{S1,\xa0\u2026,\xa0Sn}. A probabilistic label map \u2112 =\xa0{L1,\xa0\u2026,\xa0L\u03b7} specifies the likelihood for each label l \u2208 {1, ..., \u03b7} ', 'kwd': u'Image segmentation, atlas, patch, spectral clustering, Gaussian process, left atrium, parotid glands', 'title': u'Contour-Driven Atlas-Based Segmentation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4848047/', 'p': u'Minimally invasive transcatheter technologies have demonstrated substantial promise for the diagnosis and the treatment of cardiovascular diseases. For example, transcatheter aortic valve implantation is an alternative to aortic valve replacement for the treatment of severe aortic stenosis, and transcatheter atrial fibrillation ablation is widely used for the treatment and the cure of atrial fibrillation. In addition, catheter-based intravascular ultrasound and optical coherence tomography imaging of coronary arteries provides important information about the coronary lumen, wall, and plaque characteristics. Qualitative and quantitative analysis of these cross-sectional image data will be beneficial to the evaluation and the treatment of coronary artery diseases such as atherosclerosis. In all the phases (preoperative, intraoperative, and postoperative) during the transcatheter intervention procedure, computer vision techniques (e.g., image segmentation and motion tracking) have been largely applied in the field to accomplish tasks like annulus measurement, valve selection, catheter placement control, and vessel centerline extraction. This provides beneficial guidance for the clinicians in surgical planning, disease diagnosis, and treatment assessment. In this paper, we present a systematical review on these state-of-the-art methods. We aim to give a comprehensive overview for researchers in the area of computer vision on the subject of transcatheter intervention. Research in medical computing is multi-disciplinary due to its nature, and hence, it is important to understand the application domain, clinical background, and imaging modality, so that methods and quantitative measurements derived from analyzing the imaging data are appropriate and meaningful. We thus provide an overview on the background information of the transcatheter intervention procedures, as well as a review of the computer vision techniques and methodologies applied in this area.To date, there are four types of commercial transcatheter aortic valve prostheses (TAVP) available in the European market: the Sapien\xae valve by Edwards Lifesciences (Irvine, California, USA) [32], the CoreValve\xae revalving system by Medtronic (Minneapolis, Minnesota, USA) [33], the Jenavalve\xae by Jenavalve Technology (Munich, Germany) [34], and the Acurate TA\xae by Symetis (Ecublens, Switzerland) [35]. The Sapien\xae (stainless steel stent) and Sapien XT\xae (Cobalt-chromium stent) models were approved for both transapical and transfemoral approaches, and Sapien\xae is the only balloon-expandable TAVP in clinical use. The CoreValve\xae (Nitinol stent) system is a self-expandable TAVP, which was approved for transfemoral, subclavian and direct aortic approaches. Both Jenavalve\xae and Acurate TA\xae are self-expandable TAVP and they were approved for transapical procedure only. Recently, Symetis demonstrated the Acurate TF\xae at EuroPCR 2013 that can be delivered via transfemoral procedure. Fig. 5 shows examples of these heart valve models and Table 2 gives a summary of them. The impact of these heart valve prostheses is impressive. More than 40,000 TAVPs have been implanted worldwide, among which Germany is the leading country. In 2010, approximately 25% of all aortic valve replacements were performed with TAVP [43]. The total number of patients currently eligible for TAVI procedure is approximately 200,000, representing a $2B market worldwide.\n', 'kwd': u'Image processing, IVUS, medical imaging, OCT, reconstruction, registration, segmentation, transcatheter intervention, TAFA, TAVI, TMVR, TPVR, TTVI', 'title': u'Computer Vision Techniques for Transcatheter Intervention'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3570946/', 'p': u"This paper overviews one of the most important, interesting, and challenging problems in oncology, the problem of lung cancer diagnosis. Developing an effective computer-aided diagnosis (CAD) system for lung cancer is of great clinical importance and can increase the patient's chance of survival. For this reason, CAD systems for lung cancer have been investigated in a huge number of research studies. A typical CAD system for lung cancer diagnosis is composed of four main processing steps: segmentation of the lung fields, detection of nodules inside the lung fields, segmentation of the detected nodules, and diagnosis of the nodules as benign or malignant. This paper overviews the current state-of-the-art techniques that have been developed to implement each of these CAD processing steps. For each technique, various aspects of technical issues, implemented methodologies, training and testing databases, and validation methods, as well as achieved performances, are described. In addition, the paper addresses several challenges that researchers face in each implementation step and outlines the strengths and drawbacks of the existing approaches for lung cancer CAD systems. Several challenges and aspects have been facing CAD systems for lung cancer. These challenges can be summarized as follows.", 'kwd': '-', 'title': u'Computer-Aided Diagnosis Systems for Lung Cancer: Challenges and Methodologies'}], 'Risk Score AND Deep Learning': [], 'Coronary Artery Disease AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123729/', 'p': u'Despite advances in the pharmacologic and interventional treatment of coronary artery disease (CAD), atherosclerosis remains the leading cause of death in Western societies. X-ray coronary angiography has been the modality of choice for diagnosing the presence and extent of CAD. However, this technique is invasive and provides limited information on the composition of atherosclerotic plaque. Coronary computed tomography angiography (CCTA) and cardiac magnetic resonance (CMR) have emerged as promising non-invasive techniques for the clinical imaging of CAD. Hereby, CCTA allows for visualization of coronary calcification, lumen narrowing and atherosclerotic plaque composition. In this regard, data from the CONFIRM Registry recently demonstrated that both atherosclerotic plaque burden and lumen narrowing exhibit incremental value for the prediction of future cardiac events. However, due to technical limitations with CCTA, resulting in false positive or negative results in the presence of severe calcification or motion artifacts, this technique cannot entirely replace invasive angiography at the present time. CMR on the other hand, provides accurate assessment of the myocardial function due to its high spatial and temporal resolution and intrinsic blood-to-tissue contrast. Hereby, regional wall motion and perfusion abnormalities, during dobutamine or vasodilator stress, precede the development of ST-segment depression and anginal symptoms enabling the detection of functionally significant CAD. While CT generally offers better spatial resolution, the versatility of CMR can provide information on myocardial function, perfusion, and viability, all without ionizing radiation for the patients. Technical developments with these 2 non-invasive imaging tools and their current implementation in the clinical imaging of CAD will be presented and discussed herein.The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.', 'kwd': u'coronary artery disease, atherosclerotic plaque, coronary computed tomography, cardiac magnetic resonance, risk stratification', 'title': u'Cardiac magnetic resonance and computed tomography angiography for clinical imaging of stable coronary artery disease. Diagnostic classification and risk stratification'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4922813/', 'p': u'After a decade of clinical use of coronary computed tomographic angiography (CCTA) to evaluate the anatomic severity of coronary artery disease, new methods of deriving functional information from CCTA have been developed. These methods utilize the anatomic information provided by CCTA in conjunction with computational fluid dynamics to calculate fractional flow reserve (FFR) values from CCTA image data sets. Computed tomography-derived FFR (CT-FFR) enables the identification of lesion-specific drop noninvasively. A three-dimensional CT-FFR modeling technique, which provides FFR values throughout the coronary tree (HeartFlow FFRCT analysis), has been validated against measured FFR and is now approved by the US Food and Drug Administration for clinical use. This technique requires off-site supercomputer analysis. More recently, a one-dimensional computational analysis technique (Siemens cFFR), which can be performed on on-site workstations, has been developed and is currently under investigation. This article reviews CT-FFR technology and clinical evidence for its use in stable patients with suspected coronary artery disease.Intermediate degrees of stenosis (30%\u201370%) present the greatest challenge in the diagnosis of CAD. Since hemodynamically significant lesions are occasionally observed in intermediate lesions with <70% stenosis,13 the use of invasive FFR is recommended to evaluate the function of intermediate coronary lesions as a class IIa indication.6 However, given the relatively lower prevalence of lesion-specific pressure drop caused by intermediate stenosis compared to that of severe stenosis in the FAME study,13 CT-derived FFR would be of great use for assessing the functional significance of intermediate lesions to avoid unnecessary ICA and help in treatment decision making. Table 2 provides a summary of the studies of FFRCT and cFFR. Similar to the overall diagnostic accuracy of CT-derived FFR, all studies demonstrated high diagnostic performance for intermediate stenosis, with the highest accuracy and specificity for FFRCT.24,31,38,39', 'kwd': u'fractional flow reserve, coronary computed tomographic angiography, FFRCT, cFFR', 'title': u'Noninvasive FFR derived from coronary CT angiography in the management of coronary artery disease: technology and clinical update'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430913/', 'p': u'Computerized tomographic angiography (3D data representing the coronary arteries) and X-ray angiography (2D X-ray image sequences providing information about coronary arteries and their stenosis) are standard and popular assessment tools utilized for medical diagnosis of coronary artery diseases. At present, the results of both modalities are individually analyzed by specialists and it is difficult for them to mentally connect the details of these two techniques. The aim of this work is to assist medical diagnosis by providing specialists with the relationship between computerized tomographic angiography and X-ray angiography.In this study, coronary arteries from two modalities are registered in order to create a 3D reconstruction of the stenosis position. The proposed method starts with coronary artery segmentation and labeling for both modalities. Then, stenosis and relevant labeled artery in X-ray angiography image are marked by a specialist. Proper control points for the marked artery in both modalities are automatically detected and normalized. Then, a geometrical transformation function is computed using these control points. Finally, this function is utilized to register the marked artery from the X-ray angiography image on the computerized tomographic angiography and get the 3D position of the stenosis lesion.', 'kwd': u'Angiography, Computerized tomography angiography, Segmentation, Labeling, Multimodal registration, 3D reconstruction', 'title': u'3D multimodal cardiac data reconstruction using angiography and computerized tomographic angiography registration'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5422837/', 'p': u'During the last decade coronary computed tomography angiography (CTA) has become the preeminent non-invasive imaging modality to detect coronary artery disease (CAD) with high accuracy. However, CTA has a limited value in assessing the hemodynamic significance of a given stenosis due to a modest specificity and positive predictive value. In recent years, different CT techniques for detecting myocardial ischemia have emerged, such as CT-derived fractional flow reserve (FFR-CT), transluminal attenuation gradient (TAG), and myocardial CT perfusion (CTP) imaging. Myocardial CTP imaging can be performed with a single static scan during first pass of the contrast agent, with monoenergetic or dual-energy acquisition, or as a dynamic, time-resolved scan during stress by using coronary vasodilator agents (adenosine, dipyridamole, or regadenoson). A number of CTP techniques are available, which can assess myocardial perfusion in both a qualitative, semi-quantitative or quantitative manner. Once used primarily as research tools, these modalities are increasingly being used in routine clinical practice. All these techniques offer the substantial advantage of combining anatomical and functional evaluation of flow-limiting coronary stenosis in the same examination that would be beneficial for clinical decision-making. This review focuses on the state-of the-art and future trends of these evolving imaging modalities in the field of cardiology for the physiologic assessments of CAD.The FFR-CT method allows the extraction of \u2018stress induced\u2019 quantitative functional information from an anatomic CTA of at least moderate quality acquired at rest without adenosine infusion. The method uses computational fluid dynamics with simulated hyperemia to calculate the FFR measurement at any point in the vascular tree (39). The concept of coronary FFR, defined as the ratio of the mean coronary pressure distal to a coronary stenosis to the mean aortic pressure during maximal coronary blood flow, has evolved into an accepted functional measure of stenosis severity since first proposed 15 years ago (22). FFR has now become the invasive gold standard for assessing lesion-specific ischemia. A FFR value less than 0.80 or less than 0.75 identifies hemodynamic significance of coronary stenosis (7,8). FFR-CT correlates well with invasive-derived FFR measurements in patients with suspected or known CAD (17,39,40). One of the temporary drawbacks is that FFR-CT needs extreme computational ability and analysis time, which hampers widespread dissemination.', 'kwd': u'Cardiac computed tomography, coronary artery disease (CAD), stress imaging, myocardial perfusion imaging, myocardial blood flow quantification', 'title': u'Myocardial blood flow quantification for evaluation of coronary artery disease by computed tomography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566965/', 'p': u'More than a million diagnostic cardiac catheterizations are performed annually in the US for evaluation of coronary artery anatomy and the presence of atherosclerosis. Nearly half of these patients have no significant coronary lesions or do not require mechanical or surgical revascularization. Consequently, the ability to rule out clinically significant coronary artery disease (CAD) using low cost, low risk tests of serum biomarkers in even a small percentage of patients with normal coronary arteries could be highly beneficial.Serum from 359 symptomatic subjects referred for catheterization was interrogated for proteins involved in atherogenesis, atherosclerosis, and plaque vulnerability. Coronary angiography classified 150 patients without flow-limiting CAD who did not require percutaneous intervention (PCI) while 209 required coronary revascularization (stents, angioplasty, or coronary artery bypass graft surgery). Continuous variables were compared across the two patient groups for each analyte including calculation of false discovery rate (FDR \u2264 1%) and Q value (P value for statistical significance adjusted to \u2264 0.01).', 'kwd': u'atherosclerosis, biomarkers, cardiac catheterization, coronary angiography, coronary stenosis, multiplex proteomics', 'title': u'Serum protein profiles predict coronary artery disease in symptomatic patients referred for coronary angiography'}], 'Tomography AND Image processing': [], 'Coronary Artery Disease AND image segmentation': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111838/', 'p': u'The authors are developing a computer-aided detection system to assist radiologists in analysis of coronary artery disease in coronary CT angiograms (cCTA). This study evaluated the accuracy of the authors\u2019 coronary artery segmentation and tracking method which are the essential steps to define the search space for the detection of atherosclerotic plaques.The heart region in cCTA is segmented and the vascular structures are enhanced using the authors\u2019 multiscale coronary artery response (MSCAR) method that performed 3D multiscale filtering and analysis of the eigenvalues of Hessian matrices. Starting from seed points at the origins of the left and right coronary arteries, a 3D rolling balloon region growing (RBG) method that adapts to the local vessel size segmented and tracked each of the coronary arteries and identifies the branches along the tracked vessels. The branches are queued and subsequently tracked until the queue is exhausted. With Institutional Review Board approval, 62 cCTA were collected retrospectively from the authors\u2019 patient files. Three experienced cardiothoracic radiologists manually tracked and marked center points of the coronary arteries as reference standard following the 17-segment model that includes clinically significant coronary arteries. Two radiologists visually examined the computer-segmented vessels and marked the mistakenly tracked veins and noisy structures as false positives (FPs). For the 62 cases, the radiologists marked a total of 10191 center points on 865 visible coronary artery segments.', 'kwd': u'coronary arteries, vessel segmentation, computer-aided detection, coronary artery diseases, atherosclerotic plaque, multiscale filtering', 'title': u'Computerized analysis of coronary artery disease: Performance evaluation of segmentation and tracking of coronary arteries in CT angiograms'}], 'Risk Stratification AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1479368/', 'p': u'In recent years a number of algorithms for cardiovascular risk assessment has been proposed to the medical community. These algorithms consider a number of variables and express their results as the percentage risk of developing a major fatal or non-fatal cardiovascular event in the following 10 to 20 yearsThe use of predictive algorithms to assess individual absolute risk of cardiovascular future events is currently hampered by methodological and mathematical flaws. The use of newer approaches, such as fuzzy logic and artificial neural networks, linked to artificial intelligence, seems to better address both the challenge of increasing complexity resulting from a correlation between predisposing factors, data on the occurrence of cardiovascular events, and the prediction of future events on an individual level.', 'kwd': '-', 'title': u'How artificial intelligence tools can be used to assess individual patient risk in cardiovascular disease: problems with the current methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4738045/', 'p': u'The aim of this study is to present an objective method based on support vector machines (SVMs) and gravitational search algorithm (GSA) which is initially utilized for recognition the pattern among risk factors and hypertension (HTN) to stratify and analysis HTN\u2019s risk factors in an Iranian urban population.This community-based and cross-sectional research has been designed based on the probabilistic sample of residents of Isfahan, Iran, aged 19 years or over from 2001 to 2007. One of the household members was randomly selected from different age groups. Selected individuals were invited to a predefined health center to be educated on how to collect 24-hour urine sample as well as learning about topographic parameters and blood pressure measurement. The data from both the estimated and measured blood pressure [for both systolic blood pressure (SBP) and diastolic blood pressure (DBP)] demonstrated that optimized SVMs have a highest estimation potential.', 'kwd': u'Support Vector Machines, Gravitational Search Algorithm, High Blood Pressure', 'title': u'Advanced method used for hypertension\u2019s risk factors strati\ufb01cation: support vector machines and gravitational search algorithm'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4303550/', 'p': u'Multivariate pattern analysis (MVPA) methods have become an important tool in neuroimaging, revealing complex associations and yielding powerful prediction models. Despite methodological developments and novel application domains, there has been little effort to compile benchmark results that researchers can reference and compare against. This study takes a significant step in this direction. We employed three classes of state-of-the-art MVPA algorithms and common types of structural measurements from brain Magnetic Resonance Imaging (MRI) scans to predict an array of clinically relevant variables (diagnosis of Alzheimer\u2019s, schizophrenia, autism, and attention deficit and hyperactivity disorder; age, cerebrospinal fluid derived amyloid\u2013\u03b2 levels and mini-mental state exam score). We analyzed data from over 2,800 subjects, compiled from six publicly available datasets. The employed data and computational tools are freely distributed (https://www.nmr.mgh.harvard.edu/lab/mripredict), making this the largest, most comprehensive, reproducible benchmark image-based prediction experiment to date in structural neuroimaging. Finally, we make several observations regarding the factors that influence prediction performance and point to future research directions. Unsurprisingly, our results suggest that the biological footprint (effect size) has a dramatic influence on prediction performance. Though the choice of image measurement and MVPA algorithm can impact the result, there was no universally optimal selection. Intriguingly, the choice of algorithm seemed to be less critical than the choice of measurement type. Finally, our results showed that cross-validation estimates of performance, while generally optimistic, correlate well with generalization accuracy on a new dataset.We conducted a mass-univariate analysis to map regions where cortical thickness is associated with clinical variables of interest. For this analysis, we used the thickness values sampled onto the highest resolution template, fsaverage, which contains over 140k vertices on each hemisphere, and smoothed on the cortical surface with a Gaussian-like filter of a 10 mm FWHM. We then applied a general linear model at each vertex, where the outcome was thickness and the independent variables were age, gender and the clinical variable. The p-value associated with the clinical variables was then saved for each vertex (see Fig. 3). When identifying cortical areas of significant associations, we applied the false discovery rate (Benjamini and Hochberg, 1995) (FDR, q = 0.05) to correct for multiple comparisons. The total area of significant associations was then computed as the sum of the areas corresponding to the significant vertices in fsaverage.', 'kwd': u'Image-based prediction, Computer aided diagnosis, machine learning, MRI', 'title': u'Clinical prediction from structural brain MRI scans: A large-scale empirical study'}], 'Risk Score AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3768292/', 'p': u'Celiac disease (CD) is a difficult-to-diagnose condition because of its multiple clinical presentations and symptoms shared with other diseases. Gold-standard diagnostic confirmation of suspected CD is achieved by biopsying the small intestine.To develop a clinical decision\u2013support system (CDSS) integrated with an automated classifier to recognize CD cases, by selecting from experimental models developed using intelligence artificial techniques.', 'kwd': u'Decision support systems, clinical Celiac disease, Artificial intelligence', 'title': u'Artificial intelligence techniques applied to the development of a decision\u2013support system for diagnosing celiac disease'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1479368/', 'p': u'In recent years a number of algorithms for cardiovascular risk assessment has been proposed to the medical community. These algorithms consider a number of variables and express their results as the percentage risk of developing a major fatal or non-fatal cardiovascular event in the following 10 to 20 yearsThe use of predictive algorithms to assess individual absolute risk of cardiovascular future events is currently hampered by methodological and mathematical flaws. The use of newer approaches, such as fuzzy logic and artificial neural networks, linked to artificial intelligence, seems to better address both the challenge of increasing complexity resulting from a correlation between predisposing factors, data on the occurrence of cardiovascular events, and the prediction of future events on an individual level.', 'kwd': '-', 'title': u'How artificial intelligence tools can be used to assess individual patient risk in cardiovascular disease: problems with the current methods'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4689880/', 'p': u"This paper introduces Lynx, an intelligent system for personal safety at home environments, oriented to elderly people living independently, which encompasses a decision support machine for automatic home risk prevention, tested in real-life environments to respond to real time situations. The automatic system described in this paper prevents such risks by an advanced analytic methods supported by an expert knowledge system. It is minimally intrusive, using plug-and-play sensors and machine learning algorithms to learn the elder's daily activity taking into account even his health records. If the system detects that something unusual happens (in a wide sense) or if something is wrong relative to the user's health habits or medical recommendations, it sends at real-time alarm to the family, care center, or medical agents, without human intervention. The system feeds on information from sensors deployed in the home and knowledge of subject physical activities, which can be collected by mobile applications and enriched by personalized health information from clinical reports encoded in the system. The system usability and reliability have been tested in real-life conditions, with an accuracy larger than 81%. The ultimate goal of automatic summaries is to feed the telecare platform with the most relevant clinical data obtained by an unassisted way from medical summaries and move the therapeutics procedures, treatments, or medical recommendations from medical summaries to universAAL ontology. So, the anomaly engine and the predicting intentions engine are capable of learning about personal living habits of the patients, highly correlated with their clinical conditions and prescriptions. To build this integration, thanks to semantic annotation process, we need only to join the most relevant concepts (principal diseases, diagnosis, treatment, and procedures) in the control platform (see Figure 9) with a \u201csame_as\u201d link between ontologies, by the paradigm of Linked Data recommendations [39].", 'kwd': '-', 'title': u'Lynx: Automatic Elderly Behavior Prediction in Home Telecare'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2646316/', 'p': u'The goal of personalised medicine in the intensive care unit (ICU) is to predict which diagnostic tests, monitoring interventions and treatments translate to improved outcomes given the variation between patients. Unfortunately, processes such as gene transcription and drug metabolism are dynamic in the critically ill; that is, information obtained during static non-diseased conditions may have limited applicability. We propose an alternative way of personalising medicine in the ICU on a real-time basis using information derived from the application of artificial intelligence on a high-resolution database. Calculation of maintenance fluid requirement at the height of systemic inflammatory response was selected to investigate the feasibility of this approach.The Multi-parameter Intelligent Monitoring for Intensive Care II (MIMIC II) is a database of patients admitted to the Beth Israel Deaconess Medical Center ICU in Boston. Patients who were on vasopressors for more than six hours during the first 24 hours of admission were identified from the database. Demographic and physiological variables that might affect fluid requirement or reflect the intravascular volume during the first 24 hours in the ICU were extracted from the database. The outcome to be predicted is the total amount of fluid given during the second 24 hours in the ICU, including all the fluid boluses administered.', 'kwd': '-', 'title': u'An artificial intelligence tool to predict fluid requirement in the intensive care unit: a proof-of-concept study'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/', 'p': u'Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.SVM is a supervised machine learning method, which is useful for developing a model to allocate an object to one category or the other. Therefore, SVM is widely used in clinical imaging analysis, which categorizes or classifies a diagnosis. SVM constructs a hyper-plane in a high-dimensional space as the decision surface. To accomplish better performance, the margin of separation between classes needs to be maximized (Figure 2) [8]. For a non-linear classification, SVM uses the kernel technique, which implicitly converts the input features into high-dimensional feature spaces. Therefore, selection of the kernel should be appropriate, to avoid increases in error rates.', 'kwd': u'Artificial intelligence, Machine learning, Stroke', 'title': u'Deep into the Brain: Artificial Intelligence in Stroke Imaging'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1964229/', 'p': '-', 'kwd': '-', 'title': u'Artificial intelligence in medicine.'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3228706/', 'p': u'The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique.Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE).', 'kwd': '-', 'title': u'Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model'}], 'Arterial Coronary Syndrome AND Artificial Intelligence': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3848855/', 'p': u'The classification of Acute Coronary Syndrome (ACS), using artificial intelligence (AI), has recently drawn the attention of the medical researchers. Using this approach, patients with myocardial infarction can be differentiated from those with unstable angina. The present study aims to develop an integrated model, based on the feature selection and classification, for the automatic classification of ACS.A dataset containing medical records of 809 patients suspected to suffer from ACS was used. For each subject, 266 clinical factors were collected. At first, a feature selection was performed based on interviews with 20 cardiologists; thereby 40 seminal features for classifying ACS were selected. Next, a feature selection algorithm was also applied to detect a subset of the features with the best classification accuracy. As a result, the feature numbers considerably reduced to only seven. Lastly, based on the seven selected features, eight various common pattern recognition tools for classification of ACS were used.', 'kwd': u'Acute coronary syndrome, Artificial intelligence, Clinical decision support systems, Classification, Diagnosis', 'title': u'Application of pattern recognition tools for classifying acute coronary syndrome: an integrated medical modeling'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5450918/', 'p': u'Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions.This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice.', 'kwd': u'Artificial intelligence, machine learning, cardiac imaging, deep learning, image segmentation', 'title': u'Cardiac imaging: working towards fully-automated machine analysis & interpretation'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5052591/', 'p': u'Frequency domain measures of heart rate variability (HRV) are associated with adverse events after a myocardial infarction. However, patterns in the traditional frequency domain (measured in Hz, or cycles per second) may capture different cardiac phenomena at different heart rates. An alternative is to consider frequency with respect to heartbeats, or beatquency. We compared the use of frequency and beatquency domains to predict patient risk after an acute coronary syndrome. We then determined whether machine learning could further improve the predictive performance. We first evaluated the use of pre-defined frequency and beatquency bands in a clinical trial dataset (N\u2009=\u20092302) for the HRV risk measure LF/HF (the ratio of low frequency to high frequency power). Relative to frequency, beatquency improved the ability of LF/HF to predict cardiovascular death within one year (Area Under the Curve, or AUC, of 0.730 vs. 0.704, p\u2009<\u20090.001). Next, we used machine learning to learn frequency and beatquency bands with optimal predictive power, which further improved the AUC for beatquency to 0.753 (p\u2009<\u20090.001), but not for frequency. Results in additional validation datasets (N\u2009=\u20092255 and N\u2009=\u2009765) were similar. Our results suggest that beatquency and machine learning provide valuable tools in physiological studies of HRV.Our work primarily utilized electrocardiographic (ECG) recordings obtained from a clinical trial of patients after NSTEACS19. The dataset consists of all 2,302 patients in the placebo arm, and contains 93 cardiovascular deaths within the median follow-up of one year. We focused on the placebo arm because the treatment arm was prescribed ranolazine, a drug that may have anti-arrhythmic properties20 and thus affect ECG measures. We used this dataset to compare frequency and beatquency LF/HF and to train and test machine learning models. If not otherwise indicated, all results in this work refer to this dataset. In addition, we employed two additional \u201choldout datasets\u201d, for further validation of the machine learning models that were developed, using the dataset described above. These datasets are described in the Supplementary Information. Patient characteristics are reported in Table 1. For all three datasets, up to 7 days of ambulatory ECG signals recorded at 128\u2009Hz are available for each patient. In this work, we used the first 24\u2009hours from each patient to compute the heart rate time series. The protocol was approved by the local or central Institutional Review Board at all participating centers.', 'kwd': '-', 'title': u'Beatquency domain and machine learning improve prediction of cardiovascular death after acute coronary syndrome'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3808723/', 'p': u'\nBackground. Coronary artery disease (CAD) is a complex, multifactorial disease in which personality seems to play a role but with no definition in combination with other risk factors. Objective. To explore the nonlinear and simultaneous pathways between traditional and personality traits risk factors and coronary stenosis by Artificial Neural Networks (ANN) data mining analysis. Method. Seventy-five subjects were examined for traditional cardiac risk factors and personality traits. Analyses were based on a new data mining method using a particular artificial adaptive system, the autocontractive map (AutoCM). Results. Several traditional Cardiovascular Risk Factors (CRF) present significant relations with coronary artery plaque (CAP) presence or severity. Moreover, anger turns out to be the main factor of personality for CAP in connection with numbers of traditional risk factors. Hidden connection map showed that anger, hostility, and the Type D personality subscale social inhibition are the core factors related to the traditional cardiovascular risk factors (CRF) specifically by hypertension. Discussion. This study shows a nonlinear and simultaneous pathway between traditional risk factors and personality traits associated with coronary stenosis in CAD patients without history of cardiovascular disease. In particular, anger seems to be the main personality factor for CAP in addition to traditional risk factors.The following traditional cardiac risk factors were examined: hypertension: arterial blood pressure \u2265 140/90\u2009mm\u2009Hg or taking antihypertensive medications [16]; diabetes: nonfasting plasma glucose concentration of at least 200\u2009mg/dL (11.1\u2009mmol/L), or fasting plasma glucose level of at least 126\u2009mg/dL (7.0\u2009mmol/L), or being treated with antidiabetic medication; overweight: body mass index (BMI) (calculated as weight divided by height squared) \u226527\u2009kg/m2 (WHO); dyslipidemia: total serum cholesterol level is higher than 240\u2009mg/dL or a serum triglyceride level is 200\u2009mg/dL or more (or both) or use of a lipid-lowering agent; smoking: at least one cigarette per day or quit smoking during the previous year; family history of CAD: a first degree or second degree relative with premature cardiovascular disease (age \u2264 55 years).', 'kwd': '-', 'title': u'Combining Personality Traits with Traditional Risk Factors for Coronary Stenosis: An Artificial Neural Networks Solution in Patients with Computed Tomography Detected Coronary Artery Disease'}], 'Tomography AND Deep Learning': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722/', 'p': u'The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements.Under a mild assumption on the activation function, a two-layer neural network with a finite number of hidden units can approximate any continuous function (63), and thus it is regarded as universal approximator. However, it is also possible to approximate complex functions to the same accuracy using a \u2018deep\u2019 architecture, i.e., more than two layers, with much fewer number of units in total (3). Hence, it is possible to reduce the number of trainable parameters, thus allowing to train with a relatively small dataset (64).', 'kwd': u'Medical image analysis, deep learning, unsupervised feature learning', 'title': u'Deep Learning in Medical Image Analysis'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478984/', 'p': u'Visual identification of coronary arterial lesion from three-dimensional coronary computed tomography angiography (CTA) remains challenging. We aimed to develop a robust automated algorithm for computer detection of coronary artery lesions by machine learning techniques. A structured learning technique is proposed to detect all coronary arterial lesions with stenosis \xa0\u2265\xa025%. Our algorithm consists of two stages: (1)\xa0two independent base decisions indicating the existence of lesions in each arterial segment and (b) the final decision made by combining the base decisions. One of the base decisions is the support vector machine (SVM) based learning algorithm, which divides each artery into small volume patches and integrates several quantitative geometric and shape features for arterial lesions in each small volume patch by SVM algorithm. The other base decision is the formula-based analytic method. The final decision in the first stage applies SVM-based decision fusion to combine the two base decisions in the second stage. The proposed algorithm was applied to 42 CTA patient datasets, acquired with dual-source CT, where 21 datasets had 45 lesions with stenosis \xa0\u2265\xa025%. Visual identification of lesions with stenosis \xa0\u2265\xa025% by three expert readers, using consensus reading, was considered as a reference standard. Our method performed with high sensitivity (93%), specificity (95%), and accuracy (94%), with receiver operator characteristic area under the curve of 0.94. The proposed algorithm shows promising results in the automated detection of obstructive and nonobstructive lesions from CTA.Our study selected 42 consecutive patients, who underwent CTA for clinical reasons at the Cedars-Sinai Medical Center between 2007 and 2009. All patients were imaged using a dual-source 64-slice CT scanner (SOMATOM Definition Siemens Medical Solution, Forchheim, Germany). Twenty-one patients had coronary lesions with stenosis \xa0\u2265\xa025%. In these patients, 45 segments including lesions with stenosis \xa0\u2265\xa025% were identified. Eight out of the remaining 21 patients had lesions with stenosis \xa0<\xa025% and 13 patients did not have any lesions (no luminal stenosis or plaque).36', 'kwd': u'structured learning, learning-based detection, machine learning, image feature extraction, support vector machines, support vector regression, coronary computed tomography angiography, coronary arterial disease, coronary arterial lesion detection from coronary computed tomography angiography', 'title': u'Structured learning algorithm for detection of nonobstructive and obstructive coronary plaque lesions from computed tomography angiography'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5337239/', 'p': '-', 'kwd': u'Pulmonary image analysis, Computer-aided detection, Computer-aided diagnosis, Image processing, Machine learning, Deep learning', 'title': u'Fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning'}, {'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5330543/', 'p': u'Kawasaki disease (KD) is an acute childhood disease complicated by coronary artery aneurysms, intima thickening, thrombi, stenosis, lamellar calcifications, and disappearance of the media border. Automatic classification of the coronary artery layers (intima, media, and scar features) is important for analyzing optical coherence tomography (OCT) images recorded in pediatric patients. OCT has been known as an intracoronary imaging modality using near-infrared light which has recently been used to image the inner coronary artery tissues of pediatric patients, providing high spatial resolution (ranging from 10 to 20 \u03bcm). This study aims to develop a robust and fully automated tissue classification method by using the convolutional neural networks (CNNs) as feature extractor and comparing the predictions of three state-of-the-art classifiers, CNN, random forest (RF), and support vector machine (SVM). The results show the robustness of CNN as the feature extractor and random forest as the classifier with classification rate up to 96%, especially to characterize the second layer of coronary arteries (media), which is a very thin layer and it is challenging to be recognized and specified from other tissues.Generating an ensemble of trees using random vectors, which control the growth of each tree in the ensemble significantly increases the classification accuracy. Random Forest works efficiently on large data sets, carries a very low risk of overfitting, and is a robust classifier for noisy data. The trees are grown based on the CART methodology to maximum size without pruning. Two important factors which affect the Random Forest accuracy are the strength, s, of each tree and the correlation, \u03c1, between them. Generalization error for Random Forest classifier is proportional to the ratio \u03c1/s2. Hence, the smaller this ratio, the better functioning of Random Forest will be concluded. The correlation between trees is reduced by random selection of subset of features at each node to split on [43,44]. To improve the performance of the classifier in our experiments, we started from 100 trees and increase the number of trees to 1000. The optimal number of trees is chosen by considering the Out Of Bag (OOB) error rate. By setting the number of trees to 241, the error rate is low, almost close to the minimum error rate, and fewer number of trees reduces the computational burden; so, classifier performance is faster. The number of randomly selected predictors (another tuning parameter in Random Forest) is set to 7. Random Forest training and validation is described in section 2.3.4.', 'kwd': u'(100.0100) Image processing, (100.2960) Image analysis, (100.4996) Pattern recognition, neural networks, (110.0110) Imaging systems, (110.2960) Image analysis, (110.4500) Optical coherence tomography', 'title': u'Deep feature learning for automatic tissue classification of coronary artery using optical coherence tomography'}], 'Risk Score AND Image processing': [{'url': u'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684738/', 'p': u'Coronary computed tomography angiography (CTA) can be used to detect and quantitatively assess high-risk plaque features.To validate the ROMICAT score, which was derived using semi-automated quantitative measurements of high-risk plaque features, for the prediction of ACS.', 'kwd': u'coronary computed tomography angiography, acute coronary syndrome, coronary atherosclerotic plaque, acute chest pain, risk score', 'title': u'Computed Tomography-Based High-Risk Coronary Plaque Score to Predict Acute Coronary Syndrome Among Patients With Acute Chest Pain \u2013 Results from the ROMICAT II Trial'}], 'Risk Stratification AND image segmentation': []}